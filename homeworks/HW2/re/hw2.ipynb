{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9df6e03a",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b6e744",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0acec7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1babab47-7f9c-4d79-a406-2374cd08c7b4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "094a98d1ed4204dab1e460b8d9429297",
     "grade": false,
     "grade_id": "cell-73d87937382dd137",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from sklearn.datasets import make_moons\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "! pip install pot\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_two_moons(n_samples=5000, noise=0.1):\n",
    "    X, y = make_moons(n_samples=n_samples, noise=noise)\n",
    "    X = (X - X.mean()) / X.std()\n",
    "    return torch.tensor(X, dtype=torch.float32, device=device)\n",
    "\n",
    "seed = 2025\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "x_data = load_two_moons(5000, noise=0.05)\n",
    "dataset = TensorDataset(x_data)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "train_data_np = x_data.cpu().numpy()\n",
    "\n",
    "x_data = load_two_moons(1000, noise=0.05)\n",
    "val1_data_np = x_data.cpu().numpy()\n",
    "\n",
    "x_data = load_two_moons(1000, noise=0.2)\n",
    "val2_data_np = x_data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8e7e10-b259-432c-ba57-e5e789d157c7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a20d21030d2c4f083bc98256360c73c9",
     "grade": false,
     "grade_id": "cell-4cb9fae7d085cb56",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import ot\n",
    "\n",
    "def calc_w_distance(X, Y):\n",
    "    assert X.shape == Y.shape, \"The shape of X and Y do not match\"\n",
    "    # Compute the pairwise squared Euclidean distance matrix (cost matrix)\n",
    "    #M = np.sum((X[:, np.newaxis, :] - Y[np.newaxis, :, :]) ** 2, axis=2)\n",
    "    M = ot.dist(X, Y, metric='euclidean')\n",
    "\n",
    "    # Uniform weights (each point has equal probability)\n",
    "    a = np.ones(X.shape[0]) / X.shape[0]\n",
    "    b = np.ones(Y.shape[0]) / Y.shape[0]\n",
    "\n",
    "    # Solve the optimal transport problem to compute W2 distance\n",
    "    W2_distance = ot.emd2(a, b, M)  # Returns squared Wasserstein-2 distance\n",
    "    #W2_distance = np.sqrt(W2_squared)  # Take square root to get W2\n",
    "\n",
    "    print(f\"Wasserstein-1 Distance: {W2_distance:.4f}\")\n",
    "    return W2_distance\n",
    "\n",
    "\n",
    "def plot_two_moon(X, model_name):\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.scatter(X[:, 0], X[:, 1], alpha=0.5, edgecolors='k')\n",
    "    plt.title(model_name)\n",
    "    plt.xlabel(\"X-axis\")\n",
    "    plt.ylabel(\"Y-axis\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc00780-039e-4268-9a3f-db4cb823ad86",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d67f5183b24568b07b19bc9cb99a88fd",
     "grade": false,
     "grade_id": "cell-8badb1b8fc802af3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#plot_two_moon(train_data_np, \"Training Data\")\n",
    "#plot_two_moon(val1_data_np, \"Validation Data 1\")\n",
    "#plot_two_moon(val2_data_np, \"Validation Data 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415f42d1-0f5b-4cf5-bef1-abb836fd844c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d52ef1b2e46a6c8778efb48fc911ef5c",
     "grade": false,
     "grade_id": "cell-594a87fc66a8251c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# NICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134a24ce-60b3-46fd-b342-26db6a1b7663",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e9e47a04a162cf4fec934054bfaa95a9",
     "grade": false,
     "grade_id": "nice_layer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "class StandardLogistic(torch.distributions.Distribution):\n",
    "    \"\"\"Standard logistic distribution.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(StandardLogistic, self).__init__(validate_args=False)\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        \"\"\"Computes data log-likelihood.\n",
    "        Args:\n",
    "            x: input tensor.\n",
    "        Returns:\n",
    "            log-likelihood.\n",
    "        \"\"\"\n",
    "        return -(F.softplus(x) + F.softplus(-x))\n",
    "\n",
    "    def sample(self, size):\n",
    "        \"\"\"Samples from the distribution.\n",
    "        Args:\n",
    "            size: number of samples to generate.\n",
    "        Returns:\n",
    "            samples.\n",
    "        \"\"\"\n",
    "        z = torch.distributions.Uniform(0., 1.).sample(size)\n",
    "        return torch.log(z) - torch.log(1. - z)\n",
    "\n",
    "class AdditiveCouplingLayer(nn.Module):    \n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, partition):\n",
    "        super().__init__()\n",
    "        assert partition in ['odd', 'even']\n",
    "        self.partition = partition\n",
    "        _get_even = lambda xs: xs[:, 0::2]\n",
    "        _get_odd = lambda xs: xs[:, 1::2]\n",
    "        if (partition == 'even'):\n",
    "            self._first = _get_even\n",
    "            self._second = _get_odd\n",
    "        else:\n",
    "            self._first = _get_odd\n",
    "            self._second = _get_even\n",
    "        \n",
    "        _modules = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
    "        for _ in range(num_layers):\n",
    "            _modules.append(nn.Linear(hidden_dim, hidden_dim) )\n",
    "            _modules.append(nn.ReLU())\n",
    "        _modules.append(nn.Linear(hidden_dim, input_dim) )\n",
    "        self.net = nn.Sequential(*_modules)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Map an input through the partition and nonlinearity.\n",
    "        y1 = x1\n",
    "        y2 = x2 + m(x1)\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return out\n",
    "\n",
    "    def inverse(self, y):\n",
    "        \"\"\"Inverse mapping through the layer. Gradients should be turned off for this pass.\n",
    "        x1 = y1\n",
    "        x2 = y2 - m(y1)\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e834d56b-d7f2-42a6-b710-ea3e1029ed8e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9c5381da0ddec114c0f2b52adf468942",
     "grade": false,
     "grade_id": "nice_model",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class NICEModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_m_layers, prior):\n",
    "        super(NICEModel, self).__init__()\n",
    "        assert (input_dim % 2 == 0)\n",
    "        self.input_dim = input_dim\n",
    "        # define network\n",
    "        half_dim = int(input_dim / 2)\n",
    "        self.layer1 = AdditiveCouplingLayer(half_dim, hidden_dim, num_m_layers, 'odd')\n",
    "        self.layer2 = AdditiveCouplingLayer(half_dim, hidden_dim, num_m_layers, 'even')\n",
    "        self.layer3 = AdditiveCouplingLayer(half_dim, hidden_dim, num_m_layers, 'odd')\n",
    "        self.layer4 = AdditiveCouplingLayer(half_dim, hidden_dim, num_m_layers, 'even')\n",
    "        self.scaling_diag = nn.Parameter(torch.zeros(1, input_dim), requires_grad=True)\n",
    "        self.prior = prior\n",
    "        self.register_buffer('_dummy', torch.empty([0, ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through all invertible coupling layers.\n",
    "        Args:\n",
    "            x: Input data. float tensor of shape (batch_size, input_dim).\n",
    "        Returns:\n",
    "            z: Latent variable. float tensor of shape (batch_size, input_dim).\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return z\n",
    "\n",
    "\n",
    "    def inverse(self, z):\n",
    "        \"\"\"Invert a set of draws from logistic prior\n",
    "        Args:\n",
    "            z: Latent variable. float tensor of shape (batch_size, input_dim).\n",
    "        Returns:\n",
    "            x: Generated data. float tensor of shape (batch_size, input_dim).\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "        return x\n",
    "    \n",
    "    def log_prob(self, x):\n",
    "        \"\"\"Computes data log-likelihood. (See Section 3.3 in the NICE paper.)\n",
    "        Args:\n",
    "            x: input minibatch.\n",
    "        Returns:\n",
    "            log_p: log-likelihood of input.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return log_p\n",
    "    \n",
    "    def sample(self, num_samples):\n",
    "        \"\"\"Generates samples.\n",
    "        Args:\n",
    "            num_samples: number of samples to generate.\n",
    "        Returns:\n",
    "            x: samples from the data space X.\n",
    "        \"\"\"\n",
    "        z = self.prior.sample((num_samples, self.input_dim)).to(self._dummy.device)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076a23fc-cdb8-4faf-b6d2-3caca68cdac6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9ccbe6653b7b71e05c188fdb5ecd0cd4",
     "grade": false,
     "grade_id": "cell-855813fdfa595204",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "INPUT_DIM = 2\n",
    "HIDDEN_DIM = 32\n",
    "NUM_M_LAYERS = 3\n",
    "MAX_ITERS = 20000\n",
    "\n",
    "def inf_iterator(iterable):\n",
    "    iterator = iterable.__iter__()\n",
    "    while True:\n",
    "        try:\n",
    "            yield iterator.__next__()\n",
    "        except StopIteration:\n",
    "            iterator = iterable.__iter__()\n",
    "            \n",
    "train_loader = inf_iterator(dataloader)\n",
    "\n",
    "prior = StandardLogistic()\n",
    "nice_ckpt = 'model_nice_2025.pt'\n",
    "best_loss = np.inf\n",
    "if not os.path.exists(nice_ckpt):\n",
    "    model_nice = NICEModel(INPUT_DIM, HIDDEN_DIM, num_m_layers=NUM_M_LAYERS, prior=prior).to(device)\n",
    "    train_op = optim.Adam(model_nice.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-4)\n",
    "\n",
    "    model_nice.train()\n",
    "    for i in range(1, MAX_ITERS + 1):\n",
    "        batch = next(train_loader)\n",
    "        batch = batch[0].to(device)\n",
    "\n",
    "        log_p = model_nice.log_prob(batch)\n",
    "        loss = -log_p.mean()\n",
    "\n",
    "        train_op.zero_grad()\n",
    "        loss.backward()\n",
    "        train_op.step()\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            print(f'iter: {i} loss: {loss:.4f}')\n",
    "            best_model = copy.deepcopy(model_nice)\n",
    "        if i % 1000 == 0:\n",
    "            print(f'iter: {i} loss: {loss:.4f}')\n",
    "    torch.save(best_model.state_dict(), nice_ckpt)\n",
    "\n",
    "def sample_nice(model_nice, num_samples):\n",
    "    samples = model_nice.sample(num_samples)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714406ff-8c7c-4cfe-9317-b2b5a9f3269b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "488040c61ac6bcde43716d303a1792bb",
     "grade": false,
     "grade_id": "cell-8a68c9023efb6293",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "samples = sample_nice(best_model, 1000)\n",
    "calc_w_distance(samples, val1_data_np)\n",
    "calc_w_distance(samples, val2_data_np)\n",
    "#plot_two_moon(samples, \"NICE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97112b1-6f52-4ffb-99d8-e740452e308a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aee9031686280463f6dedec1769ea003",
     "grade": true,
     "grade_id": "nice_test",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# HIDDEN TEST CELL, DO NOT CHANGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69647b7-2074-43ce-8744-36a6b3cbf8aa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2893ebf375c6140a52a8c1c91dac514f",
     "grade": false,
     "grade_id": "cell-60546c04beea8d6a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# WGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edf1191-7d71-4c79-85c0-af0925eb2b22",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "459d2a6f4738991b09c3b17dddb786ab",
     "grade": false,
     "grade_id": "cell-f73464cd933bb95a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# Feel free to use these variables\n",
    "DIM = 512  # Model dimensionality\n",
    "# Gaussian noise, as in the plots in the paper\n",
    "LAMBDA = .1  # Smaller lambda seems to help for toy tasks specifically\n",
    "CRITIC_ITERS = 5  # How many critic iterations per generator iteration\n",
    "BATCH_SIZE = 128  # Batch size\n",
    "ITERS = 500  # how many generator iterations to train for\n",
    "one = torch.tensor(1, dtype=torch.float).to(device)\n",
    "mone = one * -1\n",
    "one = one.to(device)\n",
    "mone = mone.to(device)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        main = nn.Sequential(\n",
    "            nn.Linear(2, DIM),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(DIM, DIM),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(DIM, DIM),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(DIM, 2),\n",
    "        )\n",
    "        self.main = main\n",
    "\n",
    "    def forward(self, noise, real_data):\n",
    "        output = self.main(noise)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        main = nn.Sequential(\n",
    "            nn.Linear(2, DIM),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(DIM, DIM),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(DIM, DIM),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(DIM, 1),\n",
    "        )\n",
    "        self.main = main\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        output = self.main(inputs)\n",
    "        return output.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f1ea47-bb1c-491a-93f8-3492749660fc",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5998add61be0410fbd87b2c8fd339adf",
     "grade": false,
     "grade_id": "wgan_gp",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def calc_gradient_penalty(netD, real_data, fake_data):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dc4789-9c8e-4ab4-9bdb-4acbf8ee6428",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c5850ccf9e17d7e156bbaadd26f88a00",
     "grade": false,
     "grade_id": "wgan_train",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "wgan_ckpt = 'model_wgan_2025.pt'\n",
    "seed = 2025\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "netG = Generator()\n",
    "netD = Discriminator()\n",
    "\n",
    "netD = netD.to(device)\n",
    "netG = netG.to(device)\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "\n",
    "if not os.path.exists(wgan_ckpt):\n",
    "    for iteration in range(ITERS):\n",
    "        ############################\n",
    "        # (1) Update D network\n",
    "        ###########################\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "            \n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ###########################\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    torch.save(netG.state_dict(), wgan_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47622ea4-7cf3-425e-ab4d-c58a46f4be22",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4dfefd21674a4fe04e164c5b3c24317f",
     "grade": false,
     "grade_id": "wgan_sample",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sample_wgan(netG, num_samples):\n",
    "    \"\"\" Sampler from the WGAN model from an isotropic Gaussian distribution \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return samples\n",
    "\n",
    "samples = sample_wgan(netG, 1000)\n",
    "#plot_two_moon(samples, \"WGAN\")\n",
    "calc_w_distance(samples, val1_data_np)\n",
    "calc_w_distance(samples, val2_data_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7333898f-3702-41e8-868e-0f59b4eb4b34",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b3d626c08ff2c55ce3dd0304f9a23403",
     "grade": true,
     "grade_id": "wgan_test",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# HIDDEN TEST CELL, DO NOT CHANGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8458c6-0b51-484b-b46d-dc1ca9d03c0a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a00c58f1a12d99673de1dde6547a25e8",
     "grade": false,
     "grade_id": "cell-cef0ff46cd20c88d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# DDPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7415e0b-6079-4a65-bd6d-3f3ba6407d93",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "921dda511e4fc5c6fdbe994eba25753c",
     "grade": false,
     "grade_id": "ddpm_train",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "ddpm_ckpt = 'model_ddpm_2025.pt'\n",
    "betas = torch.linspace(0.0001, 0.02, 100, device=device)\n",
    "\n",
    "def q_sample(x_0, t, betas):\n",
    "    \"\"\"Adds noise to x_0 at timestep t.\"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(1, 10),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(10, input_dim)\n",
    "        )\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim * 4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 4, hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x + self.mlp(t.float().unsqueeze(-1))\n",
    "\n",
    "def train_ddpm(model, betas, n_epochs=100, lr=2e-3):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    torch.save(model.state_dict(), ddpm_ckpt)\n",
    "\n",
    "model = FeedForward().to(device)\n",
    "if not os.path.exists(ddpm_ckpt):\n",
    "    train_ddpm(model, betas, n_epochs=100, lr=2e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11d3665-e2b6-4ebe-b6e6-68b0ee1bda0c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e22ada2481bffe2104c8959cfaf3330",
     "grade": false,
     "grade_id": "ddpm_sample",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Generate new samples\n",
    "alpha_cumprod = torch.sqrt((1 - betas).cumprod(dim=0))\n",
    "\n",
    "def sample_ddpm(model, num_samples, betas):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f986e74b-c7a1-4e9a-b168-bf6f24712f9a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d1da01718edd5ce2f491b084f5e9c5a",
     "grade": false,
     "grade_id": "cell-3258c6e753cc5a32",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#samples = sample_ddpm(model, 1000, betas)\n",
    "#plot_two_moon(samples, \"DDPM\")\n",
    "#calc_w_distance(samples, val1_data_np)\n",
    "#calc_w_distance(samples, val2_data_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2108ee-fe3f-4fc4-adac-55785cd4962e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f1fceff6aee38c70a5f995854f9182d5",
     "grade": true,
     "grade_id": "ddpm_test",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# HIDDEN TEST CELL, DO NOT CHANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd80a417",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
