{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "train_data, test_data = train_test_split(trainset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and test sets\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each picture has 28*28 pixels. \n",
    "Intotal 60000 pictures.\n",
    "Learning rate 0.01, each batch takes 64 pictures. \n",
    "How many epochs should we set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def train_model_once(model,device,epoch,lr):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "    loss_record = []\n",
    "    running_loss = 0.0    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 100 mini-batches\n",
    "            with open(f'log/{model.__class__.__name__}_log_train.txt', 'a') as f:\n",
    "                f.write(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}\\n')\n",
    "            loss_record.append(running_loss / 100)\n",
    "            running_loss = 0.0\n",
    "    return loss_record\n",
    "\n",
    "def test_model(model,device,epoch):\n",
    "    model = model.to(device)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    # Initialize lists to store true labels and predicted labels\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    # Iterate over the test data\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            _, predicted = torch.max(model(inputs), 1)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Compute metrics\n",
    "    precision = precision_score(true_labels, predicted_labels, average='macro')\n",
    "    recall = recall_score(true_labels, predicted_labels, average='macro')\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "    conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    with open(f'log/{model.__class__.__name__}_log_test.txt', 'a') as f:\n",
    "        f.write(f'at epoch {epoch + 1}\\n')\n",
    "        f.write(f'Precision: {precision:.4f}\\n')\n",
    "        f.write(f'Recall: {recall:.4f}\\n')\n",
    "        f.write(f'F1 Score: {f1:.4f}\\n')\n",
    "    #print(conf_matrix)\n",
    "    return precision, recall, f1, conf_matrix\n",
    "\n",
    "def run(model_class):\n",
    "    model = model_class()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('training models on ', device)\n",
    "    \n",
    "    loss_history = []\n",
    "    metric_history = []\n",
    "    for epoch in range(20):  # loop over the dataset multiple times\n",
    "        loss_record = train_model_once(model,device,epoch,lr=0.01)\n",
    "        loss_history.append(loss_record)\n",
    "        metric_record = test_model(model,device,epoch)\n",
    "        metric_history.append(metric_record)\n",
    "    return loss_history, metric_history\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metric(metric_history):\n",
    "    precision = [m[0] for m in metric_history]\n",
    "    recall = [m[1] for m in metric_history]\n",
    "    f1 = [m[2] for m in metric_history]\n",
    "    plt.plot(precision, label='Precision')\n",
    "    plt.plot(recall, label='Recall')\n",
    "    plt.plot(f1, label='F1')\n",
    "    plt.legend(['Precision', 'Recall', 'F1'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        x = self.conv1(x)\n",
    "        # print(x.shape)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        # print(x.shape)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        # print(x.shape)\n",
    "        x = self.dropout1(x)\n",
    "        # print(x.shape)\n",
    "        x = torch.flatten(x, 1)\n",
    "        # print(x.shape)\n",
    "        x = self.fc1(x)\n",
    "        # print(x.shape)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        # print(x.shape)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training models on  cuda\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m loss_history,metric_history \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCNN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m plot_metric(metric_history\u001b[38;5;241m=\u001b[39mmetric_history)\n",
      "Cell \u001b[1;32mIn[38], line 74\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(model_class)\u001b[0m\n\u001b[0;32m     72\u001b[0m metric_history \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):  \u001b[38;5;66;03m# loop over the dataset multiple times\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m     loss_record \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     loss_history\u001b[38;5;241m.\u001b[39mappend(loss_record)\n\u001b[0;32m     76\u001b[0m     metric_record \u001b[38;5;241m=\u001b[39m test_model(model,device,epoch)\n",
      "Cell \u001b[1;32mIn[38], line 25\u001b[0m, in \u001b[0;36mtrain_model_once\u001b[1;34m(model, device, epoch, lr)\u001b[0m\n\u001b[0;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     24\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 25\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# print statistics\u001b[39;00m\n\u001b[0;32m     28\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\bingy\\anaconda3\\envs\\num-embeddings\\lib\\site-packages\\torch\\optim\\optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    491\u001b[0m             )\n\u001b[1;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bingy\\anaconda3\\envs\\num-embeddings\\lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\bingy\\anaconda3\\envs\\num-embeddings\\lib\\site-packages\\torch\\optim\\adam.py:244\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    232\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    234\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    235\u001b[0m         group,\n\u001b[0;32m    236\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    241\u001b[0m         state_steps,\n\u001b[0;32m    242\u001b[0m     )\n\u001b[1;32m--> 244\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\bingy\\anaconda3\\envs\\num-embeddings\\lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bingy\\anaconda3\\envs\\num-embeddings\\lib\\site-packages\\torch\\optim\\adam.py:876\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    874\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 876\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bingy\\anaconda3\\envs\\num-embeddings\\lib\\site-packages\\torch\\optim\\adam.py:619\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    612\u001b[0m         device_grads \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_foreach_add(  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m    613\u001b[0m             device_grads, device_params, alpha\u001b[38;5;241m=\u001b[39mweight_decay\n\u001b[0;32m    614\u001b[0m         )\n\u001b[0;32m    616\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;66;03m# Use device beta1 if beta1 is a tensor to ensure all\u001b[39;00m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;66;03m# tensors are on the same device\u001b[39;00m\n\u001b[1;32m--> 619\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_lerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_beta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    621\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_mul_(device_exp_avg_sqs, beta2)\n\u001b[0;32m    623\u001b[0m \u001b[38;5;66;03m# Due to the strictness of the _foreach_addcmul API, we can't have a single\u001b[39;00m\n\u001b[0;32m    624\u001b[0m \u001b[38;5;66;03m# tensor scalar as the scalar arg (only python number is supported there)\u001b[39;00m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;66;03m# as a result, separate out the value mul\u001b[39;00m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;66;03m# Filed https://github.com/pytorch/pytorch/issues/139795\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_history,metric_history = run(CNN)\n",
    "plot_metric(metric_history=metric_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AlexNet \n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet    \n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(64, 3)\n",
    "        self.layer2 = self._make_layer(128, 4, stride=2)\n",
    "        self.layer3 = self._make_layer(256, 6, stride=2)\n",
    "        self.layer4 = self._make_layer(512, 3, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or planes != 64:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(64, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(BasicBlock(64, planes, stride, downsample))\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(BasicBlock(planes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training models on  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bingy\\anaconda3\\envs\\num-embeddings\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0110\n",
      "Recall: 0.1000\n",
      "F1 Score: 0.0198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bingy\\anaconda3\\envs\\num-embeddings\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0110\n",
      "Recall: 0.1000\n",
      "F1 Score: 0.0198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bingy\\anaconda3\\envs\\num-embeddings\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0110\n",
      "Recall: 0.1000\n",
      "F1 Score: 0.0198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bingy\\anaconda3\\envs\\num-embeddings\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0110\n",
      "Recall: 0.1000\n",
      "F1 Score: 0.0198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bingy\\anaconda3\\envs\\num-embeddings\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0110\n",
      "Recall: 0.1000\n",
      "F1 Score: 0.0198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bingy\\anaconda3\\envs\\num-embeddings\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0110\n",
      "Recall: 0.1000\n",
      "F1 Score: 0.0198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bingy\\anaconda3\\envs\\num-embeddings\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0110\n",
      "Recall: 0.1000\n",
      "F1 Score: 0.0198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bingy\\anaconda3\\envs\\num-embeddings\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0110\n",
      "Recall: 0.1000\n",
      "F1 Score: 0.0198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bingy\\anaconda3\\envs\\num-embeddings\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0110\n",
      "Recall: 0.1000\n",
      "F1 Score: 0.0198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bingy\\anaconda3\\envs\\num-embeddings\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0110\n",
      "Recall: 0.1000\n",
      "F1 Score: 0.0198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bingy\\anaconda3\\envs\\num-embeddings\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0110\n",
      "Recall: 0.1000\n",
      "F1 Score: 0.0198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bingy\\anaconda3\\envs\\num-embeddings\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0110\n",
      "Recall: 0.1000\n",
      "F1 Score: 0.0198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bingy\\anaconda3\\envs\\num-embeddings\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0110\n",
      "Recall: 0.1000\n",
      "F1 Score: 0.0198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bingy\\anaconda3\\envs\\num-embeddings\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0110\n",
      "Recall: 0.1000\n",
      "F1 Score: 0.0198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bingy\\anaconda3\\envs\\num-embeddings\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0110\n",
      "Recall: 0.1000\n",
      "F1 Score: 0.0198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bingy\\anaconda3\\envs\\num-embeddings\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0110\n",
      "Recall: 0.1000\n",
      "F1 Score: 0.0198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bingy\\anaconda3\\envs\\num-embeddings\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0110\n",
      "Recall: 0.1000\n",
      "F1 Score: 0.0198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bingy\\anaconda3\\envs\\num-embeddings\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0110\n",
      "Recall: 0.1000\n",
      "F1 Score: 0.0198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bingy\\anaconda3\\envs\\num-embeddings\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0110\n",
      "Recall: 0.1000\n",
      "F1 Score: 0.0198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bingy\\anaconda3\\envs\\num-embeddings\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0110\n",
      "Recall: 0.1000\n",
      "F1 Score: 0.0198\n",
      "training models on  cuda\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (192x2x2). Calculated output size: (192x0x0). Output size is too small",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m models \u001b[38;5;129;01min\u001b[39;00m [CNN, AlexNet, ResNet]:\n\u001b[1;32m----> 2\u001b[0m     loss_history,metric_history \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     plot_metric(metric_history\u001b[38;5;241m=\u001b[39mmetric_history)\n",
      "Cell \u001b[1;32mIn[33], line 73\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(model_class)\u001b[0m\n\u001b[0;32m     71\u001b[0m metric_history \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):  \u001b[38;5;66;03m# loop over the dataset multiple times\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     loss_record \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m     loss_history\u001b[38;5;241m.\u001b[39mappend(loss_record)\n\u001b[0;32m     75\u001b[0m     metric_record \u001b[38;5;241m=\u001b[39m test_model(model,device)\n",
      "Cell \u001b[1;32mIn[33], line 22\u001b[0m, in \u001b[0;36mtrain_model_once\u001b[1;34m(model, device, epoch, lr)\u001b[0m\n\u001b[0;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# forward + backward + optimize\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     24\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\bingy\\anaconda3\\envs\\num-embeddings\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bingy\\anaconda3\\envs\\num-embeddings\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[35], line 33\u001b[0m, in \u001b[0;36mAlexNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 33\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[0;32m     35\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\bingy\\anaconda3\\envs\\num-embeddings\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bingy\\anaconda3\\envs\\num-embeddings\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\bingy\\anaconda3\\envs\\num-embeddings\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\bingy\\anaconda3\\envs\\num-embeddings\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bingy\\anaconda3\\envs\\num-embeddings\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\bingy\\anaconda3\\envs\\num-embeddings\\lib\\site-packages\\torch\\nn\\modules\\pooling.py:213\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[1;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bingy\\anaconda3\\envs\\num-embeddings\\lib\\site-packages\\torch\\_jit_internal.py:624\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_false(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bingy\\anaconda3\\envs\\num-embeddings\\lib\\site-packages\\torch\\nn\\functional.py:830\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[1;32m--> 830\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given input size: (192x2x2). Calculated output size: (192x0x0). Output size is too small"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for model_class in [CNN, AlexNet, ResNet]:\n",
    "    loss_history,metric_history = run(model_class)\n",
    "    plot_metric(metric_history=metric_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "num-embeddings",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
