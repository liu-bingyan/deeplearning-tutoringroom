{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/2503.10622"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Notions\n",
    "\n",
    "Variant: Encoder-Decoder, Encoder Only, Decoder Only\n",
    "\n",
    "- Encoder-only models are usually trained with self-supervised masked language modeling (MLM) objectives. Encoder-only Transformers excel at language understanding tasks. In encoder-only architectures, self-attention is unconstrained across the sequence.\n",
    "- Decoder-only models are trained with the objective of causal language modeling (CLM) – predicting the next token in a sequence given all preceding tokens. Decoder-only Transformers are the workhorses of text generation. In decoder-only models, self-attention is masked to be causal, meaning each position $i$ in a sequence can only attend to positions < $i$. \n",
    "- Encoder–decoder models can be trained either on supervised seq2seq tasks or using self-supervised denoising objectives (or a mix of both). ncoder–decoder Transformers are naturally suited for any task that involves mapping an input sequence to a output sequence.Encoder–decoder models utilize both unconstrained self-attention (in the encoder) and causal self-attention (in the decoder), as well as cross-attention between encoder and decoder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
