{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "s = torch.randn(3, 4)\n",
    "s.requires_grad = True\n",
    "X = torch.randn(4, 3)\n",
    "S = s@X\n",
    "t = torch.trace(S)\n",
    "b = torch.randn(1)\n",
    "b.requires_grad = True\n",
    "f = torch.trace(s@X)+b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(s.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4361, -0.5809, -0.6347, -0.3099],\n",
       "        [ 0.1746, -0.6219,  1.5531,  1.5621],\n",
       "        [-1.1245,  0.2617,  0.8457, -0.8959]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.backward()\n",
    "s.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2087,  1.1355,  0.1602, -1.9971],\n",
      "        [-1.8136,  0.3177,  0.2591, -0.4667],\n",
      "        [-0.3390, -2.4810, -0.5078, -0.4501]], requires_grad=True)\n",
      "tensor([[-0.4361, -0.5809, -0.6347, -0.3099],\n",
      "        [ 0.1746, -0.6219,  1.5531,  1.5621],\n",
      "        [-1.1245,  0.2617,  0.8457, -0.8959]])\n",
      "tensor([[ 1.2130,  1.1413,  0.1666, -1.9940],\n",
      "        [-1.8154,  0.3240,  0.2435, -0.4824],\n",
      "        [-0.3277, -2.4836, -0.5163, -0.4411]], requires_grad=True)\n",
      "tensor([[-0.4361, -0.5809, -0.6347, -0.3099],\n",
      "        [ 0.1746, -0.6219,  1.5531,  1.5621],\n",
      "        [-1.1245,  0.2617,  0.8457, -0.8959]])\n"
     ]
    }
   ],
   "source": [
    "print(s,s.grad, sep='\\n')\n",
    "optimizer = torch.optim.SGD([s, b], lr=0.01)\n",
    "optimizer.step()\n",
    "print(s,s.grad, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "print(s.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.s = torch.nn.Parameter(torch.randn(3, 4))\n",
    "        self.b = torch.nn.Parameter(torch.randn(1))\n",
    "    def forward(self,X):\n",
    "        return torch.trace(self.s@X)+self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.4932, -1.0155,  0.9066, -0.9174],\n",
      "        [ 1.4060, -0.7470, -0.9215, -1.1042],\n",
      "        [-0.6361,  0.2985, -0.3791, -0.4518]], requires_grad=True)\n",
      "None\n",
      "Parameter containing:\n",
      "tensor([1.0643], requires_grad=True)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = MyModule()\n",
    "X = torch.randn(4, 3)\n",
    "y = model(X)\n",
    "for v in model.parameters():\n",
    "    print(v)\n",
    "    print(v.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.4932, -1.0155,  0.9066, -0.9174],\n",
      "        [ 1.4060, -0.7470, -0.9215, -1.1042],\n",
      "        [-0.6361,  0.2985, -0.3791, -0.4518]], requires_grad=True)\n",
      "tensor([[-2.0575e+00,  7.8419e-01, -1.3891e+00,  2.9726e-01],\n",
      "        [ 2.6775e-01,  9.7127e-01, -7.6050e-01, -6.7354e-01],\n",
      "        [-3.8233e-01, -9.6500e-04,  7.6826e-01,  3.4220e-01]])\n",
      "Parameter containing:\n",
      "tensor([1.0643], requires_grad=True)\n",
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "y.backward()\n",
    "for v in model.parameters():\n",
    "    print(v)\n",
    "    print(v.grad)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.5138, -1.0233,  0.9205, -0.9203],\n",
      "        [ 1.4034, -0.7567, -0.9139, -1.0974],\n",
      "        [-0.6323,  0.2985, -0.3868, -0.4552]], requires_grad=True)\n",
      "tensor([[-2.0575e+00,  7.8419e-01, -1.3891e+00,  2.9726e-01],\n",
      "        [ 2.6775e-01,  9.7127e-01, -7.6050e-01, -6.7354e-01],\n",
      "        [-3.8233e-01, -9.6500e-04,  7.6826e-01,  3.4220e-01]])\n",
      "Parameter containing:\n",
      "tensor([1.0543], requires_grad=True)\n",
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "optimizer.step()\n",
    "for v in model.parameters():\n",
    "    print(v)\n",
    "    print(v.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.5138, -1.0233,  0.9205, -0.9203],\n",
      "        [ 1.4034, -0.7567, -0.9139, -1.0974],\n",
      "        [-0.6323,  0.2985, -0.3868, -0.4552]], requires_grad=True)\n",
      "None\n",
      "Parameter containing:\n",
      "tensor([1.0543], requires_grad=True)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "for v in model.parameters():\n",
    "    print(v)\n",
    "    print(v.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why calling zero_grad() is necesary? \n",
    "Torch design in the way that gradient accumulates, but how and why?\n",
    " - how to accumulate? they don't allow second time\n",
    " - how to accumulate different batches? they seems in the different computational graph\n",
    " - why design this way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m f \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtrace(s\u001b[38;5;129m@X\u001b[39m)\u001b[38;5;241m+\u001b[39mb\n\u001b[0;32m     12\u001b[0m f\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 13\u001b[0m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bingy\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bingy\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bingy\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "s = torch.randn(3, 4)\n",
    "s.requires_grad = True\n",
    "X = torch.randn(4, 3)\n",
    "S = s@X\n",
    "t = torch.trace(S)\n",
    "b = torch.randn(1)\n",
    "b.requires_grad = True\n",
    "f = torch.trace(s@X)+b\n",
    "\n",
    "f.backward()\n",
    "f.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8990, 0.3552, 0.0863, 0.0066, 0.0147], requires_grad=True)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([2., 2., 2., 2., 2.])\n",
      "tensor([3., 3., 3., 3., 3.])\n",
      "tensor([4., 4., 4., 4., 4.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "w = torch.rand(5)\n",
    "w.requires_grad_()\n",
    "print(w) \n",
    "s = w.sum() \n",
    "s.backward()\n",
    "print(w.grad) # tensor([1., 1., 1., 1., 1.])\n",
    "s.backward()\n",
    "print(w.grad) # tensor([2., 2., 2., 2., 2.])\n",
    "s.backward()\n",
    "print(w.grad) # tensor([3., 3., 3., 3., 3.])\n",
    "s.backward()\n",
    "print(w.grad) # tensor([4., 4., 4., 4., 4.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0], requires_grad=True)\n",
    "p = torch.nn.Parameter(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In gradient_descent, it is observed that PyTorch much slower than numpy. Investigate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
