{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf78ee80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[ 101, 2182, 2003, 2019, 2742, 6251, 1012,  102]])\n",
      "Token type IDs: tensor([[0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "Attention mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Tokens: ['[CLS]', 'here', 'is', 'an', 'example', 'sentence', '.', '[SEP]']\n",
      "Shape of [CLS] embedding: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example text\n",
    "text = \"Here is an example sentence.\"\n",
    "\n",
    "# Tokenization with special tokens\n",
    "# add_special_tokens=True adds [CLS] at beginning and [SEP] at end\n",
    "encoded_input = tokenizer(text, add_special_tokens=True, return_tensors='pt')\n",
    "\n",
    "# Let's see what's in the encoded input\n",
    "print(\"Input IDs:\", encoded_input['input_ids'])\n",
    "print(\"Token type IDs:\", encoded_input['token_type_ids'])\n",
    "print(\"Attention mask:\", encoded_input['attention_mask'])\n",
    "\n",
    "# Decode back to see the tokens (including special tokens)\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0])\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Load model\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Get BERT embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_input)\n",
    "\n",
    "# Get the [CLS] token representation (first token's embedding from last hidden state)\n",
    "cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "print(\"Shape of [CLS] embedding:\", cls_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0204d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct way to tokenize sentence pairs\n",
    "sentence_a = \"How are you?\"\n",
    "sentence_b = \"I am fine.\"\n",
    "\n",
    "# Use the tokenizer's sentence pair processing\n",
    "encoded_pair = tokenizer(sentence_a, \n",
    "                         sentence_b,  # Second text provided separately\n",
    "                         add_special_tokens=True, \n",
    "                         return_tensors='pt')\n",
    "\n",
    "pair_tokens = tokenizer.convert_ids_to_tokens(encoded_pair['input_ids'][0])\n",
    "print(\"Tokens for sentence pair:\", pair_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b786e11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "num-embeddings",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
