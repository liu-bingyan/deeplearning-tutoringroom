{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FTTransformer(nn.Module):\n",
    "    def __init__(self, category_sizes, num_continuous, emb_dim=32, n_heads=8, depth=6, dim_out=1):\n",
    "        super().__init__()\n",
    "        self.n_cat = len(category_sizes)\n",
    "        self.n_num = num_continuous\n",
    "        # Categorical embeddings and feature biases\n",
    "        self.cat_embeddings = nn.ModuleList([\n",
    "            nn.Embedding(num_cat, emb_dim) for num_cat in category_sizes\n",
    "        ])\n",
    "        # Learnable weight and bias for each numerical feature\n",
    "        self.num_weight = nn.Parameter(torch.randn(num_continuous, emb_dim))\n",
    "        self.num_bias   = nn.Parameter(torch.zeros(num_continuous, emb_dim))\n",
    "        # Feature bias for categorical features (same role as above bias, can merge with embedding as single param)\n",
    "        self.cat_bias   = nn.Parameter(torch.zeros(len(category_sizes), emb_dim))\n",
    "        # CLS token embedding\n",
    "        self.cls_token  = nn.Parameter(torch.zeros(1, emb_dim))\n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=n_heads, dim_feedforward=4*emb_dim)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "        # Prediction head\n",
    "        self.head = nn.Linear(emb_dim, dim_out)\n",
    "    \n",
    "    def forward(self, x_categ, x_cont):\n",
    "        batch_size = x_cont.size(0)\n",
    "        # Tokenize categorical features:\n",
    "        cat_tokens = []\n",
    "        for j, embed in enumerate(self.cat_embeddings):\n",
    "            cat_tok = embed(x_categ[:, j]) + self.cat_bias[j]  # [batch, emb_dim]\n",
    "            cat_tokens.append(cat_tok)\n",
    "        # Tokenize numeric features:\n",
    "        num_tokens = []\n",
    "        # x_cont shape: [batch, n_num]\n",
    "        # Use learned weight & bias: essentially x * W + bias for each feature\n",
    "        for j in range(self.n_num):\n",
    "            # elementwise multiplication of scalar feature by weight vector\n",
    "            num_tok = x_cont[:, j].unsqueeze(-1) * self.num_weight[j] + self.num_bias[j]\n",
    "            # num_tok shape [batch, emb_dim] (broadcast multiplication)\n",
    "            num_tokens.append(num_tok)\n",
    "        # Stack all feature tokens and append CLS token\n",
    "        # feature_tokens: [batch, n_cat + n_num, emb_dim]\n",
    "        feature_tokens = torch.cat(cat_tokens + num_tokens, dim=1).view(batch_size, -1, emb_dim)\n",
    "        # Append CLS token at position 0:\n",
    "        cls_token_batch = self.cls_token.expand(batch_size, 1, emb_dim)  # [batch, 1, emb_dim]\n",
    "        tokens = torch.cat([cls_token_batch, feature_tokens], dim=1)      # [batch, k+1, emb_dim]\n",
    "        # Transformer expects [seq_len, batch, emb_dim] by default\n",
    "        tokens = tokens.permute(1, 0, 2)\n",
    "        out = self.transformer(tokens)        # [seq_len, batch, emb_dim]\n",
    "        out = out.permute(1, 0, 2)            # [batch, seq_len, emb_dim]\n",
    "        # Take output corresponding to CLS token (position 0)\n",
    "        cls_out = out[:, 0, :]               # [batch, emb_dim]\n",
    "        # Final prediction\n",
    "        return self.head(cls_out)            # [batch, dim_out]\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
