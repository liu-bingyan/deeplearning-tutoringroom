{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAE = Variational Inferent + AutoEncoder\n",
    "\n",
    "From encoder_decoder experiments, the naive encoder decoder doesn't not give normal latent variables, which makes generationg hard:\n",
    " - we are trying to learn the distribution of X, encoding to latent variable only reduce the dimension, we still need to learn the latent distribution\n",
    " - we wish the latent variable follows the normal distribution.\n",
    " - consider variational inference: learn the latent distirbution as they are variational gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "dataset = datasets.MNIST(root='./data', train=True, download=False, transform=transform)\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in trainloader:\n",
    "    images, labels = batch\n",
    "    print(images.shape)\n",
    "    print(images.min(), images.max())\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and testing framework\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "using_beta_vae = False\n",
    "using_deterministic_encoder = False\n",
    "\n",
    "def report(loss_records, i=0, epochs=10, mode='train'):\n",
    "    n = len(loss_records)\n",
    "    if not using_deterministic_encoder:\n",
    "        total_loss, rc_loss, kl_div = zip(*loss_records)\n",
    "        total_loss, rc_loss, kl_div = sum(total_loss)/n, sum(rc_loss)/n, sum(kl_div)/n\n",
    "        if mode == 'train':\n",
    "            print(f'Epoch: {i+1}/{epochs}, Total Loss: {total_loss:.4f}, Reconstruction Loss: {rc_loss:.4f}, KL Divergence: {kl_div:.4f}')\n",
    "        else:\n",
    "            print(f'Test Loss: Total Loss: {total_loss:.4f}, Reconstruction Loss: {rc_loss:.4f}, KL Divergence: {kl_div:.4f}')\n",
    "    else:\n",
    "        total_loss = sum(loss_records)/n\n",
    "        if mode == 'train':\n",
    "            print(f'Epoch: {i+1}/{epochs}, Total Loss: {total_loss:.4f}')\n",
    "        else:\n",
    "            print(f'Test Loss: Total Loss: {total_loss:.4f}')\n",
    "\n",
    "def train(model, trainloader, epochs=10, lr=0.001):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_records = []\n",
    "        for batch in trainloader:\n",
    "            images, labels = batch\n",
    "            images = images.to(device)\n",
    "            if using_deterministic_encoder:\n",
    "                loss = model.get_loss(images)\n",
    "            elif not using_beta_vae:\n",
    "                loss, rc_loss,kl_loss = model.get_loss(images)\n",
    "            else:\n",
    "                loss, rc_loss,kl_loss = model.get_loss(images,epoch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if using_deterministic_encoder:\n",
    "                loss_records.append(loss.item())\n",
    "            else:\n",
    "                loss_records.append((loss.item(), rc_loss.item(), kl_loss.item()))\n",
    "        report(loss_records,epoch, epochs)\n",
    "\n",
    "def test(model, testloader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss_records = []\n",
    "        for batch in testloader:\n",
    "            images, labels = batch\n",
    "            images = images.to(device)\n",
    "            loss_records.append(model.get_loss(images))\n",
    "        report(loss_records,mode='test')\n",
    "\n",
    "import torchvision.utils as vutils\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visual_latent(model,testloader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    latents = []\n",
    "    with torch.no_grad():\n",
    "        for batch in testloader:\n",
    "            images, labels = batch\n",
    "            images = images.to(device)\n",
    "            if using_deterministic_encoder:\n",
    "                _, z = model(images)\n",
    "                latent = z\n",
    "            else:\n",
    "                _, mu, log_var = model(images)\n",
    "                latent = mu + torch.exp(0.5*log_var) * torch.randn_like(mu)\n",
    "            latents.append(latent)\n",
    "        latents = torch.cat(latents, dim=0)\n",
    "\n",
    "    pca = PCA(n_components=2) \n",
    "    latents = pca.fit_transform(latents.cpu())\n",
    "    plt.scatter(latents[:,0], latents[:,1], c='b', marker='o')\n",
    "    plt.title('Latent Space')\n",
    "    suffix = f'_beta_{int(1/model.beta)}' if using_beta_vae else ''\n",
    "    plt.savefig(f'log/VAE/{model._get_name()}/{model._get_name()}_{model.latent_dim}_latent_plots{suffix}.png')\n",
    "\n",
    "def reconstruct(model, testloader):\n",
    "    device = \"cpu\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    for batch in testloader:\n",
    "        images, labels = batch\n",
    "        if using_deterministic_encoder:\n",
    "            x_hat,_ = model(images)\n",
    "        else:\n",
    "            x_hat, _,_ = model(images)  \n",
    "        # Save the reconstructed images\n",
    "        pair = torch.cat((images, x_hat), dim=0)\n",
    "        suffix = f'_beta_{int(1/model.beta)}' if using_beta_vae else ''\n",
    "        vutils.save_image(pair, f'log/VAE/{model._get_name()}/{model._get_name()}_{model.latent_dim}_reconstructed_images{suffix}.png', normalize=True)\n",
    "        print(torch.nn.functional.mse_loss(x_hat, images))\n",
    "        break\n",
    "    \n",
    "def generate(model, num_samples=64):\n",
    "    z = torch.randn(num_samples, model.latent_dim)\n",
    "    x_hat = model.generator(z)\n",
    "    suffix = f'_beta_{int(1/model.beta)}' if using_beta_vae else ''\n",
    "    vutils.save_image(x_hat, f'log/VAE/{model._get_name()}/{model._get_name()}_{model.latent_dim}_generated_images{suffix}.png', normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(cls, epochs=5,beta=1):\n",
    "    for latent_dim in [2,5,10,20]:\n",
    "        print(f'Traing {cls.__name__} with latent dim:',latent_dim)\n",
    "        if using_beta_vae:\n",
    "            model = cls(latent_dim,beta=beta)\n",
    "        else:\n",
    "            model = cls(latent_dim)\n",
    "        test(model, testloader)\n",
    "        train(model, trainloader, epochs=epochs, lr=0.001)\n",
    "        test(model, testloader)\n",
    "        visual_latent(model,testloader)\n",
    "        reconstruct(model,testloader)\n",
    "        generate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A autoencoder model to test if the framework is working        \n",
    "import torch.nn as nn\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, latent_dim)\n",
    "    def forward(self,x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 28*28)\n",
    "    def forward(self,z):\n",
    "        z = torch.relu(self.fc1(z))\n",
    "        z = torch.sigmoid(self.fc2(z))\n",
    "        z = z.view(-1, 1, 28, 28)\n",
    "        return z\n",
    "\n",
    "class NaiveAE(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(NaiveAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.inferencer = Encoder(latent_dim)\n",
    "        self.generator = Decoder(latent_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        z = self.inferencer(x)\n",
    "        x_hat = self.generator(z)\n",
    "        return x_hat,z\n",
    "    \n",
    "    def get_loss(self,x):\n",
    "        x_hat,z = self.forward(x)\n",
    "        # Reconstruction loss\n",
    "        reconstruction_loss = nn.functional.mse_loss(x_hat,x,reduction='mean')\n",
    "        return reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using_deterministic_encoder = True\n",
    "#run(NaiveAE, epochs=5)\n",
    "using_deterministic_encoder = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE using MLP\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class NaiveInferencer(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(NaiveInferencer, self).__init__()\n",
    "        self.fc = nn.Linear(28*28, 500)\n",
    "        self.fc_mu = nn.Linear(500, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(500, latent_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = torch.tanh(self.fc(x))\n",
    "        \n",
    "        mu = self.fc_mu(x)\n",
    "        log_var = self.fc_logvar(x)\n",
    "        return mu,log_var\n",
    "    \n",
    "class NaiveGenerator(nn.Module):\n",
    "    def __init__(self,latent_dim):\n",
    "        super(NaiveGenerator, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, 500)\n",
    "        self.fc2 = nn.Linear(500, 28*28)\n",
    "        \n",
    "    def forward(self,z):\n",
    "        z = torch.tanh(self.fc1(z))\n",
    "        z = torch.sigmoid(self.fc2(z))\n",
    "        z = z.view(-1, 1, 28, 28)\n",
    "        return z\n",
    "        \n",
    "class NaiveVAE(nn.Module):\n",
    "    def __init__(self, latent_dim,beta = 1):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.inferencer = NaiveInferencer(latent_dim)\n",
    "        self.generator = NaiveGenerator(latent_dim)\n",
    "        self.beta = beta\n",
    "        \n",
    "    def forward(self,x):\n",
    "        mu, log_var = self.inferencer(x)\n",
    "\n",
    "        z = mu + torch.exp(0.5*log_var) * torch.randn_like(mu)\n",
    "        x_hat= self.generator(z)\n",
    "        return x_hat, mu, log_var\n",
    "\n",
    "    def get_loss(self,x,epoch=10): \n",
    "        x_hat, mu, log_var = self.forward(x)\n",
    "\n",
    "        if using_beta_vae:\n",
    "            reconstruction_loss = torch.nn.functional.mse_loss(x_hat, x, reduction='mean')\n",
    "            KL_divergence = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp(),dim=1).mean()\n",
    "            ELBO = reconstruction_loss + self.beta * KL_divergence\n",
    "        else:\n",
    "            reconstruction_loss = torch.nn.functional.mse_loss(x_hat, x, reduction='sum')\n",
    "            KL_divergence = 0.5 * torch.sum(mu.pow(2) + log_var.exp() - log_var - 1)\n",
    "            ELBO = reconstruction_loss + KL_divergence        \n",
    "        return ELBO, reconstruction_loss, KL_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE using MLP, reference version\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NaiveInferencer2(nn.Module):\n",
    "    def __init__(self, latent_dim=20, input_dim=784, hidden_dim=500):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        # Encoder\n",
    "        self.enc_lin = nn.Linear(input_dim, hidden_dim)\n",
    "        self.enc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.enc_sigma = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        x = x.view(-1,28*28)\n",
    "        x = torch.tanh(self.enc_lin(x))\n",
    "        z_mu = self.enc_mu(x)\n",
    "        z_sigma = self.enc_sigma(x)\n",
    "        return z_mu, z_sigma\n",
    "    \n",
    "class NaiveGenerator2(nn.Module):\n",
    "    def __init__(self, latent_dim=20, input_dim=784, hidden_dim=500):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        # Decoder\n",
    "        self.dec_lin1 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.dec_lin2 = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        # YOUR CODE HERE\n",
    "        z = torch.tanh(self.dec_lin1(z))\n",
    "        z = F.sigmoid(self.dec_lin2(z))\n",
    "        z = z.view(-1,1,28,28)\n",
    "        return z\n",
    "\n",
    "class NaiveVAE2(nn.Module):\n",
    "    def __init__(self, latent_dim=20, input_dim=784, hidden_dim=500):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.inferencer = NaiveInferencer2(latent_dim, input_dim, hidden_dim)\n",
    "        self.generator = NaiveGenerator2(latent_dim, input_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,28*28)\n",
    "        z_mu, z_sigma = self.inferencer(x)\n",
    "        z = z_mu + torch.randn_like(z_sigma) * torch.exp(0.5 * z_sigma)\n",
    "        x_hat = self.generator(z)\n",
    "        x_hat = x_hat.view(-1,1,28,28)\n",
    "        return x_hat, z_mu, z_sigma\n",
    "\n",
    "    def get_loss(self, x):\n",
    "        x_hat, z_mu, z_sigma = self(x)\n",
    "        reconstruction_loss = F.mse_loss(x_hat, x, reduction='sum')\n",
    "        kl_loss = -0.5 * torch.sum(1 + z_sigma - z_mu.pow(2) - torch.exp(z_sigma))\n",
    "        return reconstruction_loss + kl_loss, reconstruction_loss, kl_loss\n",
    "\n",
    "    # @torch.no_grad()\n",
    "    # def sample(self, n_samples, device):\n",
    "    #     random_z = torch.randn(n_samples, self.latent_dim).to(device)\n",
    "    #     x_hat = self.generator(random_z)\n",
    "    #     return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE using convolutional blocks\n",
    "import torch.nn as nn\n",
    "\n",
    "class Inferencer(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Inferencer, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, 3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(8, 8, 3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.conv3 = nn.Conv2d(8, 8, 3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.fc_mu = nn.Linear(8*3*3, latent_dim)        \n",
    "        self.fc_logvar = nn.Linear(8*3*3, latent_dim)     \n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x) # 1*28*28 -> 8*28*28\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool1(x) # 8*28*28 -> 8*14*14\n",
    "\n",
    "        x = self.conv2(x) # 8*14*14 -> 8*14*14\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool2(x) # 8*14*14 -> 8*7*7\n",
    "\n",
    "        x = self.conv3(x) # 8*7*7 -> 8*7*7\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool3(x) # 8*7*7 -> 8*3*3\n",
    "\n",
    "        x = x.view(-1, 8*3*3)\n",
    "        \n",
    "        mu = self.fc_mu(x)\n",
    "        log_var = self.fc_logvar(x)\n",
    "        return mu,log_var\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self,latent_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.fc = nn.Linear(latent_dim, 8*3*3)\n",
    "        self.deconv1 = nn.ConvTranspose2d(8, 8, kernel_size=2, stride=2,output_padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(8, 8, kernel_size=2, stride=2)\n",
    "        self.deconv3 = nn.ConvTranspose2d(8, 8, kernel_size=2, stride=2)\n",
    "        self.conv1 = nn.Conv2d(8,1,3,padding=1)\n",
    "        \n",
    "    def forward(self,z):\n",
    "        z = self.fc(z)\n",
    "        z = z.view(-1, 8, 3, 3)\n",
    "        z = self.deconv1(z)\n",
    "        z = torch.relu(z)\n",
    "        z = self.deconv2(z)\n",
    "        z = torch.relu(z)\n",
    "        z = self.deconv3(z)\n",
    "        z = self.conv1(z)\n",
    "        z = torch.sigmoid(z)\n",
    "        return z\n",
    "        \n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim,beta=1):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.inferencer = Inferencer(latent_dim)\n",
    "        self.generator = Generator(latent_dim)\n",
    "        self.beta = beta\n",
    "    \n",
    "    def forward(self,x):\n",
    "        mu, log_var = self.inferencer(x)\n",
    "        z = mu + torch.exp(0.5*log_var) * torch.randn_like(mu)\n",
    "        x_hat = self.generator(z)\n",
    "        return x_hat, mu, log_var\n",
    "    \n",
    "    def get_loss(self,x,epoch=10): \n",
    "        x_hat, mu, log_var = self.forward(x)\n",
    "        reconstruction_loss = nn.functional.mse_loss(x_hat,x,reduction='mean')\n",
    "        KL_divergence = 0.5 * torch.sum(mu.pow(2) + log_var.exp() - log_var - 1,dim=1).mean()\n",
    "        ELBO = reconstruction_loss + self.beta * KL_divergence        \n",
    "        return ELBO, reconstruction_loss, KL_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training beta = 0.001]\n",
      "Traing NaiveVAE with latent dim: 2\n",
      "Test Loss: Total Loss: 0.2385, Reconstruction Loss: 0.2385, KL Divergence: 0.0158\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004999876022338867,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 40,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f83452c4a24f80ba2758831024ee59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40, Total Loss: 0.0594, Reconstruction Loss: 0.0560, KL Divergence: 3.3386\n",
      "Epoch: 2/40, Total Loss: 0.0530, Reconstruction Loss: 0.0493, KL Divergence: 3.7449\n",
      "Epoch: 3/40, Total Loss: 0.0515, Reconstruction Loss: 0.0475, KL Divergence: 4.0176\n",
      "Epoch: 4/40, Total Loss: 0.0505, Reconstruction Loss: 0.0463, KL Divergence: 4.2014\n",
      "Epoch: 5/40, Total Loss: 0.0498, Reconstruction Loss: 0.0454, KL Divergence: 4.3410\n",
      "Epoch: 6/40, Total Loss: 0.0493, Reconstruction Loss: 0.0448, KL Divergence: 4.4511\n",
      "Epoch: 7/40, Total Loss: 0.0488, Reconstruction Loss: 0.0443, KL Divergence: 4.5416\n",
      "Epoch: 8/40, Total Loss: 0.0485, Reconstruction Loss: 0.0439, KL Divergence: 4.6226\n",
      "Epoch: 9/40, Total Loss: 0.0481, Reconstruction Loss: 0.0435, KL Divergence: 4.6853\n",
      "Epoch: 10/40, Total Loss: 0.0479, Reconstruction Loss: 0.0431, KL Divergence: 4.7413\n",
      "Epoch: 11/40, Total Loss: 0.0476, Reconstruction Loss: 0.0428, KL Divergence: 4.7902\n",
      "Epoch: 12/40, Total Loss: 0.0474, Reconstruction Loss: 0.0425, KL Divergence: 4.8467\n",
      "Epoch: 13/40, Total Loss: 0.0471, Reconstruction Loss: 0.0422, KL Divergence: 4.8832\n",
      "Epoch: 14/40, Total Loss: 0.0469, Reconstruction Loss: 0.0420, KL Divergence: 4.9167\n",
      "Epoch: 15/40, Total Loss: 0.0468, Reconstruction Loss: 0.0418, KL Divergence: 4.9641\n",
      "Epoch: 16/40, Total Loss: 0.0466, Reconstruction Loss: 0.0416, KL Divergence: 4.9961\n",
      "Epoch: 17/40, Total Loss: 0.0464, Reconstruction Loss: 0.0413, KL Divergence: 5.0359\n",
      "Epoch: 18/40, Total Loss: 0.0463, Reconstruction Loss: 0.0412, KL Divergence: 5.0598\n",
      "Epoch: 19/40, Total Loss: 0.0462, Reconstruction Loss: 0.0411, KL Divergence: 5.0943\n",
      "Epoch: 20/40, Total Loss: 0.0460, Reconstruction Loss: 0.0409, KL Divergence: 5.1194\n",
      "Epoch: 21/40, Total Loss: 0.0458, Reconstruction Loss: 0.0407, KL Divergence: 5.1397\n",
      "Epoch: 22/40, Total Loss: 0.0458, Reconstruction Loss: 0.0406, KL Divergence: 5.1706\n",
      "Epoch: 23/40, Total Loss: 0.0456, Reconstruction Loss: 0.0404, KL Divergence: 5.1779\n",
      "Epoch: 24/40, Total Loss: 0.0455, Reconstruction Loss: 0.0403, KL Divergence: 5.2136\n",
      "Epoch: 25/40, Total Loss: 0.0454, Reconstruction Loss: 0.0402, KL Divergence: 5.2262\n",
      "Epoch: 26/40, Total Loss: 0.0453, Reconstruction Loss: 0.0400, KL Divergence: 5.2478\n",
      "Epoch: 27/40, Total Loss: 0.0452, Reconstruction Loss: 0.0399, KL Divergence: 5.2662\n",
      "Epoch: 28/40, Total Loss: 0.0451, Reconstruction Loss: 0.0399, KL Divergence: 5.2794\n",
      "Epoch: 29/40, Total Loss: 0.0451, Reconstruction Loss: 0.0398, KL Divergence: 5.3005\n",
      "Epoch: 30/40, Total Loss: 0.0450, Reconstruction Loss: 0.0396, KL Divergence: 5.3183\n",
      "Epoch: 31/40, Total Loss: 0.0449, Reconstruction Loss: 0.0395, KL Divergence: 5.3322\n",
      "Epoch: 32/40, Total Loss: 0.0448, Reconstruction Loss: 0.0395, KL Divergence: 5.3350\n",
      "Epoch: 33/40, Total Loss: 0.0447, Reconstruction Loss: 0.0394, KL Divergence: 5.3534\n",
      "Epoch: 34/40, Total Loss: 0.0447, Reconstruction Loss: 0.0393, KL Divergence: 5.3633\n",
      "Epoch: 35/40, Total Loss: 0.0447, Reconstruction Loss: 0.0393, KL Divergence: 5.3663\n",
      "Epoch: 36/40, Total Loss: 0.0446, Reconstruction Loss: 0.0392, KL Divergence: 5.3901\n",
      "Epoch: 37/40, Total Loss: 0.0445, Reconstruction Loss: 0.0391, KL Divergence: 5.3978\n",
      "Epoch: 38/40, Total Loss: 0.0444, Reconstruction Loss: 0.0390, KL Divergence: 5.3990\n",
      "Epoch: 39/40, Total Loss: 0.0444, Reconstruction Loss: 0.0390, KL Divergence: 5.4191\n",
      "Epoch: 40/40, Total Loss: 0.0443, Reconstruction Loss: 0.0389, KL Divergence: 5.4236\n",
      "Test Loss: Total Loss: 0.0455, Reconstruction Loss: 0.0400, KL Divergence: 5.4266\n",
      "tensor(0.0426, grad_fn=<MseLossBackward0>)\n",
      "Traing NaiveVAE with latent dim: 5\n",
      "Test Loss: Total Loss: 0.2366, Reconstruction Loss: 0.2365, KL Divergence: 0.0628\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006011247634887695,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 40,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d91b8c9673e94c7fad47a549524a5c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40, Total Loss: 0.0532, Reconstruction Loss: 0.0462, KL Divergence: 7.0242\n",
      "Epoch: 2/40, Total Loss: 0.0449, Reconstruction Loss: 0.0374, KL Divergence: 7.5628\n",
      "Epoch: 3/40, Total Loss: 0.0425, Reconstruction Loss: 0.0346, KL Divergence: 7.8987\n",
      "Epoch: 4/40, Total Loss: 0.0411, Reconstruction Loss: 0.0330, KL Divergence: 8.1494\n",
      "Epoch: 5/40, Total Loss: 0.0403, Reconstruction Loss: 0.0320, KL Divergence: 8.3365\n",
      "Epoch: 6/40, Total Loss: 0.0397, Reconstruction Loss: 0.0312, KL Divergence: 8.4876\n",
      "Epoch: 7/40, Total Loss: 0.0392, Reconstruction Loss: 0.0306, KL Divergence: 8.5978\n",
      "Epoch: 8/40, Total Loss: 0.0388, Reconstruction Loss: 0.0302, KL Divergence: 8.6873\n",
      "Epoch: 9/40, Total Loss: 0.0385, Reconstruction Loss: 0.0298, KL Divergence: 8.7572\n",
      "Epoch: 10/40, Total Loss: 0.0383, Reconstruction Loss: 0.0294, KL Divergence: 8.8215\n",
      "Epoch: 11/40, Total Loss: 0.0380, Reconstruction Loss: 0.0292, KL Divergence: 8.8629\n",
      "Epoch: 12/40, Total Loss: 0.0378, Reconstruction Loss: 0.0289, KL Divergence: 8.9117\n",
      "Epoch: 13/40, Total Loss: 0.0377, Reconstruction Loss: 0.0287, KL Divergence: 8.9887\n",
      "Epoch: 14/40, Total Loss: 0.0375, Reconstruction Loss: 0.0285, KL Divergence: 9.0200\n",
      "Epoch: 15/40, Total Loss: 0.0373, Reconstruction Loss: 0.0283, KL Divergence: 9.0612\n",
      "Epoch: 16/40, Total Loss: 0.0372, Reconstruction Loss: 0.0281, KL Divergence: 9.0920\n",
      "Epoch: 17/40, Total Loss: 0.0371, Reconstruction Loss: 0.0280, KL Divergence: 9.1184\n",
      "Epoch: 18/40, Total Loss: 0.0369, Reconstruction Loss: 0.0278, KL Divergence: 9.1624\n",
      "Epoch: 19/40, Total Loss: 0.0368, Reconstruction Loss: 0.0277, KL Divergence: 9.1674\n",
      "Epoch: 20/40, Total Loss: 0.0368, Reconstruction Loss: 0.0275, KL Divergence: 9.2244\n",
      "Epoch: 21/40, Total Loss: 0.0366, Reconstruction Loss: 0.0274, KL Divergence: 9.2324\n",
      "Epoch: 22/40, Total Loss: 0.0366, Reconstruction Loss: 0.0273, KL Divergence: 9.2636\n",
      "Epoch: 23/40, Total Loss: 0.0365, Reconstruction Loss: 0.0272, KL Divergence: 9.2731\n",
      "Epoch: 24/40, Total Loss: 0.0364, Reconstruction Loss: 0.0271, KL Divergence: 9.3126\n",
      "Epoch: 25/40, Total Loss: 0.0363, Reconstruction Loss: 0.0270, KL Divergence: 9.3401\n",
      "Epoch: 26/40, Total Loss: 0.0363, Reconstruction Loss: 0.0270, KL Divergence: 9.3425\n",
      "Epoch: 27/40, Total Loss: 0.0362, Reconstruction Loss: 0.0269, KL Divergence: 9.3716\n",
      "Epoch: 28/40, Total Loss: 0.0362, Reconstruction Loss: 0.0268, KL Divergence: 9.3758\n",
      "Epoch: 29/40, Total Loss: 0.0361, Reconstruction Loss: 0.0267, KL Divergence: 9.3958\n",
      "Epoch: 30/40, Total Loss: 0.0360, Reconstruction Loss: 0.0267, KL Divergence: 9.3972\n",
      "Epoch: 31/40, Total Loss: 0.0360, Reconstruction Loss: 0.0266, KL Divergence: 9.4414\n",
      "Epoch: 32/40, Total Loss: 0.0360, Reconstruction Loss: 0.0265, KL Divergence: 9.4513\n",
      "Epoch: 33/40, Total Loss: 0.0359, Reconstruction Loss: 0.0264, KL Divergence: 9.4649\n",
      "Epoch: 34/40, Total Loss: 0.0359, Reconstruction Loss: 0.0264, KL Divergence: 9.4784\n",
      "Epoch: 35/40, Total Loss: 0.0358, Reconstruction Loss: 0.0263, KL Divergence: 9.4904\n",
      "Epoch: 36/40, Total Loss: 0.0358, Reconstruction Loss: 0.0263, KL Divergence: 9.5007\n",
      "Epoch: 37/40, Total Loss: 0.0357, Reconstruction Loss: 0.0262, KL Divergence: 9.5199\n",
      "Epoch: 38/40, Total Loss: 0.0357, Reconstruction Loss: 0.0261, KL Divergence: 9.5261\n",
      "Epoch: 39/40, Total Loss: 0.0356, Reconstruction Loss: 0.0261, KL Divergence: 9.5344\n",
      "Epoch: 40/40, Total Loss: 0.0356, Reconstruction Loss: 0.0260, KL Divergence: 9.5684\n",
      "Test Loss: Total Loss: 0.0367, Reconstruction Loss: 0.0273, KL Divergence: 9.4532\n",
      "tensor(0.0303, grad_fn=<MseLossBackward0>)\n",
      "Traing NaiveVAE with latent dim: 10\n",
      "Test Loss: Total Loss: 0.2363, Reconstruction Loss: 0.2362, KL Divergence: 0.0889\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006000995635986328,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 40,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e1b01bdf2f04c859af4a6ede31b7aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40, Total Loss: 0.0509, Reconstruction Loss: 0.0405, KL Divergence: 10.3845\n",
      "Epoch: 2/40, Total Loss: 0.0426, Reconstruction Loss: 0.0315, KL Divergence: 11.1221\n",
      "Epoch: 3/40, Total Loss: 0.0400, Reconstruction Loss: 0.0286, KL Divergence: 11.4089\n",
      "Epoch: 4/40, Total Loss: 0.0385, Reconstruction Loss: 0.0268, KL Divergence: 11.7364\n",
      "Epoch: 5/40, Total Loss: 0.0375, Reconstruction Loss: 0.0255, KL Divergence: 11.9893\n",
      "Epoch: 6/40, Total Loss: 0.0368, Reconstruction Loss: 0.0246, KL Divergence: 12.1890\n",
      "Epoch: 7/40, Total Loss: 0.0364, Reconstruction Loss: 0.0240, KL Divergence: 12.3490\n",
      "Epoch: 8/40, Total Loss: 0.0360, Reconstruction Loss: 0.0235, KL Divergence: 12.4616\n",
      "Epoch: 9/40, Total Loss: 0.0356, Reconstruction Loss: 0.0230, KL Divergence: 12.5618\n",
      "Epoch: 10/40, Total Loss: 0.0354, Reconstruction Loss: 0.0227, KL Divergence: 12.6701\n",
      "Epoch: 11/40, Total Loss: 0.0351, Reconstruction Loss: 0.0224, KL Divergence: 12.7201\n",
      "Epoch: 12/40, Total Loss: 0.0350, Reconstruction Loss: 0.0222, KL Divergence: 12.7829\n",
      "Epoch: 13/40, Total Loss: 0.0348, Reconstruction Loss: 0.0220, KL Divergence: 12.8385\n",
      "Epoch: 14/40, Total Loss: 0.0346, Reconstruction Loss: 0.0217, KL Divergence: 12.8818\n",
      "Epoch: 15/40, Total Loss: 0.0345, Reconstruction Loss: 0.0216, KL Divergence: 12.9409\n",
      "Epoch: 16/40, Total Loss: 0.0344, Reconstruction Loss: 0.0214, KL Divergence: 12.9835\n",
      "Epoch: 17/40, Total Loss: 0.0343, Reconstruction Loss: 0.0213, KL Divergence: 12.9966\n",
      "Epoch: 18/40, Total Loss: 0.0342, Reconstruction Loss: 0.0211, KL Divergence: 13.0397\n",
      "Epoch: 19/40, Total Loss: 0.0341, Reconstruction Loss: 0.0211, KL Divergence: 13.0775\n",
      "Epoch: 20/40, Total Loss: 0.0340, Reconstruction Loss: 0.0209, KL Divergence: 13.1052\n",
      "Epoch: 21/40, Total Loss: 0.0340, Reconstruction Loss: 0.0209, KL Divergence: 13.1110\n",
      "Epoch: 22/40, Total Loss: 0.0339, Reconstruction Loss: 0.0207, KL Divergence: 13.1496\n",
      "Epoch: 23/40, Total Loss: 0.0338, Reconstruction Loss: 0.0206, KL Divergence: 13.1899\n",
      "Epoch: 24/40, Total Loss: 0.0338, Reconstruction Loss: 0.0206, KL Divergence: 13.2084\n",
      "Epoch: 25/40, Total Loss: 0.0337, Reconstruction Loss: 0.0205, KL Divergence: 13.2248\n",
      "Epoch: 26/40, Total Loss: 0.0336, Reconstruction Loss: 0.0204, KL Divergence: 13.2167\n",
      "Epoch: 27/40, Total Loss: 0.0336, Reconstruction Loss: 0.0204, KL Divergence: 13.2221\n",
      "Epoch: 28/40, Total Loss: 0.0336, Reconstruction Loss: 0.0203, KL Divergence: 13.2542\n",
      "Epoch: 29/40, Total Loss: 0.0335, Reconstruction Loss: 0.0203, KL Divergence: 13.2507\n",
      "Epoch: 30/40, Total Loss: 0.0335, Reconstruction Loss: 0.0202, KL Divergence: 13.3015\n",
      "Epoch: 31/40, Total Loss: 0.0334, Reconstruction Loss: 0.0201, KL Divergence: 13.2960\n",
      "Epoch: 32/40, Total Loss: 0.0334, Reconstruction Loss: 0.0201, KL Divergence: 13.3358\n",
      "Epoch: 33/40, Total Loss: 0.0334, Reconstruction Loss: 0.0200, KL Divergence: 13.3343\n",
      "Epoch: 34/40, Total Loss: 0.0333, Reconstruction Loss: 0.0200, KL Divergence: 13.3280\n",
      "Epoch: 35/40, Total Loss: 0.0333, Reconstruction Loss: 0.0199, KL Divergence: 13.3887\n",
      "Epoch: 36/40, Total Loss: 0.0333, Reconstruction Loss: 0.0199, KL Divergence: 13.3488\n",
      "Epoch: 37/40, Total Loss: 0.0332, Reconstruction Loss: 0.0199, KL Divergence: 13.3598\n",
      "Epoch: 38/40, Total Loss: 0.0332, Reconstruction Loss: 0.0198, KL Divergence: 13.3678\n",
      "Epoch: 39/40, Total Loss: 0.0331, Reconstruction Loss: 0.0198, KL Divergence: 13.3744\n",
      "Epoch: 40/40, Total Loss: 0.0332, Reconstruction Loss: 0.0198, KL Divergence: 13.3998\n",
      "Test Loss: Total Loss: 0.0337, Reconstruction Loss: 0.0201, KL Divergence: 13.6122\n",
      "tensor(0.0210, grad_fn=<MseLossBackward0>)\n",
      "Traing NaiveVAE with latent dim: 20\n",
      "Test Loss: Total Loss: 0.2369, Reconstruction Loss: 0.2367, KL Divergence: 0.2156\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0060100555419921875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 40,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f2a15e9c80349f180c52f323b17b922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40, Total Loss: 0.0518, Reconstruction Loss: 0.0392, KL Divergence: 12.5610\n",
      "Epoch: 2/40, Total Loss: 0.0426, Reconstruction Loss: 0.0280, KL Divergence: 14.6650\n",
      "Epoch: 3/40, Total Loss: 0.0410, Reconstruction Loss: 0.0262, KL Divergence: 14.7840\n",
      "Epoch: 4/40, Total Loss: 0.0395, Reconstruction Loss: 0.0248, KL Divergence: 14.6955\n",
      "Epoch: 5/40, Total Loss: 0.0384, Reconstruction Loss: 0.0237, KL Divergence: 14.6967\n",
      "Epoch: 6/40, Total Loss: 0.0377, Reconstruction Loss: 0.0229, KL Divergence: 14.7700\n",
      "Epoch: 7/40, Total Loss: 0.0371, Reconstruction Loss: 0.0223, KL Divergence: 14.8097\n",
      "Epoch: 8/40, Total Loss: 0.0367, Reconstruction Loss: 0.0219, KL Divergence: 14.8699\n",
      "Epoch: 9/40, Total Loss: 0.0364, Reconstruction Loss: 0.0216, KL Divergence: 14.8677\n",
      "Epoch: 10/40, Total Loss: 0.0361, Reconstruction Loss: 0.0212, KL Divergence: 14.9081\n",
      "Epoch: 11/40, Total Loss: 0.0360, Reconstruction Loss: 0.0210, KL Divergence: 14.9591\n",
      "Epoch: 12/40, Total Loss: 0.0358, Reconstruction Loss: 0.0208, KL Divergence: 14.9448\n",
      "Epoch: 13/40, Total Loss: 0.0356, Reconstruction Loss: 0.0206, KL Divergence: 14.9626\n",
      "Epoch: 14/40, Total Loss: 0.0354, Reconstruction Loss: 0.0205, KL Divergence: 14.9586\n",
      "Epoch: 15/40, Total Loss: 0.0354, Reconstruction Loss: 0.0204, KL Divergence: 14.9795\n",
      "Epoch: 16/40, Total Loss: 0.0352, Reconstruction Loss: 0.0202, KL Divergence: 15.0026\n",
      "Epoch: 17/40, Total Loss: 0.0351, Reconstruction Loss: 0.0202, KL Divergence: 14.9467\n",
      "Epoch: 18/40, Total Loss: 0.0350, Reconstruction Loss: 0.0201, KL Divergence: 14.9770\n",
      "Epoch: 19/40, Total Loss: 0.0350, Reconstruction Loss: 0.0200, KL Divergence: 15.0118\n",
      "Epoch: 20/40, Total Loss: 0.0349, Reconstruction Loss: 0.0199, KL Divergence: 15.0028\n",
      "Epoch: 21/40, Total Loss: 0.0348, Reconstruction Loss: 0.0198, KL Divergence: 15.0270\n",
      "Epoch: 22/40, Total Loss: 0.0347, Reconstruction Loss: 0.0197, KL Divergence: 15.0106\n",
      "Epoch: 23/40, Total Loss: 0.0347, Reconstruction Loss: 0.0197, KL Divergence: 15.0008\n",
      "Epoch: 24/40, Total Loss: 0.0346, Reconstruction Loss: 0.0196, KL Divergence: 14.9866\n",
      "Epoch: 25/40, Total Loss: 0.0346, Reconstruction Loss: 0.0196, KL Divergence: 15.0425\n",
      "Epoch: 26/40, Total Loss: 0.0345, Reconstruction Loss: 0.0195, KL Divergence: 14.9659\n",
      "Epoch: 27/40, Total Loss: 0.0345, Reconstruction Loss: 0.0195, KL Divergence: 14.9889\n",
      "Epoch: 28/40, Total Loss: 0.0344, Reconstruction Loss: 0.0194, KL Divergence: 15.0083\n",
      "Epoch: 29/40, Total Loss: 0.0343, Reconstruction Loss: 0.0193, KL Divergence: 14.9870\n",
      "Epoch: 30/40, Total Loss: 0.0343, Reconstruction Loss: 0.0193, KL Divergence: 15.0002\n",
      "Epoch: 31/40, Total Loss: 0.0343, Reconstruction Loss: 0.0193, KL Divergence: 14.9866\n",
      "Epoch: 32/40, Total Loss: 0.0342, Reconstruction Loss: 0.0192, KL Divergence: 14.9868\n",
      "Epoch: 33/40, Total Loss: 0.0342, Reconstruction Loss: 0.0192, KL Divergence: 14.9946\n",
      "Epoch: 34/40, Total Loss: 0.0342, Reconstruction Loss: 0.0192, KL Divergence: 14.9728\n",
      "Epoch: 35/40, Total Loss: 0.0341, Reconstruction Loss: 0.0192, KL Divergence: 14.9586\n",
      "Epoch: 36/40, Total Loss: 0.0341, Reconstruction Loss: 0.0191, KL Divergence: 14.9617\n",
      "Epoch: 37/40, Total Loss: 0.0341, Reconstruction Loss: 0.0191, KL Divergence: 14.9716\n",
      "Epoch: 38/40, Total Loss: 0.0340, Reconstruction Loss: 0.0191, KL Divergence: 14.9646\n",
      "Epoch: 39/40, Total Loss: 0.0340, Reconstruction Loss: 0.0190, KL Divergence: 14.9454\n",
      "Epoch: 40/40, Total Loss: 0.0340, Reconstruction Loss: 0.0190, KL Divergence: 14.9354\n",
      "Test Loss: Total Loss: 0.0344, Reconstruction Loss: 0.0195, KL Divergence: 14.8522\n",
      "tensor(0.0213, grad_fn=<MseLossBackward0>)\n",
      "Traing VAE with latent dim: 2\n",
      "Test Loss: Total Loss: 0.2373, Reconstruction Loss: 0.2373, KL Divergence: 0.0080\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004037380218505859,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 40,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5234b1c31a6549aaafe0b3c28716a4f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40, Total Loss: 0.0799, Reconstruction Loss: 0.0780, KL Divergence: 1.9731\n",
      "Epoch: 2/40, Total Loss: 0.0600, Reconstruction Loss: 0.0572, KL Divergence: 2.8054\n",
      "Epoch: 3/40, Total Loss: 0.0584, Reconstruction Loss: 0.0555, KL Divergence: 2.9466\n",
      "Epoch: 4/40, Total Loss: 0.0574, Reconstruction Loss: 0.0543, KL Divergence: 3.0935\n",
      "Epoch: 5/40, Total Loss: 0.0567, Reconstruction Loss: 0.0535, KL Divergence: 3.2257\n",
      "Epoch: 6/40, Total Loss: 0.0561, Reconstruction Loss: 0.0528, KL Divergence: 3.3236\n",
      "Epoch: 7/40, Total Loss: 0.0558, Reconstruction Loss: 0.0525, KL Divergence: 3.3700\n",
      "Epoch: 8/40, Total Loss: 0.0556, Reconstruction Loss: 0.0522, KL Divergence: 3.4048\n",
      "Epoch: 9/40, Total Loss: 0.0555, Reconstruction Loss: 0.0520, KL Divergence: 3.4315\n",
      "Epoch: 10/40, Total Loss: 0.0553, Reconstruction Loss: 0.0518, KL Divergence: 3.4564\n",
      "Epoch: 11/40, Total Loss: 0.0552, Reconstruction Loss: 0.0517, KL Divergence: 3.4695\n",
      "Epoch: 12/40, Total Loss: 0.0551, Reconstruction Loss: 0.0516, KL Divergence: 3.5191\n",
      "Epoch: 13/40, Total Loss: 0.0550, Reconstruction Loss: 0.0515, KL Divergence: 3.5360\n",
      "Epoch: 14/40, Total Loss: 0.0549, Reconstruction Loss: 0.0514, KL Divergence: 3.5490\n",
      "Epoch: 15/40, Total Loss: 0.0549, Reconstruction Loss: 0.0513, KL Divergence: 3.5515\n",
      "Epoch: 16/40, Total Loss: 0.0548, Reconstruction Loss: 0.0512, KL Divergence: 3.5674\n",
      "Epoch: 17/40, Total Loss: 0.0548, Reconstruction Loss: 0.0512, KL Divergence: 3.5867\n",
      "Epoch: 18/40, Total Loss: 0.0547, Reconstruction Loss: 0.0511, KL Divergence: 3.5855\n",
      "Epoch: 19/40, Total Loss: 0.0547, Reconstruction Loss: 0.0510, KL Divergence: 3.6098\n",
      "Epoch: 20/40, Total Loss: 0.0546, Reconstruction Loss: 0.0510, KL Divergence: 3.6101\n",
      "Epoch: 21/40, Total Loss: 0.0545, Reconstruction Loss: 0.0509, KL Divergence: 3.6150\n",
      "Epoch: 22/40, Total Loss: 0.0545, Reconstruction Loss: 0.0509, KL Divergence: 3.6439\n",
      "Epoch: 23/40, Total Loss: 0.0544, Reconstruction Loss: 0.0508, KL Divergence: 3.6493\n",
      "Epoch: 24/40, Total Loss: 0.0544, Reconstruction Loss: 0.0508, KL Divergence: 3.6654\n",
      "Epoch: 25/40, Total Loss: 0.0544, Reconstruction Loss: 0.0507, KL Divergence: 3.6557\n",
      "Epoch: 26/40, Total Loss: 0.0543, Reconstruction Loss: 0.0507, KL Divergence: 3.6776\n",
      "Epoch: 27/40, Total Loss: 0.0543, Reconstruction Loss: 0.0506, KL Divergence: 3.6992\n",
      "Epoch: 28/40, Total Loss: 0.0542, Reconstruction Loss: 0.0506, KL Divergence: 3.6893\n",
      "Epoch: 29/40, Total Loss: 0.0542, Reconstruction Loss: 0.0505, KL Divergence: 3.7010\n",
      "Epoch: 30/40, Total Loss: 0.0542, Reconstruction Loss: 0.0505, KL Divergence: 3.7269\n",
      "Epoch: 31/40, Total Loss: 0.0542, Reconstruction Loss: 0.0505, KL Divergence: 3.7190\n",
      "Epoch: 32/40, Total Loss: 0.0541, Reconstruction Loss: 0.0504, KL Divergence: 3.7231\n",
      "Epoch: 33/40, Total Loss: 0.0541, Reconstruction Loss: 0.0504, KL Divergence: 3.7255\n",
      "Epoch: 34/40, Total Loss: 0.0541, Reconstruction Loss: 0.0504, KL Divergence: 3.7236\n",
      "Epoch: 35/40, Total Loss: 0.0541, Reconstruction Loss: 0.0503, KL Divergence: 3.7426\n",
      "Epoch: 36/40, Total Loss: 0.0541, Reconstruction Loss: 0.0503, KL Divergence: 3.7454\n",
      "Epoch: 37/40, Total Loss: 0.0540, Reconstruction Loss: 0.0503, KL Divergence: 3.7519\n",
      "Epoch: 38/40, Total Loss: 0.0540, Reconstruction Loss: 0.0502, KL Divergence: 3.7848\n",
      "Epoch: 39/40, Total Loss: 0.0540, Reconstruction Loss: 0.0502, KL Divergence: 3.7836\n",
      "Epoch: 40/40, Total Loss: 0.0539, Reconstruction Loss: 0.0502, KL Divergence: 3.7842\n",
      "Test Loss: Total Loss: 0.0541, Reconstruction Loss: 0.0506, KL Divergence: 3.5639\n",
      "tensor(0.0513, grad_fn=<MseLossBackward0>)\n",
      "Traing VAE with latent dim: 5\n",
      "Test Loss: Total Loss: 0.2276, Reconstruction Loss: 0.2276, KL Divergence: 0.0210\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007000446319580078,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 40,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b6bc4d37cc48f4b1d6b06971b045b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40, Total Loss: 0.0798, Reconstruction Loss: 0.0771, KL Divergence: 2.6723\n",
      "Epoch: 2/40, Total Loss: 0.0539, Reconstruction Loss: 0.0479, KL Divergence: 5.9147\n",
      "Epoch: 3/40, Total Loss: 0.0506, Reconstruction Loss: 0.0441, KL Divergence: 6.5481\n",
      "Epoch: 4/40, Total Loss: 0.0495, Reconstruction Loss: 0.0427, KL Divergence: 6.8054\n",
      "Epoch: 5/40, Total Loss: 0.0489, Reconstruction Loss: 0.0419, KL Divergence: 6.9612\n",
      "Epoch: 6/40, Total Loss: 0.0483, Reconstruction Loss: 0.0412, KL Divergence: 7.0930\n",
      "Epoch: 7/40, Total Loss: 0.0479, Reconstruction Loss: 0.0407, KL Divergence: 7.2003\n",
      "Epoch: 8/40, Total Loss: 0.0476, Reconstruction Loss: 0.0403, KL Divergence: 7.2665\n",
      "Epoch: 9/40, Total Loss: 0.0474, Reconstruction Loss: 0.0400, KL Divergence: 7.3708\n",
      "Epoch: 10/40, Total Loss: 0.0471, Reconstruction Loss: 0.0397, KL Divergence: 7.4376\n",
      "Epoch: 11/40, Total Loss: 0.0470, Reconstruction Loss: 0.0395, KL Divergence: 7.4724\n",
      "Epoch: 12/40, Total Loss: 0.0468, Reconstruction Loss: 0.0393, KL Divergence: 7.5191\n",
      "Epoch: 13/40, Total Loss: 0.0467, Reconstruction Loss: 0.0392, KL Divergence: 7.5330\n",
      "Epoch: 14/40, Total Loss: 0.0466, Reconstruction Loss: 0.0390, KL Divergence: 7.5594\n",
      "Epoch: 15/40, Total Loss: 0.0465, Reconstruction Loss: 0.0389, KL Divergence: 7.5943\n",
      "Epoch: 16/40, Total Loss: 0.0464, Reconstruction Loss: 0.0388, KL Divergence: 7.5826\n",
      "Epoch: 17/40, Total Loss: 0.0463, Reconstruction Loss: 0.0387, KL Divergence: 7.6066\n",
      "Epoch: 18/40, Total Loss: 0.0462, Reconstruction Loss: 0.0386, KL Divergence: 7.6232\n",
      "Epoch: 19/40, Total Loss: 0.0462, Reconstruction Loss: 0.0385, KL Divergence: 7.6538\n",
      "Epoch: 20/40, Total Loss: 0.0461, Reconstruction Loss: 0.0384, KL Divergence: 7.6631\n",
      "Epoch: 21/40, Total Loss: 0.0460, Reconstruction Loss: 0.0384, KL Divergence: 7.6846\n",
      "Epoch: 22/40, Total Loss: 0.0460, Reconstruction Loss: 0.0383, KL Divergence: 7.6850\n",
      "Epoch: 23/40, Total Loss: 0.0459, Reconstruction Loss: 0.0382, KL Divergence: 7.7069\n",
      "Epoch: 24/40, Total Loss: 0.0459, Reconstruction Loss: 0.0381, KL Divergence: 7.7288\n",
      "Epoch: 25/40, Total Loss: 0.0458, Reconstruction Loss: 0.0381, KL Divergence: 7.7375\n",
      "Epoch: 26/40, Total Loss: 0.0458, Reconstruction Loss: 0.0380, KL Divergence: 7.7357\n",
      "Epoch: 27/40, Total Loss: 0.0457, Reconstruction Loss: 0.0380, KL Divergence: 7.7513\n",
      "Epoch: 28/40, Total Loss: 0.0457, Reconstruction Loss: 0.0379, KL Divergence: 7.7439\n",
      "Epoch: 29/40, Total Loss: 0.0456, Reconstruction Loss: 0.0379, KL Divergence: 7.7571\n",
      "Epoch: 30/40, Total Loss: 0.0456, Reconstruction Loss: 0.0378, KL Divergence: 7.7806\n",
      "Epoch: 31/40, Total Loss: 0.0456, Reconstruction Loss: 0.0378, KL Divergence: 7.7885\n",
      "Epoch: 32/40, Total Loss: 0.0456, Reconstruction Loss: 0.0378, KL Divergence: 7.7959\n",
      "Epoch: 33/40, Total Loss: 0.0455, Reconstruction Loss: 0.0377, KL Divergence: 7.8018\n",
      "Epoch: 34/40, Total Loss: 0.0455, Reconstruction Loss: 0.0377, KL Divergence: 7.8052\n",
      "Epoch: 35/40, Total Loss: 0.0455, Reconstruction Loss: 0.0377, KL Divergence: 7.8104\n",
      "Epoch: 36/40, Total Loss: 0.0455, Reconstruction Loss: 0.0376, KL Divergence: 7.8169\n",
      "Epoch: 37/40, Total Loss: 0.0454, Reconstruction Loss: 0.0376, KL Divergence: 7.8167\n",
      "Epoch: 38/40, Total Loss: 0.0454, Reconstruction Loss: 0.0376, KL Divergence: 7.8213\n",
      "Epoch: 39/40, Total Loss: 0.0454, Reconstruction Loss: 0.0376, KL Divergence: 7.8069\n",
      "Epoch: 40/40, Total Loss: 0.0453, Reconstruction Loss: 0.0375, KL Divergence: 7.8333\n",
      "Test Loss: Total Loss: 0.0454, Reconstruction Loss: 0.0377, KL Divergence: 7.6692\n",
      "tensor(0.0396, grad_fn=<MseLossBackward0>)\n",
      "Traing VAE with latent dim: 10\n",
      "Test Loss: Total Loss: 0.2200, Reconstruction Loss: 0.2199, KL Divergence: 0.0508\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008013725280761719,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 40,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a379e9bd2fb84123970495b115e6a275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40, Total Loss: 0.0809, Reconstruction Loss: 0.0782, KL Divergence: 2.7409\n",
      "Epoch: 2/40, Total Loss: 0.0540, Reconstruction Loss: 0.0469, KL Divergence: 7.0794\n",
      "Epoch: 3/40, Total Loss: 0.0486, Reconstruction Loss: 0.0396, KL Divergence: 8.9608\n",
      "Epoch: 4/40, Total Loss: 0.0465, Reconstruction Loss: 0.0368, KL Divergence: 9.6510\n",
      "Epoch: 5/40, Total Loss: 0.0453, Reconstruction Loss: 0.0352, KL Divergence: 10.0927\n",
      "Epoch: 6/40, Total Loss: 0.0446, Reconstruction Loss: 0.0342, KL Divergence: 10.3678\n",
      "Epoch: 7/40, Total Loss: 0.0440, Reconstruction Loss: 0.0335, KL Divergence: 10.5512\n",
      "Epoch: 8/40, Total Loss: 0.0437, Reconstruction Loss: 0.0329, KL Divergence: 10.7402\n",
      "Epoch: 9/40, Total Loss: 0.0434, Reconstruction Loss: 0.0326, KL Divergence: 10.8218\n",
      "Epoch: 10/40, Total Loss: 0.0432, Reconstruction Loss: 0.0323, KL Divergence: 10.9062\n",
      "Epoch: 11/40, Total Loss: 0.0430, Reconstruction Loss: 0.0320, KL Divergence: 10.9567\n",
      "Epoch: 12/40, Total Loss: 0.0428, Reconstruction Loss: 0.0318, KL Divergence: 11.0369\n",
      "Epoch: 13/40, Total Loss: 0.0427, Reconstruction Loss: 0.0316, KL Divergence: 11.0695\n",
      "Epoch: 14/40, Total Loss: 0.0426, Reconstruction Loss: 0.0314, KL Divergence: 11.1227\n",
      "Epoch: 15/40, Total Loss: 0.0425, Reconstruction Loss: 0.0313, KL Divergence: 11.1818\n",
      "Epoch: 16/40, Total Loss: 0.0424, Reconstruction Loss: 0.0312, KL Divergence: 11.1878\n",
      "Epoch: 17/40, Total Loss: 0.0423, Reconstruction Loss: 0.0310, KL Divergence: 11.2176\n",
      "Epoch: 18/40, Total Loss: 0.0422, Reconstruction Loss: 0.0309, KL Divergence: 11.2888\n",
      "Epoch: 19/40, Total Loss: 0.0422, Reconstruction Loss: 0.0308, KL Divergence: 11.3067\n",
      "Epoch: 20/40, Total Loss: 0.0421, Reconstruction Loss: 0.0307, KL Divergence: 11.3518\n",
      "Epoch: 21/40, Total Loss: 0.0420, Reconstruction Loss: 0.0306, KL Divergence: 11.3777\n",
      "Epoch: 22/40, Total Loss: 0.0420, Reconstruction Loss: 0.0305, KL Divergence: 11.4276\n",
      "Epoch: 23/40, Total Loss: 0.0419, Reconstruction Loss: 0.0304, KL Divergence: 11.4353\n",
      "Epoch: 24/40, Total Loss: 0.0418, Reconstruction Loss: 0.0304, KL Divergence: 11.4486\n",
      "Epoch: 25/40, Total Loss: 0.0418, Reconstruction Loss: 0.0303, KL Divergence: 11.4970\n",
      "Epoch: 26/40, Total Loss: 0.0418, Reconstruction Loss: 0.0303, KL Divergence: 11.5187\n",
      "Epoch: 27/40, Total Loss: 0.0417, Reconstruction Loss: 0.0302, KL Divergence: 11.4948\n",
      "Epoch: 28/40, Total Loss: 0.0417, Reconstruction Loss: 0.0301, KL Divergence: 11.5229\n",
      "Epoch: 29/40, Total Loss: 0.0416, Reconstruction Loss: 0.0301, KL Divergence: 11.5314\n",
      "Epoch: 30/40, Total Loss: 0.0416, Reconstruction Loss: 0.0300, KL Divergence: 11.5481\n",
      "Epoch: 31/40, Total Loss: 0.0416, Reconstruction Loss: 0.0300, KL Divergence: 11.5667\n",
      "Epoch: 32/40, Total Loss: 0.0416, Reconstruction Loss: 0.0300, KL Divergence: 11.5926\n",
      "Epoch: 33/40, Total Loss: 0.0415, Reconstruction Loss: 0.0300, KL Divergence: 11.5789\n",
      "Epoch: 34/40, Total Loss: 0.0415, Reconstruction Loss: 0.0299, KL Divergence: 11.5865\n",
      "Epoch: 35/40, Total Loss: 0.0415, Reconstruction Loss: 0.0299, KL Divergence: 11.5911\n",
      "Epoch: 36/40, Total Loss: 0.0414, Reconstruction Loss: 0.0298, KL Divergence: 11.6078\n",
      "Epoch: 37/40, Total Loss: 0.0414, Reconstruction Loss: 0.0298, KL Divergence: 11.6091\n",
      "Epoch: 38/40, Total Loss: 0.0414, Reconstruction Loss: 0.0298, KL Divergence: 11.6152\n",
      "Epoch: 39/40, Total Loss: 0.0414, Reconstruction Loss: 0.0297, KL Divergence: 11.6224\n",
      "Epoch: 40/40, Total Loss: 0.0413, Reconstruction Loss: 0.0297, KL Divergence: 11.6370\n",
      "Test Loss: Total Loss: 0.0413, Reconstruction Loss: 0.0297, KL Divergence: 11.5957\n",
      "tensor(0.0316, grad_fn=<MseLossBackward0>)\n",
      "Traing VAE with latent dim: 20\n",
      "Test Loss: Total Loss: 0.2596, Reconstruction Loss: 0.2594, KL Divergence: 0.1317\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004957437515258789,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 40,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d57dd4240eff46f6adb451eab330f602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40, Total Loss: 0.0830, Reconstruction Loss: 0.0790, KL Divergence: 3.9274\n",
      "Epoch: 2/40, Total Loss: 0.0529, Reconstruction Loss: 0.0444, KL Divergence: 8.5622\n",
      "Epoch: 3/40, Total Loss: 0.0475, Reconstruction Loss: 0.0373, KL Divergence: 10.2158\n",
      "Epoch: 4/40, Total Loss: 0.0455, Reconstruction Loss: 0.0343, KL Divergence: 11.2694\n",
      "Epoch: 5/40, Total Loss: 0.0444, Reconstruction Loss: 0.0326, KL Divergence: 11.8189\n",
      "Epoch: 6/40, Total Loss: 0.0437, Reconstruction Loss: 0.0315, KL Divergence: 12.1803\n",
      "Epoch: 7/40, Total Loss: 0.0432, Reconstruction Loss: 0.0308, KL Divergence: 12.4238\n",
      "Epoch: 8/40, Total Loss: 0.0428, Reconstruction Loss: 0.0302, KL Divergence: 12.5605\n",
      "Epoch: 9/40, Total Loss: 0.0426, Reconstruction Loss: 0.0298, KL Divergence: 12.7300\n",
      "Epoch: 10/40, Total Loss: 0.0423, Reconstruction Loss: 0.0295, KL Divergence: 12.8133\n",
      "Epoch: 11/40, Total Loss: 0.0422, Reconstruction Loss: 0.0293, KL Divergence: 12.9113\n",
      "Epoch: 12/40, Total Loss: 0.0420, Reconstruction Loss: 0.0290, KL Divergence: 12.9659\n",
      "Epoch: 13/40, Total Loss: 0.0419, Reconstruction Loss: 0.0289, KL Divergence: 13.0236\n",
      "Epoch: 14/40, Total Loss: 0.0418, Reconstruction Loss: 0.0288, KL Divergence: 13.0446\n",
      "Epoch: 15/40, Total Loss: 0.0417, Reconstruction Loss: 0.0286, KL Divergence: 13.0888\n",
      "Epoch: 16/40, Total Loss: 0.0416, Reconstruction Loss: 0.0285, KL Divergence: 13.1251\n",
      "Epoch: 17/40, Total Loss: 0.0415, Reconstruction Loss: 0.0284, KL Divergence: 13.1730\n",
      "Epoch: 18/40, Total Loss: 0.0414, Reconstruction Loss: 0.0282, KL Divergence: 13.2055\n",
      "Epoch: 19/40, Total Loss: 0.0413, Reconstruction Loss: 0.0281, KL Divergence: 13.2205\n",
      "Epoch: 20/40, Total Loss: 0.0413, Reconstruction Loss: 0.0280, KL Divergence: 13.2240\n",
      "Epoch: 21/40, Total Loss: 0.0412, Reconstruction Loss: 0.0279, KL Divergence: 13.2492\n",
      "Epoch: 22/40, Total Loss: 0.0411, Reconstruction Loss: 0.0278, KL Divergence: 13.2764\n",
      "Epoch: 23/40, Total Loss: 0.0411, Reconstruction Loss: 0.0277, KL Divergence: 13.3341\n",
      "Epoch: 24/40, Total Loss: 0.0410, Reconstruction Loss: 0.0277, KL Divergence: 13.3026\n",
      "Epoch: 25/40, Total Loss: 0.0409, Reconstruction Loss: 0.0276, KL Divergence: 13.3490\n",
      "Epoch: 26/40, Total Loss: 0.0409, Reconstruction Loss: 0.0275, KL Divergence: 13.3909\n",
      "Epoch: 27/40, Total Loss: 0.0409, Reconstruction Loss: 0.0275, KL Divergence: 13.3936\n",
      "Epoch: 28/40, Total Loss: 0.0408, Reconstruction Loss: 0.0275, KL Divergence: 13.3792\n",
      "Epoch: 29/40, Total Loss: 0.0408, Reconstruction Loss: 0.0274, KL Divergence: 13.4197\n",
      "Epoch: 30/40, Total Loss: 0.0408, Reconstruction Loss: 0.0273, KL Divergence: 13.4232\n",
      "Epoch: 31/40, Total Loss: 0.0407, Reconstruction Loss: 0.0273, KL Divergence: 13.3953\n",
      "Epoch: 32/40, Total Loss: 0.0407, Reconstruction Loss: 0.0272, KL Divergence: 13.4323\n",
      "Epoch: 33/40, Total Loss: 0.0406, Reconstruction Loss: 0.0272, KL Divergence: 13.4239\n",
      "Epoch: 34/40, Total Loss: 0.0406, Reconstruction Loss: 0.0272, KL Divergence: 13.4367\n",
      "Epoch: 35/40, Total Loss: 0.0406, Reconstruction Loss: 0.0271, KL Divergence: 13.4783\n",
      "Epoch: 36/40, Total Loss: 0.0406, Reconstruction Loss: 0.0271, KL Divergence: 13.4677\n",
      "Epoch: 37/40, Total Loss: 0.0405, Reconstruction Loss: 0.0271, KL Divergence: 13.4586\n",
      "Epoch: 38/40, Total Loss: 0.0405, Reconstruction Loss: 0.0270, KL Divergence: 13.4686\n",
      "Epoch: 39/40, Total Loss: 0.0404, Reconstruction Loss: 0.0270, KL Divergence: 13.4653\n",
      "Epoch: 40/40, Total Loss: 0.0405, Reconstruction Loss: 0.0270, KL Divergence: 13.4744\n",
      "Test Loss: Total Loss: 0.0404, Reconstruction Loss: 0.0272, KL Divergence: 13.2092\n",
      "tensor(0.0284, grad_fn=<MseLossBackward0>)\n",
      "[Training beta = 0.002]\n",
      "Traing NaiveVAE with latent dim: 2\n",
      "Test Loss: Total Loss: 0.2345, Reconstruction Loss: 0.2345, KL Divergence: 0.0146\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0050084590911865234,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 40,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb0f4e910cb48238dd17fcd55ca5380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40, Total Loss: 0.0617, Reconstruction Loss: 0.0573, KL Divergence: 2.2130\n",
      "Epoch: 2/40, Total Loss: 0.0567, Reconstruction Loss: 0.0511, KL Divergence: 2.7775\n",
      "Epoch: 3/40, Total Loss: 0.0554, Reconstruction Loss: 0.0494, KL Divergence: 3.0189\n",
      "Epoch: 4/40, Total Loss: 0.0548, Reconstruction Loss: 0.0484, KL Divergence: 3.1782\n",
      "Epoch: 5/40, Total Loss: 0.0541, Reconstruction Loss: 0.0475, KL Divergence: 3.2962\n",
      "Epoch: 6/40, Total Loss: 0.0537, Reconstruction Loss: 0.0468, KL Divergence: 3.4118\n",
      "Epoch: 7/40, Total Loss: 0.0532, Reconstruction Loss: 0.0462, KL Divergence: 3.4908\n",
      "Epoch: 8/40, Total Loss: 0.0529, Reconstruction Loss: 0.0458, KL Divergence: 3.5667\n",
      "Epoch: 9/40, Total Loss: 0.0526, Reconstruction Loss: 0.0454, KL Divergence: 3.6120\n",
      "Epoch: 10/40, Total Loss: 0.0524, Reconstruction Loss: 0.0451, KL Divergence: 3.6693\n",
      "Epoch: 11/40, Total Loss: 0.0522, Reconstruction Loss: 0.0448, KL Divergence: 3.7040\n",
      "Epoch: 12/40, Total Loss: 0.0520, Reconstruction Loss: 0.0445, KL Divergence: 3.7425\n",
      "Epoch: 13/40, Total Loss: 0.0519, Reconstruction Loss: 0.0443, KL Divergence: 3.7685\n",
      "Epoch: 14/40, Total Loss: 0.0518, Reconstruction Loss: 0.0441, KL Divergence: 3.8087\n",
      "Epoch: 15/40, Total Loss: 0.0517, Reconstruction Loss: 0.0440, KL Divergence: 3.8424\n",
      "Epoch: 16/40, Total Loss: 0.0516, Reconstruction Loss: 0.0438, KL Divergence: 3.8614\n",
      "Epoch: 17/40, Total Loss: 0.0514, Reconstruction Loss: 0.0437, KL Divergence: 3.8879\n",
      "Epoch: 18/40, Total Loss: 0.0513, Reconstruction Loss: 0.0435, KL Divergence: 3.8992\n",
      "Epoch: 19/40, Total Loss: 0.0513, Reconstruction Loss: 0.0435, KL Divergence: 3.9156\n",
      "Epoch: 20/40, Total Loss: 0.0512, Reconstruction Loss: 0.0433, KL Divergence: 3.9356\n",
      "Epoch: 21/40, Total Loss: 0.0511, Reconstruction Loss: 0.0432, KL Divergence: 3.9508\n",
      "Epoch: 22/40, Total Loss: 0.0510, Reconstruction Loss: 0.0431, KL Divergence: 3.9696\n",
      "Epoch: 23/40, Total Loss: 0.0510, Reconstruction Loss: 0.0431, KL Divergence: 3.9849\n",
      "Epoch: 24/40, Total Loss: 0.0509, Reconstruction Loss: 0.0429, KL Divergence: 4.0067\n",
      "Epoch: 25/40, Total Loss: 0.0508, Reconstruction Loss: 0.0428, KL Divergence: 4.0076\n",
      "Epoch: 26/40, Total Loss: 0.0508, Reconstruction Loss: 0.0428, KL Divergence: 4.0227\n",
      "Epoch: 27/40, Total Loss: 0.0508, Reconstruction Loss: 0.0427, KL Divergence: 4.0405\n",
      "Epoch: 28/40, Total Loss: 0.0507, Reconstruction Loss: 0.0426, KL Divergence: 4.0603\n",
      "Epoch: 29/40, Total Loss: 0.0506, Reconstruction Loss: 0.0425, KL Divergence: 4.0588\n",
      "Epoch: 30/40, Total Loss: 0.0506, Reconstruction Loss: 0.0425, KL Divergence: 4.0762\n",
      "Epoch: 31/40, Total Loss: 0.0505, Reconstruction Loss: 0.0424, KL Divergence: 4.0833\n",
      "Epoch: 32/40, Total Loss: 0.0505, Reconstruction Loss: 0.0423, KL Divergence: 4.0994\n",
      "Epoch: 33/40, Total Loss: 0.0504, Reconstruction Loss: 0.0422, KL Divergence: 4.0934\n",
      "Epoch: 34/40, Total Loss: 0.0504, Reconstruction Loss: 0.0422, KL Divergence: 4.1078\n",
      "Epoch: 35/40, Total Loss: 0.0504, Reconstruction Loss: 0.0421, KL Divergence: 4.1125\n",
      "Epoch: 36/40, Total Loss: 0.0503, Reconstruction Loss: 0.0421, KL Divergence: 4.1297\n",
      "Epoch: 37/40, Total Loss: 0.0503, Reconstruction Loss: 0.0420, KL Divergence: 4.1331\n",
      "Epoch: 38/40, Total Loss: 0.0503, Reconstruction Loss: 0.0420, KL Divergence: 4.1485\n",
      "Epoch: 39/40, Total Loss: 0.0502, Reconstruction Loss: 0.0419, KL Divergence: 4.1518\n",
      "Epoch: 40/40, Total Loss: 0.0502, Reconstruction Loss: 0.0419, KL Divergence: 4.1592\n",
      "Test Loss: Total Loss: 0.0510, Reconstruction Loss: 0.0426, KL Divergence: 4.1854\n",
      "tensor(0.0453, grad_fn=<MseLossBackward0>)\n",
      "Traing NaiveVAE with latent dim: 5\n",
      "Test Loss: Total Loss: 0.2361, Reconstruction Loss: 0.2360, KL Divergence: 0.0560\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005000114440917969,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 40,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "882f5eb1338b47fdaff23221b8eda466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40, Total Loss: 0.0589, Reconstruction Loss: 0.0498, KL Divergence: 4.5186\n",
      "Epoch: 2/40, Total Loss: 0.0519, Reconstruction Loss: 0.0418, KL Divergence: 5.0483\n",
      "Epoch: 3/40, Total Loss: 0.0500, Reconstruction Loss: 0.0390, KL Divergence: 5.4783\n",
      "Epoch: 4/40, Total Loss: 0.0490, Reconstruction Loss: 0.0375, KL Divergence: 5.7331\n",
      "Epoch: 5/40, Total Loss: 0.0483, Reconstruction Loss: 0.0365, KL Divergence: 5.8984\n",
      "Epoch: 6/40, Total Loss: 0.0479, Reconstruction Loss: 0.0358, KL Divergence: 6.0395\n",
      "Epoch: 7/40, Total Loss: 0.0475, Reconstruction Loss: 0.0352, KL Divergence: 6.1443\n",
      "Epoch: 8/40, Total Loss: 0.0472, Reconstruction Loss: 0.0347, KL Divergence: 6.2274\n",
      "Epoch: 9/40, Total Loss: 0.0470, Reconstruction Loss: 0.0344, KL Divergence: 6.3044\n",
      "Epoch: 10/40, Total Loss: 0.0468, Reconstruction Loss: 0.0340, KL Divergence: 6.3722\n",
      "Epoch: 11/40, Total Loss: 0.0466, Reconstruction Loss: 0.0338, KL Divergence: 6.4204\n",
      "Epoch: 12/40, Total Loss: 0.0465, Reconstruction Loss: 0.0335, KL Divergence: 6.4831\n",
      "Epoch: 13/40, Total Loss: 0.0463, Reconstruction Loss: 0.0333, KL Divergence: 6.5150\n",
      "Epoch: 14/40, Total Loss: 0.0462, Reconstruction Loss: 0.0331, KL Divergence: 6.5559\n",
      "Epoch: 15/40, Total Loss: 0.0461, Reconstruction Loss: 0.0329, KL Divergence: 6.5857\n",
      "Epoch: 16/40, Total Loss: 0.0459, Reconstruction Loss: 0.0327, KL Divergence: 6.6121\n",
      "Epoch: 17/40, Total Loss: 0.0459, Reconstruction Loss: 0.0325, KL Divergence: 6.6583\n",
      "Epoch: 18/40, Total Loss: 0.0458, Reconstruction Loss: 0.0324, KL Divergence: 6.6743\n",
      "Epoch: 19/40, Total Loss: 0.0457, Reconstruction Loss: 0.0323, KL Divergence: 6.7073\n",
      "Epoch: 20/40, Total Loss: 0.0456, Reconstruction Loss: 0.0322, KL Divergence: 6.7252\n",
      "Epoch: 21/40, Total Loss: 0.0455, Reconstruction Loss: 0.0320, KL Divergence: 6.7578\n",
      "Epoch: 22/40, Total Loss: 0.0454, Reconstruction Loss: 0.0319, KL Divergence: 6.7668\n",
      "Epoch: 23/40, Total Loss: 0.0454, Reconstruction Loss: 0.0318, KL Divergence: 6.8028\n",
      "Epoch: 24/40, Total Loss: 0.0454, Reconstruction Loss: 0.0317, KL Divergence: 6.8107\n",
      "Epoch: 25/40, Total Loss: 0.0453, Reconstruction Loss: 0.0316, KL Divergence: 6.8335\n",
      "Epoch: 26/40, Total Loss: 0.0452, Reconstruction Loss: 0.0315, KL Divergence: 6.8402\n",
      "Epoch: 27/40, Total Loss: 0.0452, Reconstruction Loss: 0.0315, KL Divergence: 6.8604\n",
      "Epoch: 28/40, Total Loss: 0.0451, Reconstruction Loss: 0.0314, KL Divergence: 6.8834\n",
      "Epoch: 29/40, Total Loss: 0.0451, Reconstruction Loss: 0.0313, KL Divergence: 6.8900\n",
      "Epoch: 30/40, Total Loss: 0.0451, Reconstruction Loss: 0.0312, KL Divergence: 6.9203\n",
      "Epoch: 31/40, Total Loss: 0.0450, Reconstruction Loss: 0.0312, KL Divergence: 6.9264\n",
      "Epoch: 32/40, Total Loss: 0.0450, Reconstruction Loss: 0.0311, KL Divergence: 6.9331\n",
      "Epoch: 33/40, Total Loss: 0.0449, Reconstruction Loss: 0.0310, KL Divergence: 6.9406\n",
      "Epoch: 34/40, Total Loss: 0.0449, Reconstruction Loss: 0.0310, KL Divergence: 6.9433\n",
      "Epoch: 35/40, Total Loss: 0.0449, Reconstruction Loss: 0.0309, KL Divergence: 6.9648\n",
      "Epoch: 36/40, Total Loss: 0.0448, Reconstruction Loss: 0.0308, KL Divergence: 6.9775\n",
      "Epoch: 37/40, Total Loss: 0.0448, Reconstruction Loss: 0.0308, KL Divergence: 6.9814\n",
      "Epoch: 38/40, Total Loss: 0.0448, Reconstruction Loss: 0.0307, KL Divergence: 7.0047\n",
      "Epoch: 39/40, Total Loss: 0.0447, Reconstruction Loss: 0.0307, KL Divergence: 7.0142\n",
      "Epoch: 40/40, Total Loss: 0.0447, Reconstruction Loss: 0.0306, KL Divergence: 7.0265\n",
      "Test Loss: Total Loss: 0.0451, Reconstruction Loss: 0.0309, KL Divergence: 7.0601\n",
      "tensor(0.0349, grad_fn=<MseLossBackward0>)\n",
      "Traing NaiveVAE with latent dim: 10\n",
      "Test Loss: Total Loss: 0.2358, Reconstruction Loss: 0.2356, KL Divergence: 0.0927\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005570650100708008,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 40,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a1c072ef064e02938b6db34785cc9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40, Total Loss: 0.0590, Reconstruction Loss: 0.0467, KL Divergence: 6.1483\n",
      "Epoch: 2/40, Total Loss: 0.0520, Reconstruction Loss: 0.0388, KL Divergence: 6.6356\n",
      "Epoch: 3/40, Total Loss: 0.0498, Reconstruction Loss: 0.0359, KL Divergence: 6.9939\n",
      "Epoch: 4/40, Total Loss: 0.0486, Reconstruction Loss: 0.0340, KL Divergence: 7.2803\n",
      "Epoch: 5/40, Total Loss: 0.0478, Reconstruction Loss: 0.0328, KL Divergence: 7.5124\n",
      "Epoch: 6/40, Total Loss: 0.0473, Reconstruction Loss: 0.0319, KL Divergence: 7.6778\n",
      "Epoch: 7/40, Total Loss: 0.0469, Reconstruction Loss: 0.0313, KL Divergence: 7.8091\n",
      "Epoch: 8/40, Total Loss: 0.0466, Reconstruction Loss: 0.0307, KL Divergence: 7.9200\n",
      "Epoch: 9/40, Total Loss: 0.0463, Reconstruction Loss: 0.0303, KL Divergence: 7.9838\n",
      "Epoch: 10/40, Total Loss: 0.0461, Reconstruction Loss: 0.0299, KL Divergence: 8.0701\n",
      "Epoch: 11/40, Total Loss: 0.0460, Reconstruction Loss: 0.0297, KL Divergence: 8.1437\n",
      "Epoch: 12/40, Total Loss: 0.0458, Reconstruction Loss: 0.0294, KL Divergence: 8.1933\n",
      "Epoch: 13/40, Total Loss: 0.0457, Reconstruction Loss: 0.0292, KL Divergence: 8.2612\n",
      "Epoch: 14/40, Total Loss: 0.0455, Reconstruction Loss: 0.0290, KL Divergence: 8.2750\n",
      "Epoch: 15/40, Total Loss: 0.0455, Reconstruction Loss: 0.0288, KL Divergence: 8.3418\n",
      "Epoch: 16/40, Total Loss: 0.0453, Reconstruction Loss: 0.0286, KL Divergence: 8.3581\n",
      "Epoch: 17/40, Total Loss: 0.0453, Reconstruction Loss: 0.0285, KL Divergence: 8.3754\n",
      "Epoch: 18/40, Total Loss: 0.0452, Reconstruction Loss: 0.0284, KL Divergence: 8.4223\n",
      "Epoch: 19/40, Total Loss: 0.0451, Reconstruction Loss: 0.0282, KL Divergence: 8.4398\n",
      "Epoch: 20/40, Total Loss: 0.0451, Reconstruction Loss: 0.0281, KL Divergence: 8.4610\n",
      "Epoch: 21/40, Total Loss: 0.0450, Reconstruction Loss: 0.0280, KL Divergence: 8.4777\n",
      "Epoch: 22/40, Total Loss: 0.0450, Reconstruction Loss: 0.0279, KL Divergence: 8.5020\n",
      "Epoch: 23/40, Total Loss: 0.0449, Reconstruction Loss: 0.0279, KL Divergence: 8.5064\n",
      "Epoch: 24/40, Total Loss: 0.0448, Reconstruction Loss: 0.0278, KL Divergence: 8.4932\n",
      "Epoch: 25/40, Total Loss: 0.0448, Reconstruction Loss: 0.0277, KL Divergence: 8.5454\n",
      "Epoch: 26/40, Total Loss: 0.0447, Reconstruction Loss: 0.0276, KL Divergence: 8.5688\n",
      "Epoch: 27/40, Total Loss: 0.0447, Reconstruction Loss: 0.0276, KL Divergence: 8.5814\n",
      "Epoch: 28/40, Total Loss: 0.0447, Reconstruction Loss: 0.0275, KL Divergence: 8.5840\n",
      "Epoch: 29/40, Total Loss: 0.0447, Reconstruction Loss: 0.0274, KL Divergence: 8.6289\n",
      "Epoch: 30/40, Total Loss: 0.0446, Reconstruction Loss: 0.0274, KL Divergence: 8.6099\n",
      "Epoch: 31/40, Total Loss: 0.0446, Reconstruction Loss: 0.0273, KL Divergence: 8.6254\n",
      "Epoch: 32/40, Total Loss: 0.0445, Reconstruction Loss: 0.0272, KL Divergence: 8.6345\n",
      "Epoch: 33/40, Total Loss: 0.0445, Reconstruction Loss: 0.0272, KL Divergence: 8.6441\n",
      "Epoch: 34/40, Total Loss: 0.0445, Reconstruction Loss: 0.0272, KL Divergence: 8.6562\n",
      "Epoch: 35/40, Total Loss: 0.0445, Reconstruction Loss: 0.0271, KL Divergence: 8.6846\n",
      "Epoch: 36/40, Total Loss: 0.0444, Reconstruction Loss: 0.0271, KL Divergence: 8.6850\n",
      "Epoch: 37/40, Total Loss: 0.0444, Reconstruction Loss: 0.0270, KL Divergence: 8.6936\n",
      "Epoch: 38/40, Total Loss: 0.0444, Reconstruction Loss: 0.0270, KL Divergence: 8.7028\n",
      "Epoch: 39/40, Total Loss: 0.0444, Reconstruction Loss: 0.0270, KL Divergence: 8.6930\n",
      "Epoch: 40/40, Total Loss: 0.0443, Reconstruction Loss: 0.0269, KL Divergence: 8.6991\n",
      "Test Loss: Total Loss: 0.0445, Reconstruction Loss: 0.0272, KL Divergence: 8.6585\n",
      "tensor(0.0287, grad_fn=<MseLossBackward0>)\n",
      "Traing NaiveVAE with latent dim: 20\n",
      "Test Loss: Total Loss: 0.2363, Reconstruction Loss: 0.2359, KL Divergence: 0.1790\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004999637603759766,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 40,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e53b32f422e94ac79506317d4142a3cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40, Total Loss: 0.0608, Reconstruction Loss: 0.0479, KL Divergence: 6.4480\n",
      "Epoch: 2/40, Total Loss: 0.0533, Reconstruction Loss: 0.0386, KL Divergence: 7.3225\n",
      "Epoch: 3/40, Total Loss: 0.0518, Reconstruction Loss: 0.0370, KL Divergence: 7.4182\n",
      "Epoch: 4/40, Total Loss: 0.0502, Reconstruction Loss: 0.0353, KL Divergence: 7.4302\n",
      "Epoch: 5/40, Total Loss: 0.0491, Reconstruction Loss: 0.0340, KL Divergence: 7.5676\n",
      "Epoch: 6/40, Total Loss: 0.0484, Reconstruction Loss: 0.0330, KL Divergence: 7.7104\n",
      "Epoch: 7/40, Total Loss: 0.0478, Reconstruction Loss: 0.0322, KL Divergence: 7.8108\n",
      "Epoch: 8/40, Total Loss: 0.0474, Reconstruction Loss: 0.0316, KL Divergence: 7.9125\n",
      "Epoch: 9/40, Total Loss: 0.0471, Reconstruction Loss: 0.0311, KL Divergence: 8.0081\n",
      "Epoch: 10/40, Total Loss: 0.0468, Reconstruction Loss: 0.0306, KL Divergence: 8.0805\n",
      "Epoch: 11/40, Total Loss: 0.0464, Reconstruction Loss: 0.0302, KL Divergence: 8.1048\n",
      "Epoch: 12/40, Total Loss: 0.0463, Reconstruction Loss: 0.0299, KL Divergence: 8.1811\n",
      "Epoch: 13/40, Total Loss: 0.0461, Reconstruction Loss: 0.0296, KL Divergence: 8.2459\n",
      "Epoch: 14/40, Total Loss: 0.0460, Reconstruction Loss: 0.0294, KL Divergence: 8.2870\n",
      "Epoch: 15/40, Total Loss: 0.0459, Reconstruction Loss: 0.0292, KL Divergence: 8.3372\n",
      "Epoch: 16/40, Total Loss: 0.0457, Reconstruction Loss: 0.0290, KL Divergence: 8.3698\n",
      "Epoch: 17/40, Total Loss: 0.0456, Reconstruction Loss: 0.0288, KL Divergence: 8.4162\n",
      "Epoch: 18/40, Total Loss: 0.0455, Reconstruction Loss: 0.0286, KL Divergence: 8.4489\n",
      "Epoch: 19/40, Total Loss: 0.0454, Reconstruction Loss: 0.0285, KL Divergence: 8.4719\n",
      "Epoch: 20/40, Total Loss: 0.0454, Reconstruction Loss: 0.0284, KL Divergence: 8.5232\n",
      "Epoch: 21/40, Total Loss: 0.0452, Reconstruction Loss: 0.0282, KL Divergence: 8.5176\n",
      "Epoch: 22/40, Total Loss: 0.0452, Reconstruction Loss: 0.0281, KL Divergence: 8.5546\n",
      "Epoch: 23/40, Total Loss: 0.0451, Reconstruction Loss: 0.0280, KL Divergence: 8.5636\n",
      "Epoch: 24/40, Total Loss: 0.0451, Reconstruction Loss: 0.0279, KL Divergence: 8.5900\n",
      "Epoch: 25/40, Total Loss: 0.0451, Reconstruction Loss: 0.0279, KL Divergence: 8.5897\n",
      "Epoch: 26/40, Total Loss: 0.0451, Reconstruction Loss: 0.0278, KL Divergence: 8.6236\n",
      "Epoch: 27/40, Total Loss: 0.0450, Reconstruction Loss: 0.0277, KL Divergence: 8.6427\n",
      "Epoch: 28/40, Total Loss: 0.0449, Reconstruction Loss: 0.0276, KL Divergence: 8.6456\n",
      "Epoch: 29/40, Total Loss: 0.0449, Reconstruction Loss: 0.0276, KL Divergence: 8.6700\n",
      "Epoch: 30/40, Total Loss: 0.0449, Reconstruction Loss: 0.0275, KL Divergence: 8.6837\n",
      "Epoch: 31/40, Total Loss: 0.0448, Reconstruction Loss: 0.0275, KL Divergence: 8.6706\n",
      "Epoch: 32/40, Total Loss: 0.0448, Reconstruction Loss: 0.0274, KL Divergence: 8.6784\n",
      "Epoch: 33/40, Total Loss: 0.0447, Reconstruction Loss: 0.0273, KL Divergence: 8.6970\n",
      "Epoch: 34/40, Total Loss: 0.0447, Reconstruction Loss: 0.0273, KL Divergence: 8.7139\n",
      "Epoch: 35/40, Total Loss: 0.0447, Reconstruction Loss: 0.0272, KL Divergence: 8.7174\n",
      "Epoch: 36/40, Total Loss: 0.0447, Reconstruction Loss: 0.0272, KL Divergence: 8.7278\n",
      "Epoch: 37/40, Total Loss: 0.0446, Reconstruction Loss: 0.0271, KL Divergence: 8.7266\n",
      "Epoch: 38/40, Total Loss: 0.0446, Reconstruction Loss: 0.0271, KL Divergence: 8.7294\n",
      "Epoch: 39/40, Total Loss: 0.0446, Reconstruction Loss: 0.0271, KL Divergence: 8.7490\n",
      "Epoch: 40/40, Total Loss: 0.0445, Reconstruction Loss: 0.0270, KL Divergence: 8.7559\n",
      "Test Loss: Total Loss: 0.0448, Reconstruction Loss: 0.0275, KL Divergence: 8.6194\n",
      "tensor(0.0288, grad_fn=<MseLossBackward0>)\n",
      "Traing VAE with latent dim: 2\n",
      "Test Loss: Total Loss: 0.1898, Reconstruction Loss: 0.1898, KL Divergence: 0.0128\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0041353702545166016,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 40,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ffdb1a1233949b3b98a121cdbd0855b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40, Total Loss: 0.0761, Reconstruction Loss: 0.0735, KL Divergence: 1.3109\n",
      "Epoch: 2/40, Total Loss: 0.0612, Reconstruction Loss: 0.0571, KL Divergence: 2.0462\n",
      "Epoch: 3/40, Total Loss: 0.0602, Reconstruction Loss: 0.0558, KL Divergence: 2.2026\n",
      "Epoch: 4/40, Total Loss: 0.0596, Reconstruction Loss: 0.0550, KL Divergence: 2.2894\n",
      "Epoch: 5/40, Total Loss: 0.0593, Reconstruction Loss: 0.0546, KL Divergence: 2.3435\n",
      "Epoch: 6/40, Total Loss: 0.0591, Reconstruction Loss: 0.0543, KL Divergence: 2.3837\n",
      "Epoch: 7/40, Total Loss: 0.0589, Reconstruction Loss: 0.0541, KL Divergence: 2.4003\n",
      "Epoch: 8/40, Total Loss: 0.0588, Reconstruction Loss: 0.0540, KL Divergence: 2.4229\n",
      "Epoch: 9/40, Total Loss: 0.0587, Reconstruction Loss: 0.0538, KL Divergence: 2.4484\n",
      "Epoch: 10/40, Total Loss: 0.0586, Reconstruction Loss: 0.0537, KL Divergence: 2.4763\n",
      "Epoch: 11/40, Total Loss: 0.0585, Reconstruction Loss: 0.0535, KL Divergence: 2.4888\n",
      "Epoch: 12/40, Total Loss: 0.0584, Reconstruction Loss: 0.0534, KL Divergence: 2.4975\n",
      "Epoch: 13/40, Total Loss: 0.0583, Reconstruction Loss: 0.0533, KL Divergence: 2.5146\n",
      "Epoch: 14/40, Total Loss: 0.0583, Reconstruction Loss: 0.0532, KL Divergence: 2.5257\n",
      "Epoch: 15/40, Total Loss: 0.0582, Reconstruction Loss: 0.0532, KL Divergence: 2.5463\n",
      "Epoch: 16/40, Total Loss: 0.0582, Reconstruction Loss: 0.0531, KL Divergence: 2.5475\n",
      "Epoch: 17/40, Total Loss: 0.0581, Reconstruction Loss: 0.0530, KL Divergence: 2.5566\n",
      "Epoch: 18/40, Total Loss: 0.0581, Reconstruction Loss: 0.0530, KL Divergence: 2.5644\n",
      "Epoch: 19/40, Total Loss: 0.0580, Reconstruction Loss: 0.0529, KL Divergence: 2.5723\n",
      "Epoch: 20/40, Total Loss: 0.0581, Reconstruction Loss: 0.0529, KL Divergence: 2.5923\n",
      "Epoch: 21/40, Total Loss: 0.0580, Reconstruction Loss: 0.0528, KL Divergence: 2.6083\n",
      "Epoch: 22/40, Total Loss: 0.0579, Reconstruction Loss: 0.0527, KL Divergence: 2.6110\n",
      "Epoch: 23/40, Total Loss: 0.0579, Reconstruction Loss: 0.0527, KL Divergence: 2.6172\n",
      "Epoch: 24/40, Total Loss: 0.0579, Reconstruction Loss: 0.0526, KL Divergence: 2.6376\n",
      "Epoch: 25/40, Total Loss: 0.0578, Reconstruction Loss: 0.0526, KL Divergence: 2.6390\n",
      "Epoch: 26/40, Total Loss: 0.0578, Reconstruction Loss: 0.0525, KL Divergence: 2.6386\n",
      "Epoch: 27/40, Total Loss: 0.0578, Reconstruction Loss: 0.0525, KL Divergence: 2.6636\n",
      "Epoch: 28/40, Total Loss: 0.0578, Reconstruction Loss: 0.0524, KL Divergence: 2.6809\n",
      "Epoch: 29/40, Total Loss: 0.0577, Reconstruction Loss: 0.0524, KL Divergence: 2.6793\n",
      "Epoch: 30/40, Total Loss: 0.0577, Reconstruction Loss: 0.0523, KL Divergence: 2.6840\n",
      "Epoch: 31/40, Total Loss: 0.0577, Reconstruction Loss: 0.0523, KL Divergence: 2.6864\n",
      "Epoch: 32/40, Total Loss: 0.0577, Reconstruction Loss: 0.0523, KL Divergence: 2.7070\n",
      "Epoch: 33/40, Total Loss: 0.0576, Reconstruction Loss: 0.0522, KL Divergence: 2.7023\n",
      "Epoch: 34/40, Total Loss: 0.0576, Reconstruction Loss: 0.0522, KL Divergence: 2.7202\n",
      "Epoch: 35/40, Total Loss: 0.0576, Reconstruction Loss: 0.0521, KL Divergence: 2.7296\n",
      "Epoch: 36/40, Total Loss: 0.0575, Reconstruction Loss: 0.0520, KL Divergence: 2.7404\n",
      "Epoch: 37/40, Total Loss: 0.0575, Reconstruction Loss: 0.0520, KL Divergence: 2.7616\n",
      "Epoch: 38/40, Total Loss: 0.0575, Reconstruction Loss: 0.0520, KL Divergence: 2.7535\n",
      "Epoch: 39/40, Total Loss: 0.0575, Reconstruction Loss: 0.0519, KL Divergence: 2.7713\n",
      "Epoch: 40/40, Total Loss: 0.0574, Reconstruction Loss: 0.0519, KL Divergence: 2.7798\n",
      "Test Loss: Total Loss: 0.0574, Reconstruction Loss: 0.0522, KL Divergence: 2.6049\n",
      "tensor(0.0524, grad_fn=<MseLossBackward0>)\n",
      "Traing VAE with latent dim: 5\n",
      "Test Loss: Total Loss: 0.2469, Reconstruction Loss: 0.2468, KL Divergence: 0.0241\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005518913269042969,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 40,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d963ead5a5b24011a1e3cdfd50a0035a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40, Total Loss: 0.0822, Reconstruction Loss: 0.0781, KL Divergence: 2.0608\n",
      "Epoch: 2/40, Total Loss: 0.0589, Reconstruction Loss: 0.0511, KL Divergence: 3.8861\n",
      "Epoch: 3/40, Total Loss: 0.0563, Reconstruction Loss: 0.0474, KL Divergence: 4.4538\n",
      "Epoch: 4/40, Total Loss: 0.0552, Reconstruction Loss: 0.0457, KL Divergence: 4.7570\n",
      "Epoch: 5/40, Total Loss: 0.0545, Reconstruction Loss: 0.0447, KL Divergence: 4.9037\n",
      "Epoch: 6/40, Total Loss: 0.0541, Reconstruction Loss: 0.0441, KL Divergence: 5.0209\n",
      "Epoch: 7/40, Total Loss: 0.0538, Reconstruction Loss: 0.0436, KL Divergence: 5.1036\n",
      "Epoch: 8/40, Total Loss: 0.0534, Reconstruction Loss: 0.0432, KL Divergence: 5.1394\n",
      "Epoch: 9/40, Total Loss: 0.0533, Reconstruction Loss: 0.0429, KL Divergence: 5.2033\n",
      "Epoch: 10/40, Total Loss: 0.0531, Reconstruction Loss: 0.0426, KL Divergence: 5.2292\n",
      "Epoch: 11/40, Total Loss: 0.0530, Reconstruction Loss: 0.0424, KL Divergence: 5.2586\n",
      "Epoch: 12/40, Total Loss: 0.0528, Reconstruction Loss: 0.0423, KL Divergence: 5.2757\n",
      "Epoch: 13/40, Total Loss: 0.0527, Reconstruction Loss: 0.0421, KL Divergence: 5.2866\n",
      "Epoch: 14/40, Total Loss: 0.0526, Reconstruction Loss: 0.0419, KL Divergence: 5.3315\n",
      "Epoch: 15/40, Total Loss: 0.0525, Reconstruction Loss: 0.0418, KL Divergence: 5.3242\n",
      "Epoch: 16/40, Total Loss: 0.0524, Reconstruction Loss: 0.0417, KL Divergence: 5.3786\n",
      "Epoch: 17/40, Total Loss: 0.0524, Reconstruction Loss: 0.0416, KL Divergence: 5.3855\n",
      "Epoch: 18/40, Total Loss: 0.0523, Reconstruction Loss: 0.0415, KL Divergence: 5.3958\n",
      "Epoch: 19/40, Total Loss: 0.0522, Reconstruction Loss: 0.0414, KL Divergence: 5.4133\n",
      "Epoch: 20/40, Total Loss: 0.0522, Reconstruction Loss: 0.0413, KL Divergence: 5.4212\n",
      "Epoch: 21/40, Total Loss: 0.0521, Reconstruction Loss: 0.0413, KL Divergence: 5.4347\n",
      "Epoch: 22/40, Total Loss: 0.0520, Reconstruction Loss: 0.0412, KL Divergence: 5.4424\n",
      "Epoch: 23/40, Total Loss: 0.0520, Reconstruction Loss: 0.0411, KL Divergence: 5.4522\n",
      "Epoch: 24/40, Total Loss: 0.0519, Reconstruction Loss: 0.0410, KL Divergence: 5.4645\n",
      "Epoch: 25/40, Total Loss: 0.0519, Reconstruction Loss: 0.0409, KL Divergence: 5.4861\n",
      "Epoch: 26/40, Total Loss: 0.0519, Reconstruction Loss: 0.0409, KL Divergence: 5.4954\n",
      "Epoch: 27/40, Total Loss: 0.0518, Reconstruction Loss: 0.0408, KL Divergence: 5.4891\n",
      "Epoch: 28/40, Total Loss: 0.0517, Reconstruction Loss: 0.0407, KL Divergence: 5.5010\n",
      "Epoch: 29/40, Total Loss: 0.0517, Reconstruction Loss: 0.0407, KL Divergence: 5.5069\n",
      "Epoch: 30/40, Total Loss: 0.0517, Reconstruction Loss: 0.0407, KL Divergence: 5.5211\n",
      "Epoch: 31/40, Total Loss: 0.0517, Reconstruction Loss: 0.0406, KL Divergence: 5.5263\n",
      "Epoch: 32/40, Total Loss: 0.0516, Reconstruction Loss: 0.0406, KL Divergence: 5.5281\n",
      "Epoch: 33/40, Total Loss: 0.0516, Reconstruction Loss: 0.0405, KL Divergence: 5.5317\n",
      "Epoch: 34/40, Total Loss: 0.0515, Reconstruction Loss: 0.0405, KL Divergence: 5.5347\n",
      "Epoch: 35/40, Total Loss: 0.0515, Reconstruction Loss: 0.0404, KL Divergence: 5.5480\n",
      "Epoch: 36/40, Total Loss: 0.0515, Reconstruction Loss: 0.0404, KL Divergence: 5.5366\n",
      "Epoch: 37/40, Total Loss: 0.0515, Reconstruction Loss: 0.0403, KL Divergence: 5.5633\n",
      "Epoch: 38/40, Total Loss: 0.0514, Reconstruction Loss: 0.0403, KL Divergence: 5.5400\n",
      "Epoch: 39/40, Total Loss: 0.0514, Reconstruction Loss: 0.0403, KL Divergence: 5.5618\n",
      "Epoch: 40/40, Total Loss: 0.0514, Reconstruction Loss: 0.0403, KL Divergence: 5.5643\n",
      "Test Loss: Total Loss: 0.0513, Reconstruction Loss: 0.0401, KL Divergence: 5.5799\n",
      "tensor(0.0420, grad_fn=<MseLossBackward0>)\n",
      "Traing VAE with latent dim: 10\n",
      "Test Loss: Total Loss: 0.2386, Reconstruction Loss: 0.2384, KL Divergence: 0.0763\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009007692337036133,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 40,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f006580fcd4b4e9ab45874d764db1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40, Total Loss: 0.0839, Reconstruction Loss: 0.0804, KL Divergence: 1.7526\n",
      "Epoch: 2/40, Total Loss: 0.0585, Reconstruction Loss: 0.0503, KL Divergence: 4.0867\n",
      "Epoch: 3/40, Total Loss: 0.0553, Reconstruction Loss: 0.0448, KL Divergence: 5.2452\n",
      "Epoch: 4/40, Total Loss: 0.0539, Reconstruction Loss: 0.0424, KL Divergence: 5.7838\n",
      "Epoch: 5/40, Total Loss: 0.0532, Reconstruction Loss: 0.0411, KL Divergence: 6.0533\n",
      "Epoch: 6/40, Total Loss: 0.0527, Reconstruction Loss: 0.0403, KL Divergence: 6.2123\n",
      "Epoch: 7/40, Total Loss: 0.0524, Reconstruction Loss: 0.0398, KL Divergence: 6.3105\n",
      "Epoch: 8/40, Total Loss: 0.0522, Reconstruction Loss: 0.0394, KL Divergence: 6.3635\n",
      "Epoch: 9/40, Total Loss: 0.0520, Reconstruction Loss: 0.0392, KL Divergence: 6.4177\n",
      "Epoch: 10/40, Total Loss: 0.0519, Reconstruction Loss: 0.0389, KL Divergence: 6.4723\n",
      "Epoch: 11/40, Total Loss: 0.0517, Reconstruction Loss: 0.0387, KL Divergence: 6.4887\n",
      "Epoch: 12/40, Total Loss: 0.0516, Reconstruction Loss: 0.0386, KL Divergence: 6.5270\n",
      "Epoch: 13/40, Total Loss: 0.0515, Reconstruction Loss: 0.0384, KL Divergence: 6.5732\n",
      "Epoch: 14/40, Total Loss: 0.0515, Reconstruction Loss: 0.0383, KL Divergence: 6.5982\n",
      "Epoch: 15/40, Total Loss: 0.0514, Reconstruction Loss: 0.0382, KL Divergence: 6.6139\n",
      "Epoch: 16/40, Total Loss: 0.0513, Reconstruction Loss: 0.0381, KL Divergence: 6.6228\n",
      "Epoch: 17/40, Total Loss: 0.0513, Reconstruction Loss: 0.0380, KL Divergence: 6.6724\n",
      "Epoch: 18/40, Total Loss: 0.0512, Reconstruction Loss: 0.0379, KL Divergence: 6.6682\n",
      "Epoch: 19/40, Total Loss: 0.0511, Reconstruction Loss: 0.0378, KL Divergence: 6.6701\n",
      "Epoch: 20/40, Total Loss: 0.0511, Reconstruction Loss: 0.0377, KL Divergence: 6.6938\n",
      "Epoch: 21/40, Total Loss: 0.0511, Reconstruction Loss: 0.0376, KL Divergence: 6.7234\n",
      "Epoch: 22/40, Total Loss: 0.0510, Reconstruction Loss: 0.0375, KL Divergence: 6.7231\n",
      "Epoch: 23/40, Total Loss: 0.0510, Reconstruction Loss: 0.0375, KL Divergence: 6.7419\n",
      "Epoch: 24/40, Total Loss: 0.0509, Reconstruction Loss: 0.0374, KL Divergence: 6.7502\n",
      "Epoch: 25/40, Total Loss: 0.0509, Reconstruction Loss: 0.0374, KL Divergence: 6.7806\n",
      "Epoch: 26/40, Total Loss: 0.0509, Reconstruction Loss: 0.0373, KL Divergence: 6.7905\n",
      "Epoch: 27/40, Total Loss: 0.0508, Reconstruction Loss: 0.0373, KL Divergence: 6.7862\n",
      "Epoch: 28/40, Total Loss: 0.0508, Reconstruction Loss: 0.0372, KL Divergence: 6.8007\n",
      "Epoch: 29/40, Total Loss: 0.0508, Reconstruction Loss: 0.0371, KL Divergence: 6.8136\n",
      "Epoch: 30/40, Total Loss: 0.0508, Reconstruction Loss: 0.0371, KL Divergence: 6.8129\n",
      "Epoch: 31/40, Total Loss: 0.0507, Reconstruction Loss: 0.0371, KL Divergence: 6.8147\n",
      "Epoch: 32/40, Total Loss: 0.0507, Reconstruction Loss: 0.0370, KL Divergence: 6.8410\n",
      "Epoch: 33/40, Total Loss: 0.0507, Reconstruction Loss: 0.0370, KL Divergence: 6.8417\n",
      "Epoch: 34/40, Total Loss: 0.0506, Reconstruction Loss: 0.0369, KL Divergence: 6.8478\n",
      "Epoch: 35/40, Total Loss: 0.0506, Reconstruction Loss: 0.0369, KL Divergence: 6.8597\n",
      "Epoch: 36/40, Total Loss: 0.0506, Reconstruction Loss: 0.0369, KL Divergence: 6.8669\n",
      "Epoch: 37/40, Total Loss: 0.0506, Reconstruction Loss: 0.0369, KL Divergence: 6.8574\n",
      "Epoch: 38/40, Total Loss: 0.0506, Reconstruction Loss: 0.0369, KL Divergence: 6.8560\n",
      "Epoch: 39/40, Total Loss: 0.0506, Reconstruction Loss: 0.0368, KL Divergence: 6.8603\n",
      "Epoch: 40/40, Total Loss: 0.0505, Reconstruction Loss: 0.0368, KL Divergence: 6.8628\n",
      "Test Loss: Total Loss: 0.0504, Reconstruction Loss: 0.0368, KL Divergence: 6.8206\n",
      "tensor(0.0376, grad_fn=<MseLossBackward0>)\n",
      "Traing VAE with latent dim: 20\n",
      "Test Loss: Total Loss: 0.2251, Reconstruction Loss: 0.2250, KL Divergence: 0.0787\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004503965377807617,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 40,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce921a96cb9047c7b2c1c7e9844b3ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40, Total Loss: 0.0814, Reconstruction Loss: 0.0779, KL Divergence: 1.7344\n",
      "Epoch: 2/40, Total Loss: 0.0580, Reconstruction Loss: 0.0494, KL Divergence: 4.3061\n",
      "Epoch: 3/40, Total Loss: 0.0554, Reconstruction Loss: 0.0448, KL Divergence: 5.2812\n",
      "Epoch: 4/40, Total Loss: 0.0542, Reconstruction Loss: 0.0428, KL Divergence: 5.6930\n",
      "Epoch: 5/40, Total Loss: 0.0536, Reconstruction Loss: 0.0417, KL Divergence: 5.9096\n",
      "Epoch: 6/40, Total Loss: 0.0530, Reconstruction Loss: 0.0409, KL Divergence: 6.0574\n",
      "Epoch: 7/40, Total Loss: 0.0526, Reconstruction Loss: 0.0403, KL Divergence: 6.1844\n",
      "Epoch: 8/40, Total Loss: 0.0523, Reconstruction Loss: 0.0397, KL Divergence: 6.3279\n",
      "Epoch: 9/40, Total Loss: 0.0520, Reconstruction Loss: 0.0391, KL Divergence: 6.4425\n",
      "Epoch: 10/40, Total Loss: 0.0518, Reconstruction Loss: 0.0388, KL Divergence: 6.5326\n",
      "Epoch: 11/40, Total Loss: 0.0516, Reconstruction Loss: 0.0385, KL Divergence: 6.5828\n",
      "Epoch: 12/40, Total Loss: 0.0515, Reconstruction Loss: 0.0382, KL Divergence: 6.6337\n",
      "Epoch: 13/40, Total Loss: 0.0513, Reconstruction Loss: 0.0380, KL Divergence: 6.6716\n",
      "Epoch: 14/40, Total Loss: 0.0513, Reconstruction Loss: 0.0379, KL Divergence: 6.7089\n",
      "Epoch: 15/40, Total Loss: 0.0512, Reconstruction Loss: 0.0377, KL Divergence: 6.7560\n",
      "Epoch: 16/40, Total Loss: 0.0511, Reconstruction Loss: 0.0376, KL Divergence: 6.7577\n",
      "Epoch: 17/40, Total Loss: 0.0510, Reconstruction Loss: 0.0375, KL Divergence: 6.7673\n",
      "Epoch: 18/40, Total Loss: 0.0511, Reconstruction Loss: 0.0375, KL Divergence: 6.8121\n",
      "Epoch: 19/40, Total Loss: 0.0510, Reconstruction Loss: 0.0374, KL Divergence: 6.8218\n",
      "Epoch: 20/40, Total Loss: 0.0509, Reconstruction Loss: 0.0373, KL Divergence: 6.8000\n",
      "Epoch: 21/40, Total Loss: 0.0509, Reconstruction Loss: 0.0372, KL Divergence: 6.8419\n",
      "Epoch: 22/40, Total Loss: 0.0508, Reconstruction Loss: 0.0371, KL Divergence: 6.8457\n",
      "Epoch: 23/40, Total Loss: 0.0508, Reconstruction Loss: 0.0370, KL Divergence: 6.8614\n",
      "Epoch: 24/40, Total Loss: 0.0508, Reconstruction Loss: 0.0370, KL Divergence: 6.9004\n",
      "Epoch: 25/40, Total Loss: 0.0507, Reconstruction Loss: 0.0369, KL Divergence: 6.8953\n",
      "Epoch: 26/40, Total Loss: 0.0507, Reconstruction Loss: 0.0368, KL Divergence: 6.9118\n",
      "Epoch: 27/40, Total Loss: 0.0506, Reconstruction Loss: 0.0368, KL Divergence: 6.9116\n",
      "Epoch: 28/40, Total Loss: 0.0506, Reconstruction Loss: 0.0367, KL Divergence: 6.9392\n",
      "Epoch: 29/40, Total Loss: 0.0505, Reconstruction Loss: 0.0367, KL Divergence: 6.9367\n",
      "Epoch: 30/40, Total Loss: 0.0506, Reconstruction Loss: 0.0366, KL Divergence: 6.9589\n",
      "Epoch: 31/40, Total Loss: 0.0505, Reconstruction Loss: 0.0366, KL Divergence: 6.9645\n",
      "Epoch: 32/40, Total Loss: 0.0505, Reconstruction Loss: 0.0366, KL Divergence: 6.9547\n",
      "Epoch: 33/40, Total Loss: 0.0505, Reconstruction Loss: 0.0365, KL Divergence: 6.9817\n",
      "Epoch: 34/40, Total Loss: 0.0504, Reconstruction Loss: 0.0365, KL Divergence: 6.9830\n",
      "Epoch: 35/40, Total Loss: 0.0504, Reconstruction Loss: 0.0364, KL Divergence: 6.9873\n",
      "Epoch: 36/40, Total Loss: 0.0504, Reconstruction Loss: 0.0364, KL Divergence: 6.9860\n",
      "Epoch: 37/40, Total Loss: 0.0504, Reconstruction Loss: 0.0364, KL Divergence: 6.9987\n",
      "Epoch: 38/40, Total Loss: 0.0504, Reconstruction Loss: 0.0364, KL Divergence: 6.9962\n",
      "Epoch: 39/40, Total Loss: 0.0503, Reconstruction Loss: 0.0363, KL Divergence: 6.9915\n",
      "Epoch: 40/40, Total Loss: 0.0503, Reconstruction Loss: 0.0363, KL Divergence: 7.0096\n",
      "Test Loss: Total Loss: 0.0503, Reconstruction Loss: 0.0362, KL Divergence: 7.0236\n",
      "tensor(0.0376, grad_fn=<MseLossBackward0>)\n",
      "[Training beta = 0.005]\n",
      "Traing NaiveVAE with latent dim: 2\n",
      "Test Loss: Total Loss: 0.2383, Reconstruction Loss: 0.2382, KL Divergence: 0.0172\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0060236454010009766,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 40,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8445ed79a8ab46739125f464224f4046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40, Total Loss: 0.0670, Reconstruction Loss: 0.0619, KL Divergence: 1.0188\n",
      "Epoch: 2/40, Total Loss: 0.0639, Reconstruction Loss: 0.0574, KL Divergence: 1.2862\n",
      "Epoch: 3/40, Total Loss: 0.0632, Reconstruction Loss: 0.0558, KL Divergence: 1.4833\n",
      "Epoch: 4/40, Total Loss: 0.0628, Reconstruction Loss: 0.0547, KL Divergence: 1.6125\n",
      "Epoch: 5/40, Total Loss: 0.0625, Reconstruction Loss: 0.0540, KL Divergence: 1.6969\n",
      "Epoch: 6/40, Total Loss: 0.0624, Reconstruction Loss: 0.0535, KL Divergence: 1.7730\n",
      "Epoch: 7/40, Total Loss: 0.0622, Reconstruction Loss: 0.0532, KL Divergence: 1.8025\n",
      "Epoch: 8/40, Total Loss: 0.0621, Reconstruction Loss: 0.0529, KL Divergence: 1.8426\n",
      "Epoch: 9/40, Total Loss: 0.0620, Reconstruction Loss: 0.0526, KL Divergence: 1.8691\n",
      "Epoch: 10/40, Total Loss: 0.0619, Reconstruction Loss: 0.0524, KL Divergence: 1.8972\n",
      "Epoch: 11/40, Total Loss: 0.0618, Reconstruction Loss: 0.0522, KL Divergence: 1.9123\n",
      "Epoch: 12/40, Total Loss: 0.0618, Reconstruction Loss: 0.0521, KL Divergence: 1.9369\n",
      "Epoch: 13/40, Total Loss: 0.0617, Reconstruction Loss: 0.0519, KL Divergence: 1.9553\n",
      "Epoch: 14/40, Total Loss: 0.0617, Reconstruction Loss: 0.0518, KL Divergence: 1.9781\n",
      "Epoch: 15/40, Total Loss: 0.0617, Reconstruction Loss: 0.0517, KL Divergence: 1.9949\n",
      "Epoch: 16/40, Total Loss: 0.0616, Reconstruction Loss: 0.0516, KL Divergence: 2.0108\n",
      "Epoch: 17/40, Total Loss: 0.0616, Reconstruction Loss: 0.0515, KL Divergence: 2.0089\n",
      "Epoch: 18/40, Total Loss: 0.0615, Reconstruction Loss: 0.0514, KL Divergence: 2.0296\n",
      "Epoch: 19/40, Total Loss: 0.0615, Reconstruction Loss: 0.0513, KL Divergence: 2.0423\n",
      "Epoch: 20/40, Total Loss: 0.0614, Reconstruction Loss: 0.0512, KL Divergence: 2.0477\n",
      "Epoch: 21/40, Total Loss: 0.0614, Reconstruction Loss: 0.0511, KL Divergence: 2.0654\n",
      "Epoch: 22/40, Total Loss: 0.0615, Reconstruction Loss: 0.0511, KL Divergence: 2.0798\n",
      "Epoch: 23/40, Total Loss: 0.0613, Reconstruction Loss: 0.0510, KL Divergence: 2.0664\n",
      "Epoch: 24/40, Total Loss: 0.0614, Reconstruction Loss: 0.0509, KL Divergence: 2.0877\n",
      "Epoch: 25/40, Total Loss: 0.0613, Reconstruction Loss: 0.0508, KL Divergence: 2.0943\n",
      "Epoch: 26/40, Total Loss: 0.0613, Reconstruction Loss: 0.0507, KL Divergence: 2.1119\n",
      "Epoch: 27/40, Total Loss: 0.0613, Reconstruction Loss: 0.0507, KL Divergence: 2.1036\n",
      "Epoch: 28/40, Total Loss: 0.0613, Reconstruction Loss: 0.0506, KL Divergence: 2.1250\n",
      "Epoch: 29/40, Total Loss: 0.0612, Reconstruction Loss: 0.0506, KL Divergence: 2.1219\n",
      "Epoch: 30/40, Total Loss: 0.0612, Reconstruction Loss: 0.0506, KL Divergence: 2.1256\n",
      "Epoch: 31/40, Total Loss: 0.0612, Reconstruction Loss: 0.0505, KL Divergence: 2.1387\n",
      "Epoch: 32/40, Total Loss: 0.0612, Reconstruction Loss: 0.0505, KL Divergence: 2.1472\n",
      "Epoch: 33/40, Total Loss: 0.0612, Reconstruction Loss: 0.0504, KL Divergence: 2.1583\n",
      "Epoch: 34/40, Total Loss: 0.0611, Reconstruction Loss: 0.0504, KL Divergence: 2.1536\n",
      "Epoch: 35/40, Total Loss: 0.0611, Reconstruction Loss: 0.0504, KL Divergence: 2.1537\n",
      "Epoch: 36/40, Total Loss: 0.0611, Reconstruction Loss: 0.0503, KL Divergence: 2.1587\n",
      "Epoch: 37/40, Total Loss: 0.0611, Reconstruction Loss: 0.0503, KL Divergence: 2.1667\n",
      "Epoch: 38/40, Total Loss: 0.0611, Reconstruction Loss: 0.0502, KL Divergence: 2.1719\n",
      "Epoch: 39/40, Total Loss: 0.0611, Reconstruction Loss: 0.0502, KL Divergence: 2.1841\n",
      "Epoch: 40/40, Total Loss: 0.0611, Reconstruction Loss: 0.0501, KL Divergence: 2.1969\n",
      "Test Loss: Total Loss: 0.0610, Reconstruction Loss: 0.0498, KL Divergence: 2.2538\n",
      "tensor(0.0517, grad_fn=<MseLossBackward0>)\n",
      "Traing NaiveVAE with latent dim: 5\n",
      "Test Loss: Total Loss: 0.2361, Reconstruction Loss: 0.2358, KL Divergence: 0.0549\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012546539306640625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 40,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a1389c64ecc40558b66d18478b3949d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40, Total Loss: 0.0673, Reconstruction Loss: 0.0591, KL Divergence: 1.6498\n",
      "Epoch: 2/40, Total Loss: 0.0632, Reconstruction Loss: 0.0536, KL Divergence: 1.9288\n",
      "Epoch: 3/40, Total Loss: 0.0626, Reconstruction Loss: 0.0519, KL Divergence: 2.1452\n",
      "Epoch: 4/40, Total Loss: 0.0621, Reconstruction Loss: 0.0506, KL Divergence: 2.3107\n",
      "Epoch: 5/40, Total Loss: 0.0617, Reconstruction Loss: 0.0495, KL Divergence: 2.4447\n",
      "Epoch: 6/40, Total Loss: 0.0616, Reconstruction Loss: 0.0488, KL Divergence: 2.5468\n",
      "Epoch: 7/40, Total Loss: 0.0613, Reconstruction Loss: 0.0482, KL Divergence: 2.6212\n",
      "Epoch: 8/40, Total Loss: 0.0611, Reconstruction Loss: 0.0477, KL Divergence: 2.6829\n",
      "Epoch: 9/40, Total Loss: 0.0611, Reconstruction Loss: 0.0474, KL Divergence: 2.7233\n",
      "Epoch: 10/40, Total Loss: 0.0609, Reconstruction Loss: 0.0471, KL Divergence: 2.7773\n",
      "Epoch: 11/40, Total Loss: 0.0608, Reconstruction Loss: 0.0467, KL Divergence: 2.8078\n",
      "Epoch: 12/40, Total Loss: 0.0608, Reconstruction Loss: 0.0466, KL Divergence: 2.8313\n",
      "Epoch: 13/40, Total Loss: 0.0607, Reconstruction Loss: 0.0464, KL Divergence: 2.8583\n",
      "Epoch: 14/40, Total Loss: 0.0607, Reconstruction Loss: 0.0462, KL Divergence: 2.8889\n",
      "Epoch: 15/40, Total Loss: 0.0607, Reconstruction Loss: 0.0462, KL Divergence: 2.9054\n",
      "Epoch: 16/40, Total Loss: 0.0606, Reconstruction Loss: 0.0459, KL Divergence: 2.9302\n",
      "Epoch: 17/40, Total Loss: 0.0605, Reconstruction Loss: 0.0458, KL Divergence: 2.9400\n",
      "Epoch: 18/40, Total Loss: 0.0605, Reconstruction Loss: 0.0457, KL Divergence: 2.9476\n",
      "Epoch: 19/40, Total Loss: 0.0605, Reconstruction Loss: 0.0456, KL Divergence: 2.9753\n",
      "Epoch: 20/40, Total Loss: 0.0605, Reconstruction Loss: 0.0455, KL Divergence: 2.9925\n",
      "Epoch: 21/40, Total Loss: 0.0604, Reconstruction Loss: 0.0454, KL Divergence: 2.9889\n",
      "Epoch: 22/40, Total Loss: 0.0604, Reconstruction Loss: 0.0453, KL Divergence: 3.0053\n",
      "Epoch: 23/40, Total Loss: 0.0603, Reconstruction Loss: 0.0453, KL Divergence: 3.0173\n",
      "Epoch: 24/40, Total Loss: 0.0603, Reconstruction Loss: 0.0452, KL Divergence: 3.0241\n",
      "Epoch: 25/40, Total Loss: 0.0603, Reconstruction Loss: 0.0451, KL Divergence: 3.0302\n",
      "Epoch: 26/40, Total Loss: 0.0603, Reconstruction Loss: 0.0450, KL Divergence: 3.0490\n",
      "Epoch: 27/40, Total Loss: 0.0602, Reconstruction Loss: 0.0450, KL Divergence: 3.0451\n",
      "Epoch: 28/40, Total Loss: 0.0602, Reconstruction Loss: 0.0449, KL Divergence: 3.0659\n",
      "Epoch: 29/40, Total Loss: 0.0602, Reconstruction Loss: 0.0449, KL Divergence: 3.0671\n",
      "Epoch: 30/40, Total Loss: 0.0602, Reconstruction Loss: 0.0448, KL Divergence: 3.0772\n",
      "Epoch: 31/40, Total Loss: 0.0602, Reconstruction Loss: 0.0447, KL Divergence: 3.0823\n",
      "Epoch: 32/40, Total Loss: 0.0601, Reconstruction Loss: 0.0447, KL Divergence: 3.0788\n",
      "Epoch: 33/40, Total Loss: 0.0601, Reconstruction Loss: 0.0446, KL Divergence: 3.0943\n",
      "Epoch: 34/40, Total Loss: 0.0602, Reconstruction Loss: 0.0446, KL Divergence: 3.1095\n",
      "Epoch: 35/40, Total Loss: 0.0602, Reconstruction Loss: 0.0446, KL Divergence: 3.1250\n",
      "Epoch: 36/40, Total Loss: 0.0601, Reconstruction Loss: 0.0445, KL Divergence: 3.1212\n",
      "Epoch: 37/40, Total Loss: 0.0601, Reconstruction Loss: 0.0445, KL Divergence: 3.1233\n",
      "Epoch: 38/40, Total Loss: 0.0601, Reconstruction Loss: 0.0444, KL Divergence: 3.1308\n",
      "Epoch: 39/40, Total Loss: 0.0600, Reconstruction Loss: 0.0444, KL Divergence: 3.1196\n",
      "Epoch: 40/40, Total Loss: 0.0601, Reconstruction Loss: 0.0444, KL Divergence: 3.1447\n",
      "Test Loss: Total Loss: 0.0597, Reconstruction Loss: 0.0441, KL Divergence: 3.1371\n",
      "tensor(0.0452, grad_fn=<MseLossBackward0>)\n",
      "Traing NaiveVAE with latent dim: 10\n",
      "Test Loss: Total Loss: 0.2376, Reconstruction Loss: 0.2370, KL Divergence: 0.1197\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005007266998291016,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 40,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08d4cb8b8c0d4411b3f9b450c50ed8d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40, Total Loss: 0.0688, Reconstruction Loss: 0.0601, KL Divergence: 1.7423\n",
      "Epoch: 2/40, Total Loss: 0.0640, Reconstruction Loss: 0.0550, KL Divergence: 1.7974\n",
      "Epoch: 3/40, Total Loss: 0.0632, Reconstruction Loss: 0.0534, KL Divergence: 1.9616\n",
      "Epoch: 4/40, Total Loss: 0.0627, Reconstruction Loss: 0.0519, KL Divergence: 2.1447\n",
      "Epoch: 5/40, Total Loss: 0.0622, Reconstruction Loss: 0.0507, KL Divergence: 2.2946\n",
      "Epoch: 6/40, Total Loss: 0.0619, Reconstruction Loss: 0.0498, KL Divergence: 2.4228\n",
      "Epoch: 7/40, Total Loss: 0.0617, Reconstruction Loss: 0.0490, KL Divergence: 2.5300\n",
      "Epoch: 8/40, Total Loss: 0.0614, Reconstruction Loss: 0.0484, KL Divergence: 2.6043\n",
      "Epoch: 9/40, Total Loss: 0.0613, Reconstruction Loss: 0.0479, KL Divergence: 2.6757\n",
      "Epoch: 10/40, Total Loss: 0.0611, Reconstruction Loss: 0.0476, KL Divergence: 2.7181\n",
      "Epoch: 11/40, Total Loss: 0.0610, Reconstruction Loss: 0.0472, KL Divergence: 2.7576\n",
      "Epoch: 12/40, Total Loss: 0.0610, Reconstruction Loss: 0.0470, KL Divergence: 2.7968\n",
      "Epoch: 13/40, Total Loss: 0.0609, Reconstruction Loss: 0.0468, KL Divergence: 2.8304\n",
      "Epoch: 14/40, Total Loss: 0.0609, Reconstruction Loss: 0.0466, KL Divergence: 2.8580\n",
      "Epoch: 15/40, Total Loss: 0.0608, Reconstruction Loss: 0.0464, KL Divergence: 2.8789\n",
      "Epoch: 16/40, Total Loss: 0.0608, Reconstruction Loss: 0.0462, KL Divergence: 2.9175\n",
      "Epoch: 17/40, Total Loss: 0.0607, Reconstruction Loss: 0.0460, KL Divergence: 2.9402\n",
      "Epoch: 18/40, Total Loss: 0.0606, Reconstruction Loss: 0.0460, KL Divergence: 2.9343\n",
      "Epoch: 19/40, Total Loss: 0.0605, Reconstruction Loss: 0.0458, KL Divergence: 2.9446\n",
      "Epoch: 20/40, Total Loss: 0.0605, Reconstruction Loss: 0.0457, KL Divergence: 2.9619\n",
      "Epoch: 21/40, Total Loss: 0.0606, Reconstruction Loss: 0.0456, KL Divergence: 2.9874\n",
      "Epoch: 22/40, Total Loss: 0.0604, Reconstruction Loss: 0.0455, KL Divergence: 2.9786\n",
      "Epoch: 23/40, Total Loss: 0.0606, Reconstruction Loss: 0.0455, KL Divergence: 3.0161\n",
      "Epoch: 24/40, Total Loss: 0.0605, Reconstruction Loss: 0.0453, KL Divergence: 3.0273\n",
      "Epoch: 25/40, Total Loss: 0.0605, Reconstruction Loss: 0.0453, KL Divergence: 3.0297\n",
      "Epoch: 26/40, Total Loss: 0.0605, Reconstruction Loss: 0.0453, KL Divergence: 3.0406\n",
      "Epoch: 27/40, Total Loss: 0.0604, Reconstruction Loss: 0.0452, KL Divergence: 3.0434\n",
      "Epoch: 28/40, Total Loss: 0.0604, Reconstruction Loss: 0.0451, KL Divergence: 3.0647\n",
      "Epoch: 29/40, Total Loss: 0.0604, Reconstruction Loss: 0.0451, KL Divergence: 3.0566\n",
      "Epoch: 30/40, Total Loss: 0.0603, Reconstruction Loss: 0.0450, KL Divergence: 3.0654\n",
      "Epoch: 31/40, Total Loss: 0.0603, Reconstruction Loss: 0.0449, KL Divergence: 3.0760\n",
      "Epoch: 32/40, Total Loss: 0.0602, Reconstruction Loss: 0.0448, KL Divergence: 3.0761\n",
      "Epoch: 33/40, Total Loss: 0.0603, Reconstruction Loss: 0.0448, KL Divergence: 3.0848\n",
      "Epoch: 34/40, Total Loss: 0.0603, Reconstruction Loss: 0.0449, KL Divergence: 3.0947\n",
      "Epoch: 35/40, Total Loss: 0.0604, Reconstruction Loss: 0.0448, KL Divergence: 3.1097\n",
      "Epoch: 36/40, Total Loss: 0.0602, Reconstruction Loss: 0.0447, KL Divergence: 3.1013\n",
      "Epoch: 37/40, Total Loss: 0.0603, Reconstruction Loss: 0.0447, KL Divergence: 3.1126\n",
      "Epoch: 38/40, Total Loss: 0.0603, Reconstruction Loss: 0.0447, KL Divergence: 3.1133\n",
      "Epoch: 39/40, Total Loss: 0.0603, Reconstruction Loss: 0.0446, KL Divergence: 3.1276\n",
      "Epoch: 40/40, Total Loss: 0.0602, Reconstruction Loss: 0.0446, KL Divergence: 3.1207\n",
      "Test Loss: Total Loss: 0.0604, Reconstruction Loss: 0.0449, KL Divergence: 3.1051\n",
      "tensor(0.0465, grad_fn=<MseLossBackward0>)\n",
      "Traing NaiveVAE with latent dim: 20\n",
      "Test Loss: Total Loss: 0.2375, Reconstruction Loss: 0.2367, KL Divergence: 0.1580\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007999658584594727,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 40,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2b6afb9c383491cb261d78ad04bec16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40, Total Loss: 0.0703, Reconstruction Loss: 0.0625, KL Divergence: 1.5660\n",
      "Epoch: 2/40, Total Loss: 0.0644, Reconstruction Loss: 0.0560, KL Divergence: 1.6799\n",
      "Epoch: 3/40, Total Loss: 0.0640, Reconstruction Loss: 0.0552, KL Divergence: 1.7662\n",
      "Epoch: 4/40, Total Loss: 0.0634, Reconstruction Loss: 0.0540, KL Divergence: 1.8795\n",
      "Epoch: 5/40, Total Loss: 0.0628, Reconstruction Loss: 0.0527, KL Divergence: 2.0199\n",
      "Epoch: 6/40, Total Loss: 0.0624, Reconstruction Loss: 0.0515, KL Divergence: 2.1701\n",
      "Epoch: 7/40, Total Loss: 0.0620, Reconstruction Loss: 0.0506, KL Divergence: 2.2958\n",
      "Epoch: 8/40, Total Loss: 0.0617, Reconstruction Loss: 0.0498, KL Divergence: 2.3907\n",
      "Epoch: 9/40, Total Loss: 0.0615, Reconstruction Loss: 0.0492, KL Divergence: 2.4608\n",
      "Epoch: 10/40, Total Loss: 0.0613, Reconstruction Loss: 0.0486, KL Divergence: 2.5376\n",
      "Epoch: 11/40, Total Loss: 0.0612, Reconstruction Loss: 0.0482, KL Divergence: 2.6007\n",
      "Epoch: 12/40, Total Loss: 0.0611, Reconstruction Loss: 0.0478, KL Divergence: 2.6604\n",
      "Epoch: 13/40, Total Loss: 0.0610, Reconstruction Loss: 0.0475, KL Divergence: 2.7010\n",
      "Epoch: 14/40, Total Loss: 0.0610, Reconstruction Loss: 0.0472, KL Divergence: 2.7426\n",
      "Epoch: 15/40, Total Loss: 0.0608, Reconstruction Loss: 0.0470, KL Divergence: 2.7527\n",
      "Epoch: 16/40, Total Loss: 0.0608, Reconstruction Loss: 0.0469, KL Divergence: 2.7840\n",
      "Epoch: 17/40, Total Loss: 0.0608, Reconstruction Loss: 0.0467, KL Divergence: 2.8209\n",
      "Epoch: 18/40, Total Loss: 0.0606, Reconstruction Loss: 0.0465, KL Divergence: 2.8248\n",
      "Epoch: 19/40, Total Loss: 0.0607, Reconstruction Loss: 0.0464, KL Divergence: 2.8610\n",
      "Epoch: 20/40, Total Loss: 0.0607, Reconstruction Loss: 0.0463, KL Divergence: 2.8827\n",
      "Epoch: 21/40, Total Loss: 0.0605, Reconstruction Loss: 0.0461, KL Divergence: 2.8840\n",
      "Epoch: 22/40, Total Loss: 0.0605, Reconstruction Loss: 0.0460, KL Divergence: 2.9110\n",
      "Epoch: 23/40, Total Loss: 0.0604, Reconstruction Loss: 0.0459, KL Divergence: 2.9097\n",
      "Epoch: 24/40, Total Loss: 0.0605, Reconstruction Loss: 0.0458, KL Divergence: 2.9411\n",
      "Epoch: 25/40, Total Loss: 0.0604, Reconstruction Loss: 0.0457, KL Divergence: 2.9461\n",
      "Epoch: 26/40, Total Loss: 0.0604, Reconstruction Loss: 0.0456, KL Divergence: 2.9558\n",
      "Epoch: 27/40, Total Loss: 0.0604, Reconstruction Loss: 0.0455, KL Divergence: 2.9745\n",
      "Epoch: 28/40, Total Loss: 0.0603, Reconstruction Loss: 0.0454, KL Divergence: 2.9759\n",
      "Epoch: 29/40, Total Loss: 0.0604, Reconstruction Loss: 0.0454, KL Divergence: 2.9940\n",
      "Epoch: 30/40, Total Loss: 0.0603, Reconstruction Loss: 0.0453, KL Divergence: 2.9915\n",
      "Epoch: 31/40, Total Loss: 0.0603, Reconstruction Loss: 0.0453, KL Divergence: 3.0024\n",
      "Epoch: 32/40, Total Loss: 0.0603, Reconstruction Loss: 0.0452, KL Divergence: 3.0149\n",
      "Epoch: 33/40, Total Loss: 0.0603, Reconstruction Loss: 0.0451, KL Divergence: 3.0234\n",
      "Epoch: 34/40, Total Loss: 0.0602, Reconstruction Loss: 0.0451, KL Divergence: 3.0322\n",
      "Epoch: 35/40, Total Loss: 0.0603, Reconstruction Loss: 0.0450, KL Divergence: 3.0440\n",
      "Epoch: 36/40, Total Loss: 0.0602, Reconstruction Loss: 0.0450, KL Divergence: 3.0460\n",
      "Epoch: 37/40, Total Loss: 0.0602, Reconstruction Loss: 0.0449, KL Divergence: 3.0501\n",
      "Epoch: 38/40, Total Loss: 0.0601, Reconstruction Loss: 0.0449, KL Divergence: 3.0428\n",
      "Epoch: 39/40, Total Loss: 0.0602, Reconstruction Loss: 0.0449, KL Divergence: 3.0600\n",
      "Epoch: 40/40, Total Loss: 0.0602, Reconstruction Loss: 0.0448, KL Divergence: 3.0712\n",
      "Test Loss: Total Loss: 0.0601, Reconstruction Loss: 0.0441, KL Divergence: 3.1949\n",
      "tensor(0.0462, grad_fn=<MseLossBackward0>)\n",
      "Traing VAE with latent dim: 2\n",
      "Test Loss: Total Loss: 0.2457, Reconstruction Loss: 0.2457, KL Divergence: 0.0060\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005182981491088867,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 40,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6f992bd642e4096bc2b0334bb402755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40, Total Loss: 0.0865, Reconstruction Loss: 0.0846, KL Divergence: 0.3840\n",
      "Epoch: 2/40, Total Loss: 0.0662, Reconstruction Loss: 0.0624, KL Divergence: 0.7608\n",
      "Epoch: 3/40, Total Loss: 0.0653, Reconstruction Loss: 0.0607, KL Divergence: 0.9277\n",
      "Epoch: 4/40, Total Loss: 0.0650, Reconstruction Loss: 0.0600, KL Divergence: 0.9844\n",
      "Epoch: 5/40, Total Loss: 0.0648, Reconstruction Loss: 0.0597, KL Divergence: 1.0280\n",
      "Epoch: 6/40, Total Loss: 0.0647, Reconstruction Loss: 0.0594, KL Divergence: 1.0480\n",
      "Epoch: 7/40, Total Loss: 0.0646, Reconstruction Loss: 0.0593, KL Divergence: 1.0725\n",
      "Epoch: 8/40, Total Loss: 0.0645, Reconstruction Loss: 0.0591, KL Divergence: 1.0903\n",
      "Epoch: 9/40, Total Loss: 0.0644, Reconstruction Loss: 0.0589, KL Divergence: 1.1002\n",
      "Epoch: 10/40, Total Loss: 0.0644, Reconstruction Loss: 0.0587, KL Divergence: 1.1219\n",
      "Epoch: 11/40, Total Loss: 0.0643, Reconstruction Loss: 0.0586, KL Divergence: 1.1253\n",
      "Epoch: 12/40, Total Loss: 0.0642, Reconstruction Loss: 0.0585, KL Divergence: 1.1351\n",
      "Epoch: 13/40, Total Loss: 0.0642, Reconstruction Loss: 0.0584, KL Divergence: 1.1507\n",
      "Epoch: 14/40, Total Loss: 0.0641, Reconstruction Loss: 0.0583, KL Divergence: 1.1491\n",
      "Epoch: 15/40, Total Loss: 0.0641, Reconstruction Loss: 0.0583, KL Divergence: 1.1644\n",
      "Epoch: 16/40, Total Loss: 0.0641, Reconstruction Loss: 0.0582, KL Divergence: 1.1750\n",
      "Epoch: 17/40, Total Loss: 0.0640, Reconstruction Loss: 0.0581, KL Divergence: 1.1702\n",
      "Epoch: 18/40, Total Loss: 0.0641, Reconstruction Loss: 0.0581, KL Divergence: 1.1914\n",
      "Epoch: 19/40, Total Loss: 0.0640, Reconstruction Loss: 0.0580, KL Divergence: 1.1922\n",
      "Epoch: 20/40, Total Loss: 0.0640, Reconstruction Loss: 0.0579, KL Divergence: 1.2015\n",
      "Epoch: 21/40, Total Loss: 0.0639, Reconstruction Loss: 0.0578, KL Divergence: 1.2143\n",
      "Epoch: 22/40, Total Loss: 0.0639, Reconstruction Loss: 0.0578, KL Divergence: 1.2186\n",
      "Epoch: 23/40, Total Loss: 0.0639, Reconstruction Loss: 0.0578, KL Divergence: 1.2215\n",
      "Epoch: 24/40, Total Loss: 0.0639, Reconstruction Loss: 0.0577, KL Divergence: 1.2361\n",
      "Epoch: 25/40, Total Loss: 0.0638, Reconstruction Loss: 0.0576, KL Divergence: 1.2444\n",
      "Epoch: 26/40, Total Loss: 0.0638, Reconstruction Loss: 0.0576, KL Divergence: 1.2445\n",
      "Epoch: 27/40, Total Loss: 0.0638, Reconstruction Loss: 0.0575, KL Divergence: 1.2510\n",
      "Epoch: 28/40, Total Loss: 0.0638, Reconstruction Loss: 0.0574, KL Divergence: 1.2717\n",
      "Epoch: 29/40, Total Loss: 0.0637, Reconstruction Loss: 0.0574, KL Divergence: 1.2646\n",
      "Epoch: 30/40, Total Loss: 0.0638, Reconstruction Loss: 0.0574, KL Divergence: 1.2732\n",
      "Epoch: 31/40, Total Loss: 0.0637, Reconstruction Loss: 0.0573, KL Divergence: 1.2771\n",
      "Epoch: 32/40, Total Loss: 0.0637, Reconstruction Loss: 0.0573, KL Divergence: 1.2826\n",
      "Epoch: 33/40, Total Loss: 0.0637, Reconstruction Loss: 0.0572, KL Divergence: 1.2978\n",
      "Epoch: 34/40, Total Loss: 0.0637, Reconstruction Loss: 0.0572, KL Divergence: 1.2942\n",
      "Epoch: 35/40, Total Loss: 0.0636, Reconstruction Loss: 0.0572, KL Divergence: 1.2932\n",
      "Epoch: 36/40, Total Loss: 0.0637, Reconstruction Loss: 0.0571, KL Divergence: 1.3009\n",
      "Epoch: 37/40, Total Loss: 0.0637, Reconstruction Loss: 0.0571, KL Divergence: 1.3130\n",
      "Epoch: 38/40, Total Loss: 0.0637, Reconstruction Loss: 0.0571, KL Divergence: 1.3192\n",
      "Epoch: 39/40, Total Loss: 0.0636, Reconstruction Loss: 0.0570, KL Divergence: 1.3244\n",
      "Epoch: 40/40, Total Loss: 0.0636, Reconstruction Loss: 0.0570, KL Divergence: 1.3165\n",
      "Test Loss: Total Loss: 0.0635, Reconstruction Loss: 0.0563, KL Divergence: 1.4412\n",
      "tensor(0.0575, grad_fn=<MseLossBackward0>)\n",
      "Traing VAE with latent dim: 5\n",
      "Test Loss: Total Loss: 0.2278, Reconstruction Loss: 0.2277, KL Divergence: 0.0162\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006349325180053711,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 40,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a80f69d709ef461faec8af6f37f1af9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40, Total Loss: 0.0844, Reconstruction Loss: 0.0829, KL Divergence: 0.3000\n",
      "Epoch: 2/40, Total Loss: 0.0663, Reconstruction Loss: 0.0624, KL Divergence: 0.7883\n",
      "Epoch: 3/40, Total Loss: 0.0652, Reconstruction Loss: 0.0600, KL Divergence: 1.0476\n",
      "Epoch: 4/40, Total Loss: 0.0648, Reconstruction Loss: 0.0589, KL Divergence: 1.1688\n",
      "Epoch: 5/40, Total Loss: 0.0644, Reconstruction Loss: 0.0581, KL Divergence: 1.2613\n",
      "Epoch: 6/40, Total Loss: 0.0642, Reconstruction Loss: 0.0574, KL Divergence: 1.3555\n",
      "Epoch: 7/40, Total Loss: 0.0640, Reconstruction Loss: 0.0570, KL Divergence: 1.4038\n",
      "Epoch: 8/40, Total Loss: 0.0638, Reconstruction Loss: 0.0566, KL Divergence: 1.4465\n",
      "Epoch: 9/40, Total Loss: 0.0637, Reconstruction Loss: 0.0563, KL Divergence: 1.4813\n",
      "Epoch: 10/40, Total Loss: 0.0637, Reconstruction Loss: 0.0561, KL Divergence: 1.5174\n",
      "Epoch: 11/40, Total Loss: 0.0636, Reconstruction Loss: 0.0558, KL Divergence: 1.5459\n",
      "Epoch: 12/40, Total Loss: 0.0635, Reconstruction Loss: 0.0556, KL Divergence: 1.5713\n",
      "Epoch: 13/40, Total Loss: 0.0635, Reconstruction Loss: 0.0554, KL Divergence: 1.6154\n",
      "Epoch: 14/40, Total Loss: 0.0634, Reconstruction Loss: 0.0552, KL Divergence: 1.6412\n",
      "Epoch: 15/40, Total Loss: 0.0633, Reconstruction Loss: 0.0550, KL Divergence: 1.6619\n",
      "Epoch: 16/40, Total Loss: 0.0632, Reconstruction Loss: 0.0548, KL Divergence: 1.6906\n",
      "Epoch: 17/40, Total Loss: 0.0632, Reconstruction Loss: 0.0547, KL Divergence: 1.7052\n",
      "Epoch: 18/40, Total Loss: 0.0632, Reconstruction Loss: 0.0545, KL Divergence: 1.7323\n",
      "Epoch: 19/40, Total Loss: 0.0631, Reconstruction Loss: 0.0543, KL Divergence: 1.7527\n",
      "Epoch: 20/40, Total Loss: 0.0631, Reconstruction Loss: 0.0543, KL Divergence: 1.7652\n",
      "Epoch: 21/40, Total Loss: 0.0630, Reconstruction Loss: 0.0541, KL Divergence: 1.7732\n",
      "Epoch: 22/40, Total Loss: 0.0630, Reconstruction Loss: 0.0540, KL Divergence: 1.8009\n",
      "Epoch: 23/40, Total Loss: 0.0630, Reconstruction Loss: 0.0540, KL Divergence: 1.8087\n",
      "Epoch: 24/40, Total Loss: 0.0630, Reconstruction Loss: 0.0539, KL Divergence: 1.8069\n",
      "Epoch: 25/40, Total Loss: 0.0629, Reconstruction Loss: 0.0538, KL Divergence: 1.8292\n",
      "Epoch: 26/40, Total Loss: 0.0629, Reconstruction Loss: 0.0538, KL Divergence: 1.8309\n",
      "Epoch: 27/40, Total Loss: 0.0629, Reconstruction Loss: 0.0538, KL Divergence: 1.8295\n",
      "Epoch: 28/40, Total Loss: 0.0629, Reconstruction Loss: 0.0537, KL Divergence: 1.8435\n",
      "Epoch: 29/40, Total Loss: 0.0629, Reconstruction Loss: 0.0537, KL Divergence: 1.8449\n",
      "Epoch: 30/40, Total Loss: 0.0628, Reconstruction Loss: 0.0536, KL Divergence: 1.8440\n",
      "Epoch: 31/40, Total Loss: 0.0628, Reconstruction Loss: 0.0535, KL Divergence: 1.8600\n",
      "Epoch: 32/40, Total Loss: 0.0629, Reconstruction Loss: 0.0535, KL Divergence: 1.8772\n",
      "Epoch: 33/40, Total Loss: 0.0628, Reconstruction Loss: 0.0535, KL Divergence: 1.8671\n",
      "Epoch: 34/40, Total Loss: 0.0629, Reconstruction Loss: 0.0535, KL Divergence: 1.8810\n",
      "Epoch: 35/40, Total Loss: 0.0628, Reconstruction Loss: 0.0534, KL Divergence: 1.8834\n",
      "Epoch: 36/40, Total Loss: 0.0628, Reconstruction Loss: 0.0534, KL Divergence: 1.8810\n",
      "Epoch: 37/40, Total Loss: 0.0628, Reconstruction Loss: 0.0533, KL Divergence: 1.8996\n",
      "Epoch: 38/40, Total Loss: 0.0627, Reconstruction Loss: 0.0533, KL Divergence: 1.8854\n",
      "Epoch: 39/40, Total Loss: 0.0628, Reconstruction Loss: 0.0533, KL Divergence: 1.8918\n",
      "Epoch: 40/40, Total Loss: 0.0628, Reconstruction Loss: 0.0533, KL Divergence: 1.9073\n",
      "Test Loss: Total Loss: 0.0625, Reconstruction Loss: 0.0531, KL Divergence: 1.8794\n",
      "tensor(0.0534, grad_fn=<MseLossBackward0>)\n",
      "Traing VAE with latent dim: 10\n",
      "Test Loss: Total Loss: 0.2160, Reconstruction Loss: 0.2157, KL Divergence: 0.0657\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009018898010253906,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 40,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "483a44135e6f4b9793618b9da7a7e672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40, Total Loss: 0.0841, Reconstruction Loss: 0.0833, KL Divergence: 0.1567\n",
      "Epoch: 2/40, Total Loss: 0.0668, Reconstruction Loss: 0.0637, KL Divergence: 0.6143\n",
      "Epoch: 3/40, Total Loss: 0.0655, Reconstruction Loss: 0.0606, KL Divergence: 0.9793\n",
      "Epoch: 4/40, Total Loss: 0.0651, Reconstruction Loss: 0.0592, KL Divergence: 1.1734\n",
      "Epoch: 5/40, Total Loss: 0.0646, Reconstruction Loss: 0.0582, KL Divergence: 1.2870\n",
      "Epoch: 6/40, Total Loss: 0.0644, Reconstruction Loss: 0.0575, KL Divergence: 1.3788\n",
      "Epoch: 7/40, Total Loss: 0.0641, Reconstruction Loss: 0.0568, KL Divergence: 1.4607\n",
      "Epoch: 8/40, Total Loss: 0.0640, Reconstruction Loss: 0.0563, KL Divergence: 1.5334\n",
      "Epoch: 9/40, Total Loss: 0.0638, Reconstruction Loss: 0.0559, KL Divergence: 1.5750\n",
      "Epoch: 10/40, Total Loss: 0.0637, Reconstruction Loss: 0.0557, KL Divergence: 1.6037\n",
      "Epoch: 11/40, Total Loss: 0.0636, Reconstruction Loss: 0.0554, KL Divergence: 1.6447\n",
      "Epoch: 12/40, Total Loss: 0.0635, Reconstruction Loss: 0.0552, KL Divergence: 1.6628\n",
      "Epoch: 13/40, Total Loss: 0.0635, Reconstruction Loss: 0.0550, KL Divergence: 1.7043\n",
      "Epoch: 14/40, Total Loss: 0.0634, Reconstruction Loss: 0.0548, KL Divergence: 1.7104\n",
      "Epoch: 15/40, Total Loss: 0.0633, Reconstruction Loss: 0.0546, KL Divergence: 1.7220\n",
      "Epoch: 16/40, Total Loss: 0.0633, Reconstruction Loss: 0.0545, KL Divergence: 1.7698\n",
      "Epoch: 17/40, Total Loss: 0.0633, Reconstruction Loss: 0.0544, KL Divergence: 1.7835\n",
      "Epoch: 18/40, Total Loss: 0.0632, Reconstruction Loss: 0.0542, KL Divergence: 1.7932\n",
      "Epoch: 19/40, Total Loss: 0.0632, Reconstruction Loss: 0.0541, KL Divergence: 1.8306\n",
      "Epoch: 20/40, Total Loss: 0.0631, Reconstruction Loss: 0.0540, KL Divergence: 1.8301\n",
      "Epoch: 21/40, Total Loss: 0.0631, Reconstruction Loss: 0.0539, KL Divergence: 1.8509\n",
      "Epoch: 22/40, Total Loss: 0.0631, Reconstruction Loss: 0.0538, KL Divergence: 1.8604\n",
      "Epoch: 23/40, Total Loss: 0.0631, Reconstruction Loss: 0.0537, KL Divergence: 1.8653\n",
      "Epoch: 24/40, Total Loss: 0.0630, Reconstruction Loss: 0.0536, KL Divergence: 1.8722\n",
      "Epoch: 25/40, Total Loss: 0.0630, Reconstruction Loss: 0.0536, KL Divergence: 1.8750\n",
      "Epoch: 26/40, Total Loss: 0.0630, Reconstruction Loss: 0.0536, KL Divergence: 1.8846\n",
      "Epoch: 27/40, Total Loss: 0.0629, Reconstruction Loss: 0.0534, KL Divergence: 1.9030\n",
      "Epoch: 28/40, Total Loss: 0.0630, Reconstruction Loss: 0.0534, KL Divergence: 1.9164\n",
      "Epoch: 29/40, Total Loss: 0.0630, Reconstruction Loss: 0.0533, KL Divergence: 1.9302\n",
      "Epoch: 30/40, Total Loss: 0.0629, Reconstruction Loss: 0.0533, KL Divergence: 1.9322\n",
      "Epoch: 31/40, Total Loss: 0.0629, Reconstruction Loss: 0.0532, KL Divergence: 1.9337\n",
      "Epoch: 32/40, Total Loss: 0.0628, Reconstruction Loss: 0.0532, KL Divergence: 1.9286\n",
      "Epoch: 33/40, Total Loss: 0.0629, Reconstruction Loss: 0.0532, KL Divergence: 1.9406\n",
      "Epoch: 34/40, Total Loss: 0.0629, Reconstruction Loss: 0.0531, KL Divergence: 1.9535\n",
      "Epoch: 35/40, Total Loss: 0.0629, Reconstruction Loss: 0.0530, KL Divergence: 1.9673\n",
      "Epoch: 36/40, Total Loss: 0.0628, Reconstruction Loss: 0.0531, KL Divergence: 1.9472\n",
      "Epoch: 37/40, Total Loss: 0.0628, Reconstruction Loss: 0.0529, KL Divergence: 1.9613\n",
      "Epoch: 38/40, Total Loss: 0.0628, Reconstruction Loss: 0.0529, KL Divergence: 1.9674\n",
      "Epoch: 39/40, Total Loss: 0.0628, Reconstruction Loss: 0.0530, KL Divergence: 1.9690\n",
      "Epoch: 40/40, Total Loss: 0.0628, Reconstruction Loss: 0.0529, KL Divergence: 1.9646\n",
      "Test Loss: Total Loss: 0.0626, Reconstruction Loss: 0.0527, KL Divergence: 1.9679\n",
      "tensor(0.0534, grad_fn=<MseLossBackward0>)\n",
      "Traing VAE with latent dim: 20\n",
      "Test Loss: Total Loss: 0.2499, Reconstruction Loss: 0.2495, KL Divergence: 0.0820\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009524345397949219,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 40,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6161a4252b84f9f991556bd68c022e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40, Total Loss: 0.0867, Reconstruction Loss: 0.0852, KL Divergence: 0.3027\n",
      "Epoch: 2/40, Total Loss: 0.0668, Reconstruction Loss: 0.0639, KL Divergence: 0.5888\n",
      "Epoch: 3/40, Total Loss: 0.0655, Reconstruction Loss: 0.0607, KL Divergence: 0.9576\n",
      "Epoch: 4/40, Total Loss: 0.0649, Reconstruction Loss: 0.0592, KL Divergence: 1.1522\n",
      "Epoch: 5/40, Total Loss: 0.0646, Reconstruction Loss: 0.0583, KL Divergence: 1.2519\n",
      "Epoch: 6/40, Total Loss: 0.0644, Reconstruction Loss: 0.0576, KL Divergence: 1.3539\n",
      "Epoch: 7/40, Total Loss: 0.0642, Reconstruction Loss: 0.0568, KL Divergence: 1.4671\n",
      "Epoch: 8/40, Total Loss: 0.0640, Reconstruction Loss: 0.0564, KL Divergence: 1.5171\n",
      "Epoch: 9/40, Total Loss: 0.0639, Reconstruction Loss: 0.0560, KL Divergence: 1.5761\n",
      "Epoch: 10/40, Total Loss: 0.0637, Reconstruction Loss: 0.0557, KL Divergence: 1.6085\n",
      "Epoch: 11/40, Total Loss: 0.0636, Reconstruction Loss: 0.0554, KL Divergence: 1.6368\n",
      "Epoch: 12/40, Total Loss: 0.0636, Reconstruction Loss: 0.0553, KL Divergence: 1.6702\n",
      "Epoch: 13/40, Total Loss: 0.0635, Reconstruction Loss: 0.0550, KL Divergence: 1.7006\n",
      "Epoch: 14/40, Total Loss: 0.0634, Reconstruction Loss: 0.0547, KL Divergence: 1.7394\n",
      "Epoch: 15/40, Total Loss: 0.0633, Reconstruction Loss: 0.0545, KL Divergence: 1.7639\n",
      "Epoch: 16/40, Total Loss: 0.0634, Reconstruction Loss: 0.0544, KL Divergence: 1.7858\n",
      "Epoch: 17/40, Total Loss: 0.0633, Reconstruction Loss: 0.0543, KL Divergence: 1.7988\n",
      "Epoch: 18/40, Total Loss: 0.0632, Reconstruction Loss: 0.0541, KL Divergence: 1.8114\n",
      "Epoch: 19/40, Total Loss: 0.0631, Reconstruction Loss: 0.0540, KL Divergence: 1.8247\n",
      "Epoch: 20/40, Total Loss: 0.0631, Reconstruction Loss: 0.0539, KL Divergence: 1.8428\n",
      "Epoch: 21/40, Total Loss: 0.0631, Reconstruction Loss: 0.0539, KL Divergence: 1.8537\n",
      "Epoch: 22/40, Total Loss: 0.0631, Reconstruction Loss: 0.0538, KL Divergence: 1.8613\n",
      "Epoch: 23/40, Total Loss: 0.0630, Reconstruction Loss: 0.0537, KL Divergence: 1.8703\n",
      "Epoch: 24/40, Total Loss: 0.0630, Reconstruction Loss: 0.0536, KL Divergence: 1.8808\n",
      "Epoch: 25/40, Total Loss: 0.0630, Reconstruction Loss: 0.0536, KL Divergence: 1.8887\n",
      "Epoch: 26/40, Total Loss: 0.0629, Reconstruction Loss: 0.0535, KL Divergence: 1.8929\n",
      "Epoch: 27/40, Total Loss: 0.0630, Reconstruction Loss: 0.0535, KL Divergence: 1.9034\n",
      "Epoch: 28/40, Total Loss: 0.0629, Reconstruction Loss: 0.0534, KL Divergence: 1.9068\n",
      "Epoch: 29/40, Total Loss: 0.0629, Reconstruction Loss: 0.0533, KL Divergence: 1.9061\n",
      "Epoch: 30/40, Total Loss: 0.0629, Reconstruction Loss: 0.0533, KL Divergence: 1.9193\n",
      "Epoch: 31/40, Total Loss: 0.0629, Reconstruction Loss: 0.0533, KL Divergence: 1.9092\n",
      "Epoch: 32/40, Total Loss: 0.0629, Reconstruction Loss: 0.0532, KL Divergence: 1.9344\n",
      "Epoch: 33/40, Total Loss: 0.0628, Reconstruction Loss: 0.0532, KL Divergence: 1.9228\n",
      "Epoch: 34/40, Total Loss: 0.0629, Reconstruction Loss: 0.0532, KL Divergence: 1.9324\n",
      "Epoch: 35/40, Total Loss: 0.0628, Reconstruction Loss: 0.0532, KL Divergence: 1.9284\n",
      "Epoch: 36/40, Total Loss: 0.0628, Reconstruction Loss: 0.0531, KL Divergence: 1.9366\n",
      "Epoch: 37/40, Total Loss: 0.0628, Reconstruction Loss: 0.0531, KL Divergence: 1.9497\n",
      "Epoch: 38/40, Total Loss: 0.0628, Reconstruction Loss: 0.0530, KL Divergence: 1.9442\n",
      "Epoch: 39/40, Total Loss: 0.0628, Reconstruction Loss: 0.0530, KL Divergence: 1.9448\n",
      "Epoch: 40/40, Total Loss: 0.0627, Reconstruction Loss: 0.0530, KL Divergence: 1.9408\n",
      "Test Loss: Total Loss: 0.0627, Reconstruction Loss: 0.0533, KL Divergence: 1.8709\n",
      "tensor(0.0531, grad_fn=<MseLossBackward0>)\n",
      "[Training beta = 0.01]\n",
      "Traing NaiveVAE with latent dim: 2\n",
      "Test Loss: Total Loss: 0.2360, Reconstruction Loss: 0.2359, KL Divergence: 0.0133\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005578279495239258,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 40,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26586ef8213e4444af195f0c6c345177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40, Total Loss: 0.0693, Reconstruction Loss: 0.0676, KL Divergence: 0.1635\n",
      "Epoch: 2/40, Total Loss: 0.0675, Reconstruction Loss: 0.0653, KL Divergence: 0.2182\n",
      "Epoch: 3/40, Total Loss: 0.0673, Reconstruction Loss: 0.0649, KL Divergence: 0.2409\n",
      "Epoch: 4/40, Total Loss: 0.0672, Reconstruction Loss: 0.0647, KL Divergence: 0.2478\n",
      "Epoch: 5/40, Total Loss: 0.0672, Reconstruction Loss: 0.0647, KL Divergence: 0.2531\n",
      "Epoch: 6/40, Total Loss: 0.0671, Reconstruction Loss: 0.0646, KL Divergence: 0.2543\n",
      "Epoch: 7/40, Total Loss: 0.0671, Reconstruction Loss: 0.0646, KL Divergence: 0.2562\n",
      "Epoch: 8/40, Total Loss: 0.0671, Reconstruction Loss: 0.0645, KL Divergence: 0.2579\n",
      "Epoch: 9/40, Total Loss: 0.0671, Reconstruction Loss: 0.0645, KL Divergence: 0.2623\n",
      "Epoch: 10/40, Total Loss: 0.0670, Reconstruction Loss: 0.0644, KL Divergence: 0.2617\n",
      "Epoch: 11/40, Total Loss: 0.0670, Reconstruction Loss: 0.0644, KL Divergence: 0.2653\n",
      "Epoch: 12/40, Total Loss: 0.0671, Reconstruction Loss: 0.0644, KL Divergence: 0.2646\n",
      "Epoch: 13/40, Total Loss: 0.0670, Reconstruction Loss: 0.0644, KL Divergence: 0.2637\n",
      "Epoch: 14/40, Total Loss: 0.0670, Reconstruction Loss: 0.0643, KL Divergence: 0.2652\n",
      "Epoch: 15/40, Total Loss: 0.0670, Reconstruction Loss: 0.0644, KL Divergence: 0.2654\n",
      "Epoch: 16/40, Total Loss: 0.0670, Reconstruction Loss: 0.0643, KL Divergence: 0.2733\n",
      "Epoch: 17/40, Total Loss: 0.0670, Reconstruction Loss: 0.0643, KL Divergence: 0.2666\n",
      "Epoch: 18/40, Total Loss: 0.0670, Reconstruction Loss: 0.0642, KL Divergence: 0.2764\n",
      "Epoch: 19/40, Total Loss: 0.0670, Reconstruction Loss: 0.0643, KL Divergence: 0.2644\n",
      "Epoch: 20/40, Total Loss: 0.0670, Reconstruction Loss: 0.0642, KL Divergence: 0.2744\n",
      "Epoch: 21/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2711\n",
      "Epoch: 22/40, Total Loss: 0.0670, Reconstruction Loss: 0.0642, KL Divergence: 0.2728\n",
      "Epoch: 23/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2750\n",
      "Epoch: 24/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2719\n",
      "Epoch: 25/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2693\n",
      "Epoch: 26/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2680\n",
      "Epoch: 27/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2706\n",
      "Epoch: 28/40, Total Loss: 0.0670, Reconstruction Loss: 0.0642, KL Divergence: 0.2748\n",
      "Epoch: 29/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2708\n",
      "Epoch: 30/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2689\n",
      "Epoch: 31/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2759\n",
      "Epoch: 32/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2714\n",
      "Epoch: 33/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2724\n",
      "Epoch: 34/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2735\n",
      "Epoch: 35/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2716\n",
      "Epoch: 36/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2705\n",
      "Epoch: 37/40, Total Loss: 0.0669, Reconstruction Loss: 0.0641, KL Divergence: 0.2742\n",
      "Epoch: 38/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2759\n",
      "Epoch: 39/40, Total Loss: 0.0669, Reconstruction Loss: 0.0641, KL Divergence: 0.2716\n",
      "Epoch: 40/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2739\n",
      "Test Loss: Total Loss: 0.0666, Reconstruction Loss: 0.0640, KL Divergence: 0.2596\n",
      "tensor(0.0644, grad_fn=<MseLossBackward0>)\n",
      "Traing NaiveVAE with latent dim: 5\n",
      "Test Loss: Total Loss: 0.2370, Reconstruction Loss: 0.2366, KL Divergence: 0.0385\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005515575408935547,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 40,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c1261c466a4a188af087864d71bee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40, Total Loss: 0.0704, Reconstruction Loss: 0.0682, KL Divergence: 0.2228\n",
      "Epoch: 2/40, Total Loss: 0.0675, Reconstruction Loss: 0.0656, KL Divergence: 0.1964\n",
      "Epoch: 3/40, Total Loss: 0.0674, Reconstruction Loss: 0.0653, KL Divergence: 0.2054\n",
      "Epoch: 4/40, Total Loss: 0.0673, Reconstruction Loss: 0.0651, KL Divergence: 0.2207\n",
      "Epoch: 5/40, Total Loss: 0.0672, Reconstruction Loss: 0.0650, KL Divergence: 0.2294\n",
      "Epoch: 6/40, Total Loss: 0.0672, Reconstruction Loss: 0.0648, KL Divergence: 0.2353\n",
      "Epoch: 7/40, Total Loss: 0.0672, Reconstruction Loss: 0.0648, KL Divergence: 0.2383\n",
      "Epoch: 8/40, Total Loss: 0.0671, Reconstruction Loss: 0.0647, KL Divergence: 0.2371\n",
      "Epoch: 9/40, Total Loss: 0.0671, Reconstruction Loss: 0.0646, KL Divergence: 0.2425\n",
      "Epoch: 10/40, Total Loss: 0.0670, Reconstruction Loss: 0.0646, KL Divergence: 0.2420\n",
      "Epoch: 11/40, Total Loss: 0.0670, Reconstruction Loss: 0.0645, KL Divergence: 0.2483\n",
      "Epoch: 12/40, Total Loss: 0.0670, Reconstruction Loss: 0.0645, KL Divergence: 0.2506\n",
      "Epoch: 13/40, Total Loss: 0.0670, Reconstruction Loss: 0.0646, KL Divergence: 0.2461\n",
      "Epoch: 14/40, Total Loss: 0.0670, Reconstruction Loss: 0.0644, KL Divergence: 0.2550\n",
      "Epoch: 15/40, Total Loss: 0.0670, Reconstruction Loss: 0.0644, KL Divergence: 0.2555\n",
      "Epoch: 16/40, Total Loss: 0.0670, Reconstruction Loss: 0.0643, KL Divergence: 0.2632\n",
      "Epoch: 17/40, Total Loss: 0.0669, Reconstruction Loss: 0.0643, KL Divergence: 0.2620\n",
      "Epoch: 18/40, Total Loss: 0.0669, Reconstruction Loss: 0.0643, KL Divergence: 0.2640\n",
      "Epoch: 19/40, Total Loss: 0.0670, Reconstruction Loss: 0.0643, KL Divergence: 0.2681\n",
      "Epoch: 20/40, Total Loss: 0.0669, Reconstruction Loss: 0.0643, KL Divergence: 0.2632\n",
      "Epoch: 21/40, Total Loss: 0.0670, Reconstruction Loss: 0.0644, KL Divergence: 0.2592\n",
      "Epoch: 22/40, Total Loss: 0.0669, Reconstruction Loss: 0.0643, KL Divergence: 0.2613\n",
      "Epoch: 23/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2680\n",
      "Epoch: 24/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2723\n",
      "Epoch: 25/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2696\n",
      "Epoch: 26/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2713\n",
      "Epoch: 27/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2743\n",
      "Epoch: 28/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2698\n",
      "Epoch: 29/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2710\n",
      "Epoch: 30/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2705\n",
      "Epoch: 31/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2700\n",
      "Epoch: 32/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2726\n",
      "Epoch: 33/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2718\n",
      "Epoch: 34/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2703\n",
      "Epoch: 35/40, Total Loss: 0.0669, Reconstruction Loss: 0.0641, KL Divergence: 0.2754\n",
      "Epoch: 36/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2720\n",
      "Epoch: 37/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2697\n",
      "Epoch: 38/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2710\n",
      "Epoch: 39/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2699\n",
      "Epoch: 40/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2695\n",
      "Test Loss: Total Loss: 0.0666, Reconstruction Loss: 0.0642, KL Divergence: 0.2485\n",
      "tensor(0.0658, grad_fn=<MseLossBackward0>)\n",
      "Traing NaiveVAE with latent dim: 10\n",
      "Test Loss: Total Loss: 0.2357, Reconstruction Loss: 0.2349, KL Divergence: 0.0874\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005522012710571289,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 40,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c582709d6cea4b03a95e4496dfb3f5c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40, Total Loss: 0.0712, Reconstruction Loss: 0.0689, KL Divergence: 0.2387\n",
      "Epoch: 2/40, Total Loss: 0.0676, Reconstruction Loss: 0.0657, KL Divergence: 0.1863\n",
      "Epoch: 3/40, Total Loss: 0.0675, Reconstruction Loss: 0.0657, KL Divergence: 0.1748\n",
      "Epoch: 4/40, Total Loss: 0.0674, Reconstruction Loss: 0.0656, KL Divergence: 0.1831\n",
      "Epoch: 5/40, Total Loss: 0.0673, Reconstruction Loss: 0.0654, KL Divergence: 0.1939\n",
      "Epoch: 6/40, Total Loss: 0.0672, Reconstruction Loss: 0.0651, KL Divergence: 0.2116\n",
      "Epoch: 7/40, Total Loss: 0.0672, Reconstruction Loss: 0.0650, KL Divergence: 0.2154\n",
      "Epoch: 8/40, Total Loss: 0.0672, Reconstruction Loss: 0.0649, KL Divergence: 0.2239\n",
      "Epoch: 9/40, Total Loss: 0.0671, Reconstruction Loss: 0.0648, KL Divergence: 0.2238\n",
      "Epoch: 10/40, Total Loss: 0.0671, Reconstruction Loss: 0.0648, KL Divergence: 0.2324\n",
      "Epoch: 11/40, Total Loss: 0.0671, Reconstruction Loss: 0.0647, KL Divergence: 0.2311\n",
      "Epoch: 12/40, Total Loss: 0.0671, Reconstruction Loss: 0.0647, KL Divergence: 0.2394\n",
      "Epoch: 13/40, Total Loss: 0.0670, Reconstruction Loss: 0.0646, KL Divergence: 0.2405\n",
      "Epoch: 14/40, Total Loss: 0.0670, Reconstruction Loss: 0.0646, KL Divergence: 0.2453\n",
      "Epoch: 15/40, Total Loss: 0.0670, Reconstruction Loss: 0.0646, KL Divergence: 0.2471\n",
      "Epoch: 16/40, Total Loss: 0.0671, Reconstruction Loss: 0.0645, KL Divergence: 0.2528\n",
      "Epoch: 17/40, Total Loss: 0.0670, Reconstruction Loss: 0.0645, KL Divergence: 0.2509\n",
      "Epoch: 18/40, Total Loss: 0.0670, Reconstruction Loss: 0.0644, KL Divergence: 0.2548\n",
      "Epoch: 19/40, Total Loss: 0.0670, Reconstruction Loss: 0.0645, KL Divergence: 0.2523\n",
      "Epoch: 20/40, Total Loss: 0.0670, Reconstruction Loss: 0.0644, KL Divergence: 0.2580\n",
      "Epoch: 21/40, Total Loss: 0.0670, Reconstruction Loss: 0.0644, KL Divergence: 0.2585\n",
      "Epoch: 22/40, Total Loss: 0.0670, Reconstruction Loss: 0.0644, KL Divergence: 0.2623\n",
      "Epoch: 23/40, Total Loss: 0.0669, Reconstruction Loss: 0.0643, KL Divergence: 0.2622\n",
      "Epoch: 24/40, Total Loss: 0.0670, Reconstruction Loss: 0.0643, KL Divergence: 0.2644\n",
      "Epoch: 25/40, Total Loss: 0.0669, Reconstruction Loss: 0.0643, KL Divergence: 0.2643\n",
      "Epoch: 26/40, Total Loss: 0.0670, Reconstruction Loss: 0.0644, KL Divergence: 0.2627\n",
      "Epoch: 27/40, Total Loss: 0.0669, Reconstruction Loss: 0.0643, KL Divergence: 0.2648\n",
      "Epoch: 28/40, Total Loss: 0.0669, Reconstruction Loss: 0.0643, KL Divergence: 0.2681\n",
      "Epoch: 29/40, Total Loss: 0.0669, Reconstruction Loss: 0.0643, KL Divergence: 0.2639\n",
      "Epoch: 30/40, Total Loss: 0.0669, Reconstruction Loss: 0.0643, KL Divergence: 0.2638\n",
      "Epoch: 31/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2700\n",
      "Epoch: 32/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2713\n",
      "Epoch: 33/40, Total Loss: 0.0669, Reconstruction Loss: 0.0643, KL Divergence: 0.2646\n",
      "Epoch: 34/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2747\n",
      "Epoch: 35/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2742\n",
      "Epoch: 36/40, Total Loss: 0.0670, Reconstruction Loss: 0.0643, KL Divergence: 0.2715\n",
      "Epoch: 37/40, Total Loss: 0.0669, Reconstruction Loss: 0.0643, KL Divergence: 0.2689\n",
      "Epoch: 38/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2677\n",
      "Epoch: 39/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2726\n",
      "Epoch: 40/40, Total Loss: 0.0669, Reconstruction Loss: 0.0642, KL Divergence: 0.2746\n",
      "Test Loss: Total Loss: 0.0667, Reconstruction Loss: 0.0640, KL Divergence: 0.2636\n",
      "tensor(0.0658, grad_fn=<MseLossBackward0>)\n",
      "Traing NaiveVAE with latent dim: 20\n",
      "Test Loss: Total Loss: 0.2384, Reconstruction Loss: 0.2361, KL Divergence: 0.2323\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008670806884765625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 40,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3c6c797b8f040b89aed6e37fa13793c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40, Total Loss: 0.0726, Reconstruction Loss: 0.0698, KL Divergence: 0.2798\n",
      "Epoch: 2/40, Total Loss: 0.0677, Reconstruction Loss: 0.0662, KL Divergence: 0.1468\n",
      "Epoch: 3/40, Total Loss: 0.0676, Reconstruction Loss: 0.0663, KL Divergence: 0.1336\n",
      "Epoch: 4/40, Total Loss: 0.0675, Reconstruction Loss: 0.0661, KL Divergence: 0.1463\n",
      "Epoch: 5/40, Total Loss: 0.0674, Reconstruction Loss: 0.0658, KL Divergence: 0.1628\n",
      "Epoch: 6/40, Total Loss: 0.0673, Reconstruction Loss: 0.0657, KL Divergence: 0.1653\n",
      "Epoch: 7/40, Total Loss: 0.0673, Reconstruction Loss: 0.0656, KL Divergence: 0.1700\n",
      "Epoch: 8/40, Total Loss: 0.0673, Reconstruction Loss: 0.0656, KL Divergence: 0.1731\n",
      "Epoch: 9/40, Total Loss: 0.0673, Reconstruction Loss: 0.0655, KL Divergence: 0.1768\n",
      "Epoch: 10/40, Total Loss: 0.0672, Reconstruction Loss: 0.0653, KL Divergence: 0.1970\n",
      "Epoch: 11/40, Total Loss: 0.0672, Reconstruction Loss: 0.0652, KL Divergence: 0.2005\n",
      "Epoch: 12/40, Total Loss: 0.0671, Reconstruction Loss: 0.0649, KL Divergence: 0.2153\n",
      "Epoch: 13/40, Total Loss: 0.0671, Reconstruction Loss: 0.0649, KL Divergence: 0.2223\n",
      "Epoch: 14/40, Total Loss: 0.0671, Reconstruction Loss: 0.0649, KL Divergence: 0.2239\n",
      "Epoch: 15/40, Total Loss: 0.0671, Reconstruction Loss: 0.0648, KL Divergence: 0.2272\n",
      "Epoch: 16/40, Total Loss: 0.0671, Reconstruction Loss: 0.0648, KL Divergence: 0.2320\n",
      "Epoch: 17/40, Total Loss: 0.0671, Reconstruction Loss: 0.0648, KL Divergence: 0.2360\n",
      "Epoch: 18/40, Total Loss: 0.0671, Reconstruction Loss: 0.0647, KL Divergence: 0.2348\n",
      "Epoch: 19/40, Total Loss: 0.0671, Reconstruction Loss: 0.0647, KL Divergence: 0.2403\n",
      "Epoch: 20/40, Total Loss: 0.0670, Reconstruction Loss: 0.0646, KL Divergence: 0.2459\n",
      "Epoch: 21/40, Total Loss: 0.0670, Reconstruction Loss: 0.0646, KL Divergence: 0.2411\n",
      "Epoch: 22/40, Total Loss: 0.0670, Reconstruction Loss: 0.0646, KL Divergence: 0.2433\n",
      "Epoch: 23/40, Total Loss: 0.0671, Reconstruction Loss: 0.0646, KL Divergence: 0.2477\n",
      "Epoch: 24/40, Total Loss: 0.0671, Reconstruction Loss: 0.0645, KL Divergence: 0.2509\n",
      "Epoch: 25/40, Total Loss: 0.0670, Reconstruction Loss: 0.0645, KL Divergence: 0.2445\n",
      "Epoch: 26/40, Total Loss: 0.0670, Reconstruction Loss: 0.0645, KL Divergence: 0.2483\n",
      "Epoch: 27/40, Total Loss: 0.0670, Reconstruction Loss: 0.0645, KL Divergence: 0.2551\n",
      "Epoch: 28/40, Total Loss: 0.0670, Reconstruction Loss: 0.0645, KL Divergence: 0.2490\n",
      "Epoch: 29/40, Total Loss: 0.0669, Reconstruction Loss: 0.0644, KL Divergence: 0.2510\n",
      "Epoch: 30/40, Total Loss: 0.0670, Reconstruction Loss: 0.0645, KL Divergence: 0.2496\n",
      "Epoch: 31/40, Total Loss: 0.0670, Reconstruction Loss: 0.0644, KL Divergence: 0.2542\n",
      "Epoch: 32/40, Total Loss: 0.0670, Reconstruction Loss: 0.0645, KL Divergence: 0.2556\n",
      "Epoch: 33/40, Total Loss: 0.0670, Reconstruction Loss: 0.0644, KL Divergence: 0.2543\n",
      "Epoch: 34/40, Total Loss: 0.0670, Reconstruction Loss: 0.0644, KL Divergence: 0.2565\n",
      "Epoch: 35/40, Total Loss: 0.0669, Reconstruction Loss: 0.0644, KL Divergence: 0.2570\n",
      "Epoch: 36/40, Total Loss: 0.0669, Reconstruction Loss: 0.0644, KL Divergence: 0.2597\n",
      "Epoch: 37/40, Total Loss: 0.0670, Reconstruction Loss: 0.0644, KL Divergence: 0.2602\n",
      "Epoch: 38/40, Total Loss: 0.0670, Reconstruction Loss: 0.0644, KL Divergence: 0.2603\n",
      "Epoch: 39/40, Total Loss: 0.0670, Reconstruction Loss: 0.0643, KL Divergence: 0.2626\n",
      "Epoch: 40/40, Total Loss: 0.0670, Reconstruction Loss: 0.0643, KL Divergence: 0.2645\n",
      "Test Loss: Total Loss: 0.0666, Reconstruction Loss: 0.0640, KL Divergence: 0.2630\n",
      "tensor(0.0637, grad_fn=<MseLossBackward0>)\n",
      "Traing VAE with latent dim: 2\n",
      "Test Loss: Total Loss: 0.2492, Reconstruction Loss: 0.2491, KL Divergence: 0.0165\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011036872863769531,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 40,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b2518c1c7714717996c5858f4f4bb44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40, Total Loss: 0.0847, Reconstruction Loss: 0.0845, KL Divergence: 0.0159\n",
      "Epoch: 2/40, Total Loss: 0.0683, Reconstruction Loss: 0.0681, KL Divergence: 0.0162\n",
      "Epoch: 3/40, Total Loss: 0.0678, Reconstruction Loss: 0.0671, KL Divergence: 0.0646\n",
      "Epoch: 4/40, Total Loss: 0.0675, Reconstruction Loss: 0.0664, KL Divergence: 0.1097\n",
      "Epoch: 5/40, Total Loss: 0.0675, Reconstruction Loss: 0.0661, KL Divergence: 0.1423\n",
      "Epoch: 6/40, Total Loss: 0.0674, Reconstruction Loss: 0.0659, KL Divergence: 0.1505\n",
      "Epoch: 7/40, Total Loss: 0.0673, Reconstruction Loss: 0.0657, KL Divergence: 0.1601\n",
      "Epoch: 8/40, Total Loss: 0.0673, Reconstruction Loss: 0.0657, KL Divergence: 0.1660\n",
      "Epoch: 9/40, Total Loss: 0.0673, Reconstruction Loss: 0.0655, KL Divergence: 0.1777\n",
      "Epoch: 10/40, Total Loss: 0.0673, Reconstruction Loss: 0.0655, KL Divergence: 0.1836\n",
      "Epoch: 11/40, Total Loss: 0.0672, Reconstruction Loss: 0.0653, KL Divergence: 0.1894\n",
      "Epoch: 12/40, Total Loss: 0.0671, Reconstruction Loss: 0.0652, KL Divergence: 0.1942\n",
      "Epoch: 13/40, Total Loss: 0.0672, Reconstruction Loss: 0.0652, KL Divergence: 0.1993\n",
      "Epoch: 14/40, Total Loss: 0.0672, Reconstruction Loss: 0.0652, KL Divergence: 0.2056\n",
      "Epoch: 15/40, Total Loss: 0.0672, Reconstruction Loss: 0.0651, KL Divergence: 0.2038\n",
      "Epoch: 16/40, Total Loss: 0.0672, Reconstruction Loss: 0.0651, KL Divergence: 0.2061\n",
      "Epoch: 17/40, Total Loss: 0.0672, Reconstruction Loss: 0.0651, KL Divergence: 0.2050\n",
      "Epoch: 18/40, Total Loss: 0.0671, Reconstruction Loss: 0.0650, KL Divergence: 0.2138\n",
      "Epoch: 19/40, Total Loss: 0.0671, Reconstruction Loss: 0.0650, KL Divergence: 0.2084\n",
      "Epoch: 20/40, Total Loss: 0.0671, Reconstruction Loss: 0.0650, KL Divergence: 0.2155\n",
      "Epoch: 21/40, Total Loss: 0.0671, Reconstruction Loss: 0.0649, KL Divergence: 0.2112\n",
      "Epoch: 22/40, Total Loss: 0.0671, Reconstruction Loss: 0.0649, KL Divergence: 0.2151\n",
      "Epoch: 23/40, Total Loss: 0.0671, Reconstruction Loss: 0.0650, KL Divergence: 0.2150\n",
      "Epoch: 24/40, Total Loss: 0.0671, Reconstruction Loss: 0.0649, KL Divergence: 0.2141\n",
      "Epoch: 25/40, Total Loss: 0.0671, Reconstruction Loss: 0.0649, KL Divergence: 0.2183\n",
      "Epoch: 26/40, Total Loss: 0.0671, Reconstruction Loss: 0.0649, KL Divergence: 0.2222\n",
      "Epoch: 27/40, Total Loss: 0.0671, Reconstruction Loss: 0.0649, KL Divergence: 0.2227\n",
      "Epoch: 28/40, Total Loss: 0.0671, Reconstruction Loss: 0.0648, KL Divergence: 0.2240\n",
      "Epoch: 29/40, Total Loss: 0.0671, Reconstruction Loss: 0.0649, KL Divergence: 0.2241\n",
      "Epoch: 30/40, Total Loss: 0.0670, Reconstruction Loss: 0.0649, KL Divergence: 0.2183\n",
      "Epoch: 31/40, Total Loss: 0.0671, Reconstruction Loss: 0.0649, KL Divergence: 0.2179\n",
      "Epoch: 32/40, Total Loss: 0.0671, Reconstruction Loss: 0.0649, KL Divergence: 0.2219\n",
      "Epoch: 33/40, Total Loss: 0.0670, Reconstruction Loss: 0.0649, KL Divergence: 0.2156\n",
      "Epoch: 34/40, Total Loss: 0.0670, Reconstruction Loss: 0.0648, KL Divergence: 0.2215\n",
      "Epoch: 35/40, Total Loss: 0.0670, Reconstruction Loss: 0.0648, KL Divergence: 0.2214\n",
      "Epoch: 36/40, Total Loss: 0.0671, Reconstruction Loss: 0.0648, KL Divergence: 0.2289\n",
      "Epoch: 37/40, Total Loss: 0.0670, Reconstruction Loss: 0.0648, KL Divergence: 0.2245\n",
      "Epoch: 38/40, Total Loss: 0.0671, Reconstruction Loss: 0.0648, KL Divergence: 0.2236\n",
      "Epoch: 39/40, Total Loss: 0.0671, Reconstruction Loss: 0.0648, KL Divergence: 0.2243\n",
      "Epoch: 40/40, Total Loss: 0.0670, Reconstruction Loss: 0.0648, KL Divergence: 0.2251\n",
      "Test Loss: Total Loss: 0.0668, Reconstruction Loss: 0.0648, KL Divergence: 0.2004\n",
      "tensor(0.0645, grad_fn=<MseLossBackward0>)\n",
      "Traing VAE with latent dim: 5\n",
      "Test Loss: Total Loss: 0.2215, Reconstruction Loss: 0.2213, KL Divergence: 0.0198\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004512310028076172,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 40,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd7c154b6d44c7599f6d3a2a709fbd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40, Total Loss: 0.0840, Reconstruction Loss: 0.0837, KL Divergence: 0.0270\n",
      "Epoch: 2/40, Total Loss: 0.0681, Reconstruction Loss: 0.0677, KL Divergence: 0.0338\n",
      "Epoch: 3/40, Total Loss: 0.0678, Reconstruction Loss: 0.0673, KL Divergence: 0.0471\n",
      "Epoch: 4/40, Total Loss: 0.0676, Reconstruction Loss: 0.0671, KL Divergence: 0.0544\n",
      "Epoch: 5/40, Total Loss: 0.0675, Reconstruction Loss: 0.0668, KL Divergence: 0.0707\n",
      "Epoch: 6/40, Total Loss: 0.0675, Reconstruction Loss: 0.0665, KL Divergence: 0.0971\n",
      "Epoch: 7/40, Total Loss: 0.0674, Reconstruction Loss: 0.0662, KL Divergence: 0.1246\n",
      "Epoch: 8/40, Total Loss: 0.0674, Reconstruction Loss: 0.0661, KL Divergence: 0.1303\n",
      "Epoch: 9/40, Total Loss: 0.0674, Reconstruction Loss: 0.0659, KL Divergence: 0.1462\n",
      "Epoch: 10/40, Total Loss: 0.0673, Reconstruction Loss: 0.0658, KL Divergence: 0.1547\n",
      "Epoch: 11/40, Total Loss: 0.0673, Reconstruction Loss: 0.0656, KL Divergence: 0.1656\n",
      "Epoch: 12/40, Total Loss: 0.0673, Reconstruction Loss: 0.0656, KL Divergence: 0.1699\n",
      "Epoch: 13/40, Total Loss: 0.0672, Reconstruction Loss: 0.0655, KL Divergence: 0.1765\n",
      "Epoch: 14/40, Total Loss: 0.0672, Reconstruction Loss: 0.0654, KL Divergence: 0.1841\n",
      "Epoch: 15/40, Total Loss: 0.0672, Reconstruction Loss: 0.0654, KL Divergence: 0.1826\n",
      "Epoch: 16/40, Total Loss: 0.0672, Reconstruction Loss: 0.0653, KL Divergence: 0.1896\n",
      "Epoch: 17/40, Total Loss: 0.0672, Reconstruction Loss: 0.0652, KL Divergence: 0.1916\n",
      "Epoch: 18/40, Total Loss: 0.0672, Reconstruction Loss: 0.0653, KL Divergence: 0.1884\n",
      "Epoch: 19/40, Total Loss: 0.0671, Reconstruction Loss: 0.0652, KL Divergence: 0.1933\n",
      "Epoch: 20/40, Total Loss: 0.0671, Reconstruction Loss: 0.0651, KL Divergence: 0.1997\n",
      "Epoch: 21/40, Total Loss: 0.0671, Reconstruction Loss: 0.0651, KL Divergence: 0.1996\n",
      "Epoch: 22/40, Total Loss: 0.0671, Reconstruction Loss: 0.0651, KL Divergence: 0.1998\n",
      "Epoch: 23/40, Total Loss: 0.0671, Reconstruction Loss: 0.0651, KL Divergence: 0.2049\n",
      "Epoch: 24/40, Total Loss: 0.0671, Reconstruction Loss: 0.0650, KL Divergence: 0.2093\n",
      "Epoch: 25/40, Total Loss: 0.0671, Reconstruction Loss: 0.0650, KL Divergence: 0.2114\n",
      "Epoch: 26/40, Total Loss: 0.0671, Reconstruction Loss: 0.0650, KL Divergence: 0.2115\n",
      "Epoch: 27/40, Total Loss: 0.0671, Reconstruction Loss: 0.0650, KL Divergence: 0.2129\n",
      "Epoch: 28/40, Total Loss: 0.0671, Reconstruction Loss: 0.0650, KL Divergence: 0.2115\n",
      "Epoch: 29/40, Total Loss: 0.0671, Reconstruction Loss: 0.0649, KL Divergence: 0.2183\n",
      "Epoch: 30/40, Total Loss: 0.0671, Reconstruction Loss: 0.0649, KL Divergence: 0.2173\n",
      "Epoch: 31/40, Total Loss: 0.0671, Reconstruction Loss: 0.0650, KL Divergence: 0.2125\n",
      "Epoch: 32/40, Total Loss: 0.0670, Reconstruction Loss: 0.0649, KL Divergence: 0.2191\n",
      "Epoch: 33/40, Total Loss: 0.0671, Reconstruction Loss: 0.0649, KL Divergence: 0.2204\n",
      "Epoch: 34/40, Total Loss: 0.0671, Reconstruction Loss: 0.0649, KL Divergence: 0.2251\n",
      "Epoch: 35/40, Total Loss: 0.0671, Reconstruction Loss: 0.0648, KL Divergence: 0.2234\n",
      "Epoch: 36/40, Total Loss: 0.0671, Reconstruction Loss: 0.0648, KL Divergence: 0.2241\n",
      "Epoch: 37/40, Total Loss: 0.0670, Reconstruction Loss: 0.0648, KL Divergence: 0.2276\n",
      "Epoch: 38/40, Total Loss: 0.0671, Reconstruction Loss: 0.0648, KL Divergence: 0.2271\n",
      "Epoch: 39/40, Total Loss: 0.0670, Reconstruction Loss: 0.0648, KL Divergence: 0.2219\n",
      "Epoch: 40/40, Total Loss: 0.0671, Reconstruction Loss: 0.0648, KL Divergence: 0.2297\n",
      "Test Loss: Total Loss: 0.0668, Reconstruction Loss: 0.0644, KL Divergence: 0.2424\n",
      "tensor(0.0662, grad_fn=<MseLossBackward0>)\n",
      "Traing VAE with latent dim: 10\n",
      "Test Loss: Total Loss: 0.2338, Reconstruction Loss: 0.2334, KL Divergence: 0.0402\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0055887699127197266,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 40,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc98cc838a040c7ad1d8e0a5a548b3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40, Total Loss: 0.0849, Reconstruction Loss: 0.0846, KL Divergence: 0.0324\n",
      "Epoch: 2/40, Total Loss: 0.0681, Reconstruction Loss: 0.0678, KL Divergence: 0.0332\n",
      "Epoch: 3/40, Total Loss: 0.0678, Reconstruction Loss: 0.0674, KL Divergence: 0.0377\n",
      "Epoch: 4/40, Total Loss: 0.0677, Reconstruction Loss: 0.0672, KL Divergence: 0.0462\n",
      "Epoch: 5/40, Total Loss: 0.0676, Reconstruction Loss: 0.0670, KL Divergence: 0.0584\n",
      "Epoch: 6/40, Total Loss: 0.0675, Reconstruction Loss: 0.0669, KL Divergence: 0.0646\n",
      "Epoch: 7/40, Total Loss: 0.0675, Reconstruction Loss: 0.0667, KL Divergence: 0.0790\n",
      "Epoch: 8/40, Total Loss: 0.0674, Reconstruction Loss: 0.0663, KL Divergence: 0.1096\n",
      "Epoch: 9/40, Total Loss: 0.0674, Reconstruction Loss: 0.0660, KL Divergence: 0.1340\n",
      "Epoch: 10/40, Total Loss: 0.0674, Reconstruction Loss: 0.0660, KL Divergence: 0.1406\n",
      "Epoch: 11/40, Total Loss: 0.0673, Reconstruction Loss: 0.0658, KL Divergence: 0.1465\n",
      "Epoch: 12/40, Total Loss: 0.0673, Reconstruction Loss: 0.0657, KL Divergence: 0.1566\n",
      "Epoch: 13/40, Total Loss: 0.0673, Reconstruction Loss: 0.0657, KL Divergence: 0.1564\n",
      "Epoch: 14/40, Total Loss: 0.0673, Reconstruction Loss: 0.0657, KL Divergence: 0.1622\n",
      "Epoch: 15/40, Total Loss: 0.0673, Reconstruction Loss: 0.0656, KL Divergence: 0.1684\n",
      "Epoch: 16/40, Total Loss: 0.0672, Reconstruction Loss: 0.0655, KL Divergence: 0.1684\n",
      "Epoch: 17/40, Total Loss: 0.0672, Reconstruction Loss: 0.0654, KL Divergence: 0.1743\n",
      "Epoch: 18/40, Total Loss: 0.0672, Reconstruction Loss: 0.0655, KL Divergence: 0.1770\n",
      "Epoch: 19/40, Total Loss: 0.0672, Reconstruction Loss: 0.0654, KL Divergence: 0.1779\n",
      "Epoch: 20/40, Total Loss: 0.0672, Reconstruction Loss: 0.0654, KL Divergence: 0.1812\n",
      "Epoch: 21/40, Total Loss: 0.0672, Reconstruction Loss: 0.0654, KL Divergence: 0.1811\n",
      "Epoch: 22/40, Total Loss: 0.0672, Reconstruction Loss: 0.0653, KL Divergence: 0.1893\n",
      "Epoch: 23/40, Total Loss: 0.0672, Reconstruction Loss: 0.0653, KL Divergence: 0.1851\n",
      "Epoch: 24/40, Total Loss: 0.0671, Reconstruction Loss: 0.0652, KL Divergence: 0.1881\n",
      "Epoch: 25/40, Total Loss: 0.0672, Reconstruction Loss: 0.0653, KL Divergence: 0.1910\n",
      "Epoch: 26/40, Total Loss: 0.0672, Reconstruction Loss: 0.0652, KL Divergence: 0.1978\n",
      "Epoch: 27/40, Total Loss: 0.0671, Reconstruction Loss: 0.0651, KL Divergence: 0.2040\n",
      "Epoch: 28/40, Total Loss: 0.0672, Reconstruction Loss: 0.0652, KL Divergence: 0.1994\n",
      "Epoch: 29/40, Total Loss: 0.0671, Reconstruction Loss: 0.0651, KL Divergence: 0.2018\n",
      "Epoch: 30/40, Total Loss: 0.0671, Reconstruction Loss: 0.0650, KL Divergence: 0.2092\n",
      "Epoch: 31/40, Total Loss: 0.0672, Reconstruction Loss: 0.0651, KL Divergence: 0.2078\n",
      "Epoch: 32/40, Total Loss: 0.0671, Reconstruction Loss: 0.0650, KL Divergence: 0.2122\n",
      "Epoch: 33/40, Total Loss: 0.0671, Reconstruction Loss: 0.0650, KL Divergence: 0.2116\n",
      "Epoch: 34/40, Total Loss: 0.0671, Reconstruction Loss: 0.0650, KL Divergence: 0.2098\n",
      "Epoch: 35/40, Total Loss: 0.0671, Reconstruction Loss: 0.0649, KL Divergence: 0.2141\n",
      "Epoch: 36/40, Total Loss: 0.0671, Reconstruction Loss: 0.0650, KL Divergence: 0.2145\n",
      "Epoch: 37/40, Total Loss: 0.0671, Reconstruction Loss: 0.0650, KL Divergence: 0.2128\n",
      "Epoch: 38/40, Total Loss: 0.0671, Reconstruction Loss: 0.0650, KL Divergence: 0.2146\n",
      "Epoch: 39/40, Total Loss: 0.0671, Reconstruction Loss: 0.0650, KL Divergence: 0.2154\n",
      "Epoch: 40/40, Total Loss: 0.0671, Reconstruction Loss: 0.0649, KL Divergence: 0.2186\n",
      "Test Loss: Total Loss: 0.0669, Reconstruction Loss: 0.0649, KL Divergence: 0.2079\n",
      "tensor(0.0662, grad_fn=<MseLossBackward0>)\n",
      "Traing VAE with latent dim: 20\n",
      "Test Loss: Total Loss: 0.2514, Reconstruction Loss: 0.2503, KL Divergence: 0.1115\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009579658508300781,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 40,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82419415bf704025a1e3b12ba2c3e385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40, Total Loss: 0.0885, Reconstruction Loss: 0.0884, KL Divergence: 0.0125\n",
      "Epoch: 2/40, Total Loss: 0.0682, Reconstruction Loss: 0.0680, KL Divergence: 0.0196\n",
      "Epoch: 3/40, Total Loss: 0.0678, Reconstruction Loss: 0.0675, KL Divergence: 0.0292\n",
      "Epoch: 4/40, Total Loss: 0.0677, Reconstruction Loss: 0.0673, KL Divergence: 0.0364\n",
      "Epoch: 5/40, Total Loss: 0.0676, Reconstruction Loss: 0.0672, KL Divergence: 0.0433\n",
      "Epoch: 6/40, Total Loss: 0.0676, Reconstruction Loss: 0.0671, KL Divergence: 0.0507\n",
      "Epoch: 7/40, Total Loss: 0.0675, Reconstruction Loss: 0.0669, KL Divergence: 0.0586\n",
      "Epoch: 8/40, Total Loss: 0.0675, Reconstruction Loss: 0.0669, KL Divergence: 0.0579\n",
      "Epoch: 9/40, Total Loss: 0.0675, Reconstruction Loss: 0.0670, KL Divergence: 0.0563\n",
      "Epoch: 10/40, Total Loss: 0.0675, Reconstruction Loss: 0.0669, KL Divergence: 0.0573\n",
      "Epoch: 11/40, Total Loss: 0.0675, Reconstruction Loss: 0.0668, KL Divergence: 0.0629\n",
      "Epoch: 12/40, Total Loss: 0.0675, Reconstruction Loss: 0.0668, KL Divergence: 0.0624\n",
      "Epoch: 13/40, Total Loss: 0.0674, Reconstruction Loss: 0.0668, KL Divergence: 0.0638\n",
      "Epoch: 14/40, Total Loss: 0.0674, Reconstruction Loss: 0.0667, KL Divergence: 0.0714\n",
      "Epoch: 15/40, Total Loss: 0.0674, Reconstruction Loss: 0.0667, KL Divergence: 0.0723\n",
      "Epoch: 16/40, Total Loss: 0.0674, Reconstruction Loss: 0.0667, KL Divergence: 0.0709\n",
      "Epoch: 17/40, Total Loss: 0.0674, Reconstruction Loss: 0.0667, KL Divergence: 0.0699\n",
      "Epoch: 18/40, Total Loss: 0.0674, Reconstruction Loss: 0.0666, KL Divergence: 0.0773\n",
      "Epoch: 19/40, Total Loss: 0.0673, Reconstruction Loss: 0.0666, KL Divergence: 0.0792\n",
      "Epoch: 20/40, Total Loss: 0.0674, Reconstruction Loss: 0.0666, KL Divergence: 0.0752\n",
      "Epoch: 21/40, Total Loss: 0.0674, Reconstruction Loss: 0.0666, KL Divergence: 0.0778\n",
      "Epoch: 22/40, Total Loss: 0.0673, Reconstruction Loss: 0.0664, KL Divergence: 0.0875\n",
      "Epoch: 23/40, Total Loss: 0.0674, Reconstruction Loss: 0.0665, KL Divergence: 0.0863\n",
      "Epoch: 24/40, Total Loss: 0.0673, Reconstruction Loss: 0.0664, KL Divergence: 0.0967\n",
      "Epoch: 25/40, Total Loss: 0.0673, Reconstruction Loss: 0.0661, KL Divergence: 0.1198\n",
      "Epoch: 26/40, Total Loss: 0.0672, Reconstruction Loss: 0.0659, KL Divergence: 0.1387\n",
      "Epoch: 27/40, Total Loss: 0.0672, Reconstruction Loss: 0.0656, KL Divergence: 0.1598\n",
      "Epoch: 28/40, Total Loss: 0.0672, Reconstruction Loss: 0.0656, KL Divergence: 0.1627\n",
      "Epoch: 29/40, Total Loss: 0.0672, Reconstruction Loss: 0.0655, KL Divergence: 0.1709\n",
      "Epoch: 30/40, Total Loss: 0.0672, Reconstruction Loss: 0.0655, KL Divergence: 0.1719\n",
      "Epoch: 31/40, Total Loss: 0.0672, Reconstruction Loss: 0.0654, KL Divergence: 0.1765\n",
      "Epoch: 32/40, Total Loss: 0.0672, Reconstruction Loss: 0.0654, KL Divergence: 0.1798\n",
      "Epoch: 33/40, Total Loss: 0.0672, Reconstruction Loss: 0.0654, KL Divergence: 0.1813\n",
      "Epoch: 34/40, Total Loss: 0.0672, Reconstruction Loss: 0.0653, KL Divergence: 0.1848\n",
      "Epoch: 35/40, Total Loss: 0.0671, Reconstruction Loss: 0.0653, KL Divergence: 0.1839\n",
      "Epoch: 36/40, Total Loss: 0.0672, Reconstruction Loss: 0.0652, KL Divergence: 0.1925\n",
      "Epoch: 37/40, Total Loss: 0.0671, Reconstruction Loss: 0.0652, KL Divergence: 0.1928\n",
      "Epoch: 38/40, Total Loss: 0.0671, Reconstruction Loss: 0.0651, KL Divergence: 0.1982\n",
      "Epoch: 39/40, Total Loss: 0.0671, Reconstruction Loss: 0.0652, KL Divergence: 0.1975\n",
      "Epoch: 40/40, Total Loss: 0.0672, Reconstruction Loss: 0.0651, KL Divergence: 0.2019\n",
      "Test Loss: Total Loss: 0.0670, Reconstruction Loss: 0.0651, KL Divergence: 0.1844\n",
      "tensor(0.0672, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using_beta_vae = True\n",
    "n_epochs = 40\n",
    "for beta_level in [1/1000, 1/500,1/200,1/100]:\n",
    "    print(f'[Training beta = {beta_level}]')\n",
    "    run(NaiveVAE,epochs=n_epochs,beta=beta_level)\n",
    "    run(VAE,epochs=n_epochs,beta=beta_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've identified a very common issue in VAEs known as \"posterior collapse\" or \"KL vanishing.\" This happens when the KL divergence term quickly approaches zero while the reconstruction loss remains high. Here's what's happening and how to fix it:\n",
    "\n",
    "### What's Happening\n",
    "Your KL divergence is quickly dropping to nearly zero (from 0.0092 to 0.0000)\n",
    "Your reconstruction loss remains high (staying around 0.068)\n",
    "This means your model is ignoring the latent space and is not learning a meaningful distribution\n",
    "In posterior collapse, the model learns to ignore the latent variable z and the encoder outputs a posterior distribution that matches the prior (standard normal) regardless of the input. This effectively turns your VAE into a deterministic autoencoder.\n",
    "\n",
    "### Why This Happens\n",
    "Optimization Imbalance: It's easier for the model to reduce the KL divergence (by making the encoder output match the prior) than to improve reconstruction.\n",
    "Too Simple Decoder: If your decoder is powerful enough, it can learn to ignore z and still generate reasonable reconstructions.\n",
    "Lack of Control: Without proper weighting, the KL term can dominate early training then collapse.\n",
    "\n",
    "### Fix\n",
    "By experiments, it shows that the numerical level of KL divergences, is approximately 30 multiples of the reconstruction error, if using `reduction='mean'`. It can be fixed by manually set a $beta = 1/784$ at the beginning. It is safe to set beta even smaller as the priority is to reduce the reconstruction error. I tried dynamic beta scheduling for small epochs, but it is not any better.\n",
    "\n",
    "As a matter of fact, the formula of ELBO is 1/2 the mse_sum. The sum matches the theoretical formula better. I made the decision of 'mean' purely for granted and this is the consequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# However\n",
    "The problem is not solved. The output is of low quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "num-embeddings",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
