# **Optimal Transport on Lie Groups: A Literature Review**

## **Introduction**

Lie groups (smooth groups with a differentiable structure) provide a natural setting to study stochastic processes and transport problems with symmetry. In particular, Brownian motion on Lie groups, Schrödinger bridge problems, and optimal transport on these geometric spaces have seen renewed interest. These concepts are deeply interconnected: Brownian motion is a fundamental stochastic process on a manifold, Schrödinger bridges seek the "most likely" stochastic evolution (often relative to Brownian motion) connecting two distributions, and optimal transport is concerned with moving mass optimally between distributions (with Schrödinger bridges being its entropy-regularized dynamic counterpart). In recent years (post-2018), there have been significant efforts to rigorously construct these objects on Lie groups (and more generally on Riemannian manifolds) and to leverage them in machine learning. This report surveys foundational definitions and recent developments, highlighting theoretical constructions and modern applications in diffusion-based generative models and flow matching. We emphasize works providing rigorous formulations and making connections between these areas, as well as extensions to non-Euclidean domains when explicit Lie group references are sparse.

## **Brownian Motion on Lie Groups**

Brownian motion on a Lie group generalizes the classical Wiener process to a curved, non-linear state space with group structure. Kiyosi Itô first extended the notion of Brownian motion to general Lie groups in 1950. Intuitively, a "left-invariant" (or "right-invariant") Brownian motion on a Lie group $G$ is a stochastic process that respects the group's symmetries: it is statistically homogeneous in time and space and its distribution is invariant under left (or right) multiplication by group elements. Itô defined such a process by a set of conditions (Markov property, continuity, temporal homogeneity, spatial homogeneity under the group action, and continuous sample paths) that characterize the invariance and regularity expected of Brownian motion on $G$. Under these conditions, the infinitesimal generator $D$ of the process must be a uniform elliptic operator on $G$ that commutes with left (or right) translations. In fact, Itô showed $D$ can be expressed as a second-order differential operator built from the group's Lie algebra: for a basis $\{X_i\}$ of the Lie algebra, one finds (in local coordinates)

$$D = \sum_i a^i(p) \partial_{x_i} + \frac{1}{2} \sum_{i,j} B^{ij} \partial_{x_i} \partial_{x_j},$$

where $(B^{ij})$ is a symmetric positive semi-definite matrix determined by the process's covariance. In particular, for a bi-invariant Riemannian metric on $G$, one typically takes $a^i=0$ and $B^{ij}$ constant, so that $D = \tfrac{1}{2}\sum_{i,j} B^{ij}X_iX_j$. This $D$ is an elliptic operator that is essentially the Laplace–Beltrami operator on $G$ (up to a factor), often coinciding with the quadratic Casimir element of the Lie algebra in the case of compact $G$. Thus, a Lie group Brownian motion can be defined as the diffusion process on $G$ whose generator is $\frac{1}{2}\Delta_G$, the Laplace–Beltrami operator for a chosen invariant metric. Equivalently, it can be constructed as the solution to a stochastic differential equation that uses an orthonormal basis of left-invariant vector fields as diffusion directions. For example, if $\{X_1,\dots,X_n\}$ is an orthonormal basis of the Lie algebra (with respect to a positive-definite form defining the metric), a left-invariant Brownian motion $g_t$ on $G$ satisfies the Stratonovich SDE:

$$dg_t = \sum_{i=1}^n X_i(g_t)\circ dB^i_t,$$

where $B^i_t$ are independent standard Brownian motions. On a general Riemannian manifold $M$ (which includes Lie groups with a Riemannian metric as special cases), Brownian motion is rigorously defined as the diffusion with infinitesimal generator $\tfrac{1}{2}\Delta_M$, where $\Delta_M$ is the Laplace–Beltrami operator on $M$. In local coordinates, this means the process has drift and diffusion terms corresponding to the metric's Christoffel symbols such that its generator in coordinates is $\frac{1}{2}$ times the Laplace–Beltrami differential operator. This definition is equivalent to several other characterizations (via stochastic development in the frame bundle, via the heat semigroup on functions, etc.), and it ensures that on $\mathbb{R}^n$ with the flat metric one recovers the standard Euclidean Brownian motion. In summary, Brownian motion on a Lie group $G$ can be seen as a canonical stochastic process respecting $G$'s geometry: it is the unique (up to choice of metric) Markov process on $G$ that is continuous, homogeneous, and invariant under the group's symmetries, with generator equal to half the Laplace–Beltrami (or sum-of-squares) operator. This process provides a natural "reference" random motion on $G$ and plays a key role in defining Schrödinger bridges on manifolds and in constructing diffusion-based generative models on non-Euclidean spaces.

## **Schrödinger Bridges on Lie Groups and Manifolds**

The Schrödinger bridge problem (SBP) is a stochastic optimal control or entropy-minimization problem that asks: given two probability distributions $\mu_0$ and $\mu_T$ on a state space (here, a Lie group or manifold) at an initial time $t=0$ and final time $t=T$, what is the most likely evolution of a stochastic process that starts with law $\mu_0$ and ends with law $\mu_T$? "Most likely" is defined relative to a reference stochastic process, typically a Brownian motion or diffusion on that state space. In other words, the Schrödinger bridge is the solution of an entropy-regularized optimal transport on path space: one seeks a process that connects the endpoints while minimizing the Kullback–Leibler divergence from a reference (such as free Brownian motion). This problem was originally introduced by Erwin Schrödinger in the 1930s in the context of a thought experiment on gas diffusion, and it can be formulated as finding a probability law on $(X_t)_{0\le t\le T}$ that matches given marginals at $0$ and $T$ while minimizing relative entropy with respect to the law of a Wiener process (Brownian motion) on the same space. Formally, let $P$ be the law of a reference diffusion (for instance, Brownian motion on a Lie group $G$), and let $\mathbb{P}$ range over all laws on path space such that under $\mathbb{P}$, $X_0 \sim \mu_0$ and $X_T \sim \mu_T$. The Schrödinger bridge $\mathbb{P}^*$ is defined by the variational problem

$$\mathbb{P}^* = \arg\min_{\mathbb{P}} \text{KL}(\mathbb{P}\|P),$$

subject to $X_0 \sim \mu_0, X_T \sim \mu_T$. Solving this yields an optimal controlled diffusion $(X_t^*)_{t\in[0,T]}$ that has the prescribed endpoints and is as close as possible to the uncontrolled Brownian motion $P$ in law. In Euclidean space, this problem is well-understood: the solution can be obtained via an iterative proportional fitting (IPF) algorithm (also known as the Sinkhorn algorithm in OT context) on path space, or by solving a pair of dual PDEs (Schrödinger system) for forward and backward potentials. The entropy-regularization makes the problem convex and ensures a unique solution under broad conditions. Importantly, as the noise level or entropy regularization vanishes, the Schrödinger bridge approaches the solution of the deterministic optimal transport (the Benamou–Brenier geodesic in the Wasserstein space) between $\mu_0$ and $\mu_T$. Conversely, with positive noise, the bridge process is a diffusion that randomly explores paths while still matching the end-point distributions. When the state space is a Riemannian manifold or Lie group, the Schrödinger bridge problem can be posed in the same way, using the Brownian motion on that manifold as the reference process. Theoretical developments have extended the existence and characterization results of SBPs to general state spaces, including continuous manifolds, since one can work with the diffusion's transition densities in place of the standard Gaussian densities. While explicit solutions are rarely available in closed form (even in Euclidean space they require solving nonlinear PDEs), the theory guarantees that if $\mu_0$ and $\mu_T$ are absolutely continuous with respect to the invariant measure on $G$, there exists a unique Schrödinger bridge diffusion connecting them (under mild conditions such as stochastic completeness of the manifold) – essentially given by a drift-adjusted version of the Brownian motion. The bridge process's drift can be described via Schrödinger potentials: there exist two scalar functions $\varphi_0(x)$ and $\varphi_T(x)$ on $G$ (an initial "forward" and final "backward" potential) such that the law of $X_t^*$ has a density proportional to $\varphi_0(x)p(t,x,y)\varphi_T(y)$, where $p(t,x,y)$ is the heat kernel (transition density) of the Brownian motion on $G$. These potentials satisfy a pair of Schrödinger equations on the manifold. Solving for $\varphi_0,\varphi_T$ (for example, by iterating the coupling of forward and backward evolution) yields the bridge. This procedure is the continuous-state analogue of the Sinkhorn algorithm, which alternately projects onto the constraints of matching the initial and final distributions. In recent years, machine learning and generative modeling applications have spurred new interest in Schrödinger bridges, including on non-Euclidean domains. Notably, the Diffusion Schrödinger Bridge (DSB) framework by De Bortoli et al. (2021) demonstrates how SBPs can serve as a powerful tool for generative modeling. They showed that by solving a Schrödinger bridge where $\mu_0$ is a simple prior (e.g. Gaussian) and $\mu_T$ is the data distribution, one can generate samples from $\mu_T$ in finite time without requiring the lengthy diffusion that standard denoising diffusion models use. In fact, the first iteration of their Schrödinger bridge algorithm recovers the classical score-based diffusion model (as in Song et al., 2021) as a special case. Subsequent iterations refine the solution to better match the data distribution at time $T$, effectively "bridging" the gap by adjusting the drift of the SDE. This connection is profound: it places diffusion models in the context of a rigorous OT problem on path space. Moreover, DSB provides a theoretical link between diffusion models and optimal transport, since it reaches the data distribution with minimum "effort" relative to a Brownian motion prior. Beyond Euclidean data, these ideas have been extended to Riemannian manifolds. For example, Thornton et al. (2022) introduced a Riemannian Diffusion Schrödinger Bridge method that generalizes diffusion bridges to non-Euclidean settings. They integrate the geometry of the manifold (through Riemannian Brownian motion and score functions defined on the manifold) to enable generative modeling and interpolation for data on curved spaces. This extension is crucial for applications like climate or earth data on spheres, or robotics and vision data on Lie groups, where assuming a flat geometry would be inappropriate. The results demonstrate that Schrödinger bridges can incorporate geometric prior knowledge by operating with the manifold's Brownian motion and thus can respect the curvature or topology of the data space. Several works also focus on the rigorous and computational aspects of Schrödinger bridges. For instance, recent research has formulated SBPs as stochastic optimal control problems, allowing the use of control-theoretic algorithms to solve them, and even generalized the framework to include broader classes of dynamics or cost criteria. One notable generalization is the Generalized Schrödinger Bridge (GSB), where instead of the energy (kinetic energy) cost implicit in the Brownian reference, one can impose other state-dependent costs or constraints. Liu et al. (2024) propose a Generalized Schrödinger Bridge Matching algorithm that extends diffusion models with task-specific optimality criteria. They cast the SBP in a more general stochastic control form (allowing e.g. nonlinear state costs beyond the usual quadratic energy), and develop a method that maintains a feasible transport map throughout training. This kind of generalization is relevant if, for example, one wants a bridge process that not only matches distributions but also optimizes an additional objective (like traveling through likely intermediate states given by some model). Although such works are not specific to Lie groups, they provide tools that could be applied in Lie group contexts as well, by choosing the reference process appropriately (e.g. a geodesic diffusion on the group). In summary, Schrödinger bridges provide a rigorous way to interpolate between distributions on a manifold (including Lie groups) in an "optimally random" fashion, balancing the geometry of Brownian motion with endpoint constraints. Foundational results ensure existence and uniqueness of these bridges under broad conditions, and recent methods have made progress in computing them efficiently. These bridges form a conceptual link between stochastic processes on manifolds and optimal transport theory, and they have quickly become a bridge (pun intended) between mathematical theory and practical generative modeling.

## **Optimal Transport on Lie Groups and Manifolds**

The theory of optimal transport (OT) addresses the problem of finding the least-cost way to move a distribution $\mu_0$ of mass to another distribution $\mu_1$. In the classical Monge formulation, one seeks a map $T: X \to X$ pushing $\mu_0$ to $\mu_1$ that minimizes the total transportation cost $\int c(x, T(x))\,d\mu_0(x)$ for some cost function $c(x,y)$. The Kantorovich formulation relaxes this to a plan $\pi(x,y)$ on $X\times X$. When $X$ is a Riemannian manifold (or even a metric space), one natural cost is the squared geodesic distance $c(x,y) = d(x,y)^2$. This yields a geometric OT problem: find the optimal coupling of $\mu_0$ and $\mu_1$ that minimizes the average squared distance traveled. Lie groups can be treated in this framework by equipping them with a Riemannian metric (often a left-invariant metric). Indeed, optimal transport is well-defined on very general spaces, including Polish metric spaces and Riemannian manifolds. Foundational works by Brenier and others have been extended to manifolds – for example, McCann's theorem on the existence of optimal maps under convexity conditions has its analogues on Riemannian manifolds (with displacement convexity defined via geodesics). In particular, if $G$ is a Lie group with a bi-invariant or left-invariant metric, one can talk about the Wasserstein distance $W_2(\mu_0,\mu_1)$ induced by that metric, and there will exist geodesics in the Wasserstein space of probability measures (paths $(\mu_t)_{t\in[0,1]}$) corresponding to "displacement interpolations" between $\mu_0$ and $\mu_1$. These displacement interpolations are given by pushing forward $\mu_0$ along the geodesics on $G$ that solve the OT coupling; in other words, an optimal transport map on a manifold moves each point $x$ along a geodesic to its target location in an optimal way. Research specifically focusing on Lie groups has highlighted interesting effects of symmetry on optimal transport. A recent example is the work by Bon et al. (2024) on optimal transport on the Lie group SE(2) (the group of roto-translations in the plane). This group is important in image analysis because images can be lifted to distributions on position-orientation space (an SE(2) homogeneous space) to preserve orientation information. Bon et al. develop a computational OT framework respecting the group structure, using left-invariant anisotropic metrics on SE(2) that prefer movements along orientations. They prove several theoretical properties generalizable to matrix Lie groups: for instance, they show that naive guesses like "just rotate then translate" (applying group actions) are generally not optimal transport maps, and they analyze how OT plans behave under the group's invariances. An important contribution of their work is adapting the entropic regularization (Sinkhorn algorithm) to Lie groups. By regularizing the OT problem with an entropy term, one can solve it via iterative matrix-scaling algorithms; Bon et al. implement a Sinkhorn-like algorithm on SE(2) that leverages fast computation of geodesic distances on the group and group convolution operations. This allows efficient computation of entropic OT plans and Wasserstein barycenters (optimal averages of multiple distributions) on SE(2). Their experiments demonstrate that lifting data to SE(2) and performing OT with a group-based metric yields qualitatively better results in applications like image interpolation: the transport plans follow coherent orientation flows (e.g. along edges in images), producing sharper interpolations, whereas OT in the image plane (ignoring orientation) leads to blurring and mass splitting. This highlights how incorporating Lie group geometry (symmetries and curvature) into OT can preserve meaningful structures in the data. More generally, optimal transport on manifolds has been studied in contexts such as shape analysis, robotics (configuration spaces often have Lie group structure), and economics on networks of locations. Many fundamental results carry over from Euclidean OT to the Riemannian setting: for example, the existence of an optimal map when $\mu_0$ is absolutely continuous, the characterization of the solution via the convexity (c-concavity) of a potential function on the manifold, and the connection to geodesics. On a compact Lie group with a bi-invariant metric, the exponential map gives a normal coordinate system at the identity, and one can often reduce OT computations (at least locally) to Euclidean ones via the logarithm map – although global topology can introduce multiple geodesic paths and make the OT solution non-unique or more complex (e.g., on a sphere, the OT plan might not be unique if mass can go two ways around the sphere). In non-compact (or non-bi-invariant) cases, one must fix either a left- or right-invariant metric (since a fully bi-invariant Riemannian metric exists only for unimodal compact groups). Once a metric is chosen, one can proceed with OT as usual. However, as pointed out by some authors, the lack of a natural finite invariant volume or the non-positive curvature in some non-compact Lie groups can cause technical subtleties in optimal transport (e.g., the existence of optimal maps may require additional conditions). Despite these nuances, recent work suggests that tackling OT on Lie groups is both feasible and beneficial. For example, in addition to the SE(2) case, there are studies on OT on $SO(3)$ and other rotation groups relevant for 3D shape alignment, and even on infinite-dimensional Lie groups (like groups of diffeomorphisms in fluid mechanics), although those are beyond our scope here. In summary, optimal transport on Lie groups is an active area that blends geometric insights with computational algorithms. Key contributions in the last few years have provided algorithms (like Sinkhorn on groups) and theoretical results about symmetry and equivariance in transport maps. These works extend OT theory into the realm of manifold-valued data, showing that accounting for underlying group symmetries can improve the quality of transport and interpolation in applications. Optimal transport is also tightly connected to Schrödinger bridges (the dynamic formulation): the SBP can be seen as an entropy-regularized OT in time, and solving it on a Lie group involves the ingredients of static OT (cost = energy) plus the randomness of Brownian paths.

## **Applications to Diffusion Models and Flow Matching**

The interplay of Brownian motion, Schrödinger bridges, and optimal transport has become especially prominent in the context of probabilistic generative models. Two paradigms, in particular, have drawn from these concepts: score-based diffusion models (denoising diffusion probabilistic models) and flow matching models (training continuous normalizing flows by matching trajectories). When extended or generalized to non-Euclidean domains (Riemannian manifolds or Lie groups), these models explicitly rely on the theoretical foundations described above. Diffusion models use a Brownian-like noising process and then learn to reverse it. In standard (Euclidean) diffusion models, one starts with data distribution $\mu_{\text{data}}$ at $t=0$ and gradually adds Gaussian noise to reach a nearly trivial distribution (usually standard Normal) at $t=T$. This forward diffusion is typically defined by an SDE (such as $dX_t = \sigma(t)\,dW_t$ for a Brownian $W_t$) that in the $T\to\infty$ limit would make $X_T$ exactly Gaussian. In practice $T$ is finite but large enough that $X_T \approx \mathcal{N}(0,I)$. Then a neural network is trained (by score matching) to approximate the score function $\nabla_x \log p_t(x)$ of the intermediate distributions. Using this score network, one can simulate the reverse-time SDE to transform Gaussian noise back into a sample from the data distribution. Song and colleagues showed that this produces high-quality generative models, but a limitation is that one must run the diffusion for many small steps to reach the prior, which can be inefficient. From our perspective, this approach assumes a flat geometry and uses a fixed Brownian reference (standard Gaussian noise) over a long time horizon. The Schrödinger bridge approach (as in Diffusion Schrödinger Bridge, DSB) improves upon this by shortening the path and using iterative refinement. By directly solving the SBP between $\mu_{\text{data}}$ and the prior $\mu_{\text{prior}}$ (e.g. a Gaussian) over a finite horizon $[0,T]$, one finds an optimal diffusion (in law) connecting them. The first DSB iteration essentially learns the same reverse diffusion as the score-based model (hence "recovers Song et al.'s method"), but because the SBP formulation is aware of the endpoint distributions, it allows using a smaller $T$ and then corrects any mismatch with further iterations. In effect, DSB leverages the optimal transport viewpoint: rather than diffusing to an almost-Gaussian and wasting steps, it tries to directly bridge the two distributions in an entropy-regularized optimal way. Empirically, this can drastically reduce the number of steps needed for generation and improve mode coverage. The connection to optimal transport is made clear by the authors: they describe DSB as the "continuous state-space analogue of the popular Sinkhorn algorithm", highlighting that each iteration of their method is akin to a projection in OT. The application of DSB to manifold-valued data (Thornton et al. 2022) further shows how Brownian motion on a manifold can serve as the reference in a diffusion model, enabling diffusion generative models on Lie groups or other manifolds. For example, one could generate climate data on the sphere or orientations in $\text{SO}(3)$ by using the appropriate manifold Brownian motion in the diffusion/bridge algorithm, rather than an extrinsic Gaussian noise that ignores curvature. Parallel to diffusion models, Flow Matching (FM) has emerged as a new paradigm that also connects with optimal transport. Proposed by Lipman, Chen, and others (2022), flow matching trains a continuous normalizing flow (CNF, defined by an ODE $\dot{x}(t) = v(t,x)$) by directly matching a prescribed probability flow. Instead of simulating a forward SDE, FM chooses a family of intermediate distributions $(\rho(t))_{0\le t\le 1}$ connecting the base and target (for example, a simple choice is a linear interpolation in distribution or a Gaussian convolution path). Then it fits a time-dependent vector field $v(t,x)$ such that following the ODE $\dot{x}=v(t,x)$ will transport $\rho(0)=\mu_{\text{base}}$ to $\rho(1)=\mu_{\text{target}}$ exactly. Crucially, this is done by regression: one generates samples from the known intermediate $\rho(t)$ (which is easy for some designed paths) and then trains $v$ so that pushing the samples an infinitesimal step yields the correct change in density (this requires matching the continuity equation, which provides a regression target for $v$). FM is "simulation-free" in the sense that it doesn't require simulating the learned dynamics during training; simulation is only needed at test time to generate new samples via solving the ODE. This approach has a few notable connections to our themes:

### **Brownian/diffusion special case**
FM can subsume diffusion models by choosing the path $\rho(t)$ to be the result of a diffusion (noise-injection) process. In fact, the authors found that using flow matching with the same diffusion path as a usual diffusion model leads to a more stable training, essentially avoiding some pitfalls of score matching. This suggests that one can learn the generative flow in one go rather than via an SDE reversal.

### **Optimal transport paths**
FM allows other choices of interpolation paths beyond diffusions. A particularly interesting choice is to use the OT displacement interpolation between $\mu_0$ and $\mu_1$. In displacement interpolation, each particle moves along the geodesic (straightest path) in the state space under the optimal transport plan, yielding a family $\rho(t)$ that is the Wasserstein geodesic between the distributions. This path is the solution of the unregularized OT problem, and it tends to be more efficient (in terms of entropy or time) than a diffusive spread. Lipman et al. demonstrate that using the OT-geodesic path in flow matching yields faster convergence and better generative performance. Essentially, the learned flow in this case tries to track the optimal transport map, but since directly learning the OT map might be hard, the FM procedure still allows some flexibility (and can be regularized). On ImageNet, they found that OT-based flow matching outperformed diffusion-based generative models in likelihood and sample quality, and it allowed fast sampling via standard ODE solvers. This is a concrete example of how optimal transport theory improves model design: by providing an informed choice of intermediate states (instead of arbitrarily noisy ones), one can guide the learning to find better flows.

### **Manifold data**
While the initial FM work is in Euclidean space, the concept extends naturally to manifolds if one can sample the interpolation path. For instance, if $\mu_0$ and $\mu_1$ are distributions on a Lie group $G$, one could choose $\rho(t)$ to be the geodesic interpolation on $G$ (or in its Wasserstein space) and then train a vector field on $G$ to follow that. This would require accounting for the geometry in the continuity equation (using the divergence with respect to the Riemannian volume on $G$, etc.), but in principle the method is geometry-agnostic as long as we can sample and compute density ratios. Some flow matching models have indeed started considering data on manifolds (e.g., flows on tori, spheres, or other homogeneous spaces), although detailed results on Lie groups are just beginning to emerge. The promise is that one could combine equivariance (building $v(t,x)$ to respect group symmetry) with flow matching to learn very efficient generative models on group-structured data.

Lastly, it is worth mentioning efforts that explicitly unify diffusion models and flow matching. One example is the concept of Diffusion Schrödinger Bridge Matching (Shi et al., 2023), which aims to blend the strengths of diffusion (robust training through noise) with the efficiency of flow matching. While details are beyond our scope, these developments underscore a trend: modern generative modeling is essentially about distribution dynamics, and tools like Brownian motion (for injecting randomness in a controlled way), Schrödinger bridges (for optimal stochastic interpolation), and optimal transport (for cost-efficient movement of mass) are key ingredients. Each new method can be seen as finding a sweet spot between randomness and optimality: diffusion models start with maximum randomness (adding noise) and then learn to undo it; OT-based flow models start with an optimal (least random) trajectory and learn to implement it; Schrödinger bridges reside in between by allowing randomness but in an optimally controlled manner. In practical applications, these methods have been applied to image synthesis, 3D shape generation, time-series interpolation, and even scientific domains like single-cell gene expression dynamics. For example, Schrödinger bridges have been used to model the temporal evolution of distributions in biology, treating observed distributions at two time points as endpoints and bridging them with a stochastic model. In all these cases, ensuring the methods respect the underlying geometry (be it Euclidean or Lie group structure) is crucial for performance and plausibility. Therefore, the literature on Lie group Brownian motions, manifold Schrödinger bridges, and group-based optimal transport provides the theoretical foundation needed to extend state-of-the-art generative modeling to new domains and with improved efficiency.

## **Conclusion**

Brownian motion, Schrödinger bridges, and optimal transport on Lie groups (and more broadly on Riemannian manifolds) form a triad of concepts that enrich both probability theory and applied machine learning. The theoretical developments reviewed here — from Itô's 1950 construction of Brownian motion on a Lie group to modern formulations of entropy-regularized transport on manifolds — provide rigorous definitions and tools for handling stochastic processes and distribution shifts in curved spaces. We saw that Brownian motion on a Lie group is well-defined via the Laplace–Beltrami operator (or Casimir operator), and serves as a natural reference process for building bridges. Schrödinger bridges extend optimal transport to the stochastic realm, yielding diffusion processes that interpolate between end distributions in an "optimal randomness" sense. Optimal transport itself, when carried onto Lie groups, respects the group symmetries and often leads to more meaningful or efficient mappings in applications. On the application side, these concepts have become cornerstones of new generative modeling techniques. Score-based diffusion models implicitly rely on Brownian motion in $\mathbb{R}^n$, and their extension to manifolds calls for Brownian motion on those manifolds. Schrödinger bridges, once a theoretical optimal transport problem, are now implemented as algorithms (e.g. DSB) that unify diffusion model training with OT principles. Flow matching pushes this further by potentially using pure optimal transport paths as guidance for training ODE-based generative flows. The connections between these frameworks are being actively explored, with hybrid methods emerging to capitalize on the advantages of each. Overall, the literature indicates that incorporating Lie group and manifold geometry into stochastic processes and transport not only deepens our theoretical understanding but also improves practical algorithms. As data increasingly lie on non-Euclidean domains (from directional data and rotations to graph manifolds and beyond), these developments ensure that our models can respect the geometry of the data. The works cited in this review demonstrate both foundational rigor (ensuring definitions are sound and processes well-posed) and innovative application (bringing those ideas to bear on modern machine learning problems). Going forward, we can expect further convergence of these areas: for example, more efficient Schrödinger bridge solvers on manifolds, or flow matching techniques that leverage symmetry and curvature for faster training. The confluence of Brownian motion, Schrödinger bridges, and optimal transport on Lie groups exemplifies how ideas from stochastic analysis, differential geometry, and optimization can come together to advance both theory and practice in handling complex probability distributions.

## **References**
(References are cited inline throughout the text)