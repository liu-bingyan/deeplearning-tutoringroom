Diffusion Models: Theory, Training, and Applications
1. Theoretical Foundation of Diffusion Models
Diffusion models are a class of generative latent variable models that define a forward diffusion process (corrupting data with noise) and a learned reverse denoising process (generating data from noise). Formally, let $x_0 \sim q(x_0)$ be a sample from the true data distribution. The forward diffusion is a Markov chain $q(x_1, x_2, \dots, x_T|x_0)$ that progressively adds noise to $x_0$ in $T$ small steps, producing a sequence $x_1, x_2, \dots, x_T$. Each step is typically Gaussian:

$$q(x_t|x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t I),$$

with a variance schedule $0<\beta_1<\beta_2<\cdots<\beta_T<1$ controlling the noise level at each step. Here $\beta_t$ is a small positive number (often $\beta_t\ll 1$) so that each diffusion step only injects a slight amount of noise. After many steps ($T$ large), the distribution $q(x_T)$ approaches an isotropic Gaussian $\mathcal{N}(0,I)$, effectively "destroying" the original data signal. An important property is that we can sample the noisy $x_t$ at any arbitrary time $t$ in closed form without simulating the chain step-by-step. Defining $\alpha_t := 1-\beta_t$ and $\bar\alpha_t := \prod_{s=1}^t \alpha_s$, one can show by induction that:

$$x_t = \sqrt{\bar\alpha_t}x_0 + \sqrt{1-\bar\alpha_t}\epsilon,$$

with $\epsilon\sim\mathcal{N}(0,I)$. In other words, $x_t$ is distributed as

$$q(x_t|x_0) = \mathcal{N}(x_t; \sqrt{\bar\alpha_t}x_0, (1-\bar\alpha_t)I),$$

meaning that we can obtain $x_t$ by directly mixing the original sample $x_0$ with Gaussian noise in the appropriate proportion. As $t$ increases, $\bar\alpha_t$ decreases (since the $\beta_t$ are positive), so $x_t$ becomes a noisier version of $x_0$. In the limit $T\to\infty$, $\bar\alpha_T\to 0$ and $x_T$ is an exact standard normal sample. This forward process is sometimes compared to a gradual diffusion of ink in water, eventually yielding complete randomness. Reverse Diffusion Process: If we could reverse this noising process, we could start from pure noise $x_T \sim \mathcal{N}(0,I)$ and gradually remove noise to obtain a sample from the data distribution $q(x_0)$. The true reverse dynamics $q(x_{t-1}|x_t)$ of the diffusion chain is also Gaussian for sufficiently small $\beta_t$ (since adding infinitesimal Gaussian noise step-by-step is approximately reversible). However, $q(x_{t-1}|x_t)$ would depend on the entire data distribution and is intractable to compute directly. Diffusion models circumvent this by learning a parameterized reverse Markov chain $p_\theta(x_{t-1}|x_t)$ to approximate $q(x_{t-1}|x_t)$ for all steps. The learned reverse process is defined as:

$$p_\theta(x_{t-1}|x_t) = \mathcal{N}\!\Big(x_{t-1};\,\mu_\theta(x_t,t),\; \Sigma_\theta(x_t,t)\Big),$$

for $t = T, T-1, \dots, 1$. Here $\mu_\theta$ and $\Sigma_\theta$ are neural network outputs (with $\Sigma_\theta$ often simplified to a fixed variance or a time-dependent scalar). In essence, $p_\theta(x_{0:T}) = p(x_T)\prod_{t=1}^T p_\theta(x_{t-1}|x_t)$ is a chain that should transform an initial Gaussian $x_T$ into a realistic sample $x_0$ after $T$ steps. **Bayesian Posterior and Tractability:** A key insight from the original DDPM formulation is that although the true reverse single-step distribution $q(x_{t-1}|x_t)$ is unknown, the *posterior* $q(x_{t-1}|x_t,x_0)$ (i.e. conditioned on the original clean data $x_0$) can be derived in closed form from the forward process. Using Bayes' rule one can show:

$$q(x_{t-1} \mid x_t, x_0) = \mathcal{N}\!\Big(x_{t-1};\,\tilde\mu_t(x_t,x_0),\; \tilde{\beta}_t I\Big),$$

where $\tilde{\beta}_t = \frac{1-\bar\alpha_{t-1}}{1-\bar\alpha_t}\beta_t$ and $\tilde\mu_t(x_t,x_0)$ is a weighted combination of the noisy $x_t$ and the original $x_0$. Intuitively, given the current noisy image $x_t$ and the original image $x_0$, the best estimate of the previous step $x_{t-1}$ is just adding a bit of noise to $x_0$ such that it is consistent with $x_t$. This closed-form posterior is useful for deriving the training objective (see Section 3). **Intuition:** In summary, the diffusion framework consists of two processes: (1) a *forward diffusion* (data → noise) that is fixed, simple, and destroys information, and (2) a *reverse diffusion* (noise → data) that is learned, complex, and restores information. The forward process is analogous to gradually perturbing a data point into random noise, while the reverse process attempts to undo this perturbation step by step. Diffusion models are inspired by non-equilibrium thermodynamics, treating the generation of data as reversing a gradual diffusion of information. The theoretical appeal is that by the final time $T$, one has a simple prior $p(x_T)=\mathcal{N}(0,I)$, and generation can start from pure noise; all the complexity of the data distribution is captured in the learned transitional densities $p_\theta(x_{t-1}|x_t)$. :contentReference[oaicite:0]{index=0} *Figure 1: Forward vs. reverse diffusion on a 2D dataset (a "swiss roll" spiral). Top row (blue) shows samples $x_t$ from the forward process $q(x_{0:T})$: as $t$ increases (left to right), structure is lost and the data distribution approaches Gaussian noise. Middle row (red) shows samples from the learned reverse process $p_\theta(x_{0:T})$, which successfully turns noise back into spiral-like data. Bottom: the learned denoising vector field (difference $\mu_\theta(x_t,t) - x_t$) at various noise levels, indicating how the model "pushes" noisy points toward the data manifold【8†】.* ## 2. Model Architecture for Diffusion-Based Image Generation To implement $p_\theta(x_{t-1}|x_t)$, diffusion models for images typically use a **U-Net neural network** architecture. A U-Net is an encoder–decoder convolutional network with skip connections between encoding and decoding layers, giving it a characteristic "U" shape&#8203;:contentReference[oaicite:1]{index=1}. The encoder gradually downsamples the input (e.g. an image $x_t$) into lower-resolution feature maps, while the decoder gradually upsamples back to the original resolution. Skip connections pass feature maps from encoder to decoder at corresponding scales, allowing the network to combine coarse, high-level information with fine, low-level details. This design is well-suited for denoising tasks, as it can capture both global context and local texture. In diffusion models, the network takes as input a noisy image $x_t$ *and* the timestep $t$, and outputs a denoised image estimate or equivalently the noise components. The U-Net is augmented with **time embeddings**: the discrete time index $t$ is encoded (e.g. via sinusoidal position embeddings or learned embeddings) into a vector, which is then injected into the network through adaptive normalization layers or added to feature maps&#8203;:contentReference[oaicite:2]{index=2}&#8203;:contentReference[oaicite:3]{index=3}. This enables the network to condition its denoising behavior on the current noise level. For example, the DDPM of Ho *et al.* (2020) uses "adaptive group normalization" layers where the scaling and bias are functions of the time embedding&#8203;:contentReference[oaicite:4]{index=4}. Conceptually, the network learns a *time-dependent* denoising function $f_\theta(x_t, t)$. Modern diffusion U-Nets often include other architectural enhancements for expressivity: **residual blocks** (so that each layer learns a residual correction), **multi-head self-attention** at certain resolutions (to capture long-range dependencies in the image)&#8203;:contentReference[oaicite:5]{index=5}, and sometimes **conditioning inputs** (e.g. class labels or text embeddings concatenated to the time embedding, for conditional generation). Despite these complexities, the core role of the network remains: given a noisy input $x_t$, predict some quantity that helps recover $x_{t-1}$. There are a few equivalent ways to parameterize the network's output: - **Predicting noise $\epsilon$:** The network $\epsilon_\theta(x_t,t)$ outputs an estimate of the noise $\epsilon$ that was added to go from $x_{t-1}$ to $x_t$. In training, this is convenient because we can generate $x_t$ from a known $x_0$ and known $\epsilon$ (see Section 3), and then train the network to recover that $\epsilon$. This *noise prediction* parameterization was used in DDPM&#8203;:contentReference[oaicite:6]{index=6} and has become standard, as it was found to yield stable training and high sample quality. - **Predicting $x_0$ (denoised image):** Alternatively, the network can be made to predict the original signal $x_0$ from $x_t$. Since $x_t = \sqrt{\bar\alpha_t}x_0 + \sqrt{1-\bar\alpha_t}\epsilon$, one can show that $x_0 = \frac{1}{\sqrt{\bar\alpha_t}}(x_t - \sqrt{1-\bar\alpha_t}\,\epsilon)$. If the network predicts $x_0$, it's easy to convert that to a predicted noise as $\hat\epsilon = \frac{1}{\sqrt{1-\bar\alpha_t}}(x_t - \sqrt{\bar\alpha_t}\,\hat x_0)$. In practice this is mathematically equivalent to noise prediction, just a different parameterization of $\mu_\theta$. - **Predicting $\mu_{t-1}$ directly:** One could also have the network directly output the mean $\mu_\theta(x_t,t)$ of $p_\theta(x_{t-1}|x_t)$. Ho *et al.* showed that if $\Sigma_\theta$ is fixed appropriately, predicting $\epsilon$ or $x_0$ is equivalent to predicting the mean $\mu_\theta$ up to a scaling and bias, so these choices do not fundamentally change the model power. Most implementations thus use the simpler noise prediction view. In summary, a typical diffusion model uses a U-Net to predict the added noise at each diffusion step, conditioning on the timestep. The U-Net's encoder-decoder structure with skip connections allows it to effectively remove noise while preserving image details&#8203;:contentReference[oaicite:7]{index=7}. Thanks to time encoding, a single network learns to handle all diffusion steps (from $t=1$ which is almost clean, to $t=T$ which is pure noise). This **time-conditioned U-Net** is the backbone of most state-of-the-art diffusion-based image generators (with minor variations such as using transformers instead of convolutions in some cases, or adding classifier guidance, etc., which we will touch on later). ## 3. Training Process: Objectives and Optimization **Variational Objective:** Diffusion models are trained by maximum likelihood, typically via **variational inference**. The model defines a joint distribution $p_\theta(x_{0:T}) = p(x_T)\prod_{t=1}^T p_\theta(x_{t-1}|x_t)$ that approximates the true joint $q(x_{0:T}) = q(x_0)\prod_{t=1}^T q(x_t|x_{t-1})$. We maximize the log-likelihood $\log p_\theta(x_0)$ of data. As with VAEs, we can derive an evidence lower bound (ELBO) that is easier to optimize. The ELBO (negative log-likelihood upper bound) can be written as a sum of KL divergences for each diffusion step plus a term for the final reconstruction: \[ \begin{aligned} -\log p_\theta(x_0) &\leq \; KL\!\big(q(x_T|x_0)\,||\,p(x_T)\big) \\ &\quad + \sum_{t=1}^{T} KL\!\big(q(x_{t-1}|x_t,x_0)\,||\,p_\theta(x_{t-1}|x_t)\big)~, \end{aligned} \] where we have used the fact that $p_\theta(x_0|x_1)$ can be merged into the $t=1$ term. This objective has an intuitive interpretation: (i) The first term $KL(q(x_T|x_0)||p(x_T))$ forces the model's prior $p(x_T)=\mathcal{N}(0,I)$ to match the true $q(x_T|x_0)$ (which is also approximately $\mathcal{N}(0,I)$ if $T$ is large). (ii) Each KL term for $t=T$ down to $1$ pushes the learned reverse transition $p_\theta(x_{t-1}|x_t)$ to match the true posterior $q(x_{t-1}|x_t,x_0)$. In other words, the model is trained to *mimic* the exact reverse of the diffusion process at each step. Summing these yields a lower bound on log-likelihood, which the model maximizes during training. Crucially, the above KL terms are *tractable* because, as noted, $q(x_{t-1}|x_t,x_0)$ is Gaussian with known parameters ($\tilde\mu_t$ and $\tilde\beta_t$ from Section 1). Further, $p_\theta(x_{t-1}|x_t)$ is Gaussian with mean $\mu_\theta(x_t,t)$ and (often) a fixed variance $\Sigma_\theta=\sigma_t^2 I$. For a Gaussian $q$ and Gaussian $p_\theta$, the KL divergence has a simple form: it becomes a weighted quadratic loss between $\mu_\theta(x_t,t)$ and the true mean $\tilde\mu_t(x_t,x_0)$, plus a term for the variance mismatch. Ho *et al.* (2020) showed that this objective can be simplified dramatically: rather than predicting $\mu_\theta$ and comparing to $\tilde\mu_t$, one can reparameterize the loss as an expectation of a *mean squared error* between the model's predicted noise and the actual noise added in that step. Intuitively, since $\tilde\mu_t(x_t,x_0) = \frac{1}{\sqrt{\alpha_t}}(x_t - \frac{\beta_t}{\sqrt{1-\bar\alpha_t}}x_0)$ (a formula obtained from the Bayes rule derivation), minimizing the KL is equivalent to learning to reconstruct $x_0$ (or equivalently the noise $\epsilon = \frac{1}{\sqrt{1-\bar\alpha_t}}(x_t - \sqrt{\bar\alpha_t}x_0)$). Thus, the training objective often used in practice is: $$L_{\text{simple}} = \mathbb{E}_{x_0,\;t,\;\epsilon}\Big[ \big\|\epsilon - \epsilon_\theta(x_t,t)\big\|^2 \Big],$$ where $t$ is uniformly sampled from $\{1,\dots,T\}$, $\epsilon\sim \mathcal{N}(0,I)$, and $x_t = \sqrt{\bar\alpha_t}x_0 + \sqrt{1-\bar\alpha_t}\,\epsilon$ as defined before&#8203;:contentReference[oaicite:8]{index=8}. In words: *take a real image $x_0$, pick a random diffusion timestep $t$, noise the image to get $x_t$, and train the network to predict the exact noise you added*. By doing this for all timesteps, the network learns to reverse every step of the diffusion. **Training Algorithm:** The training loop follows the procedure implicit in $L_{\text{simple}}$ above. Pseudocode for one iteration is: 1. Sample a real data example $x_0 \sim q(x_0)$ (e.g. an image from the dataset). 2. Sample a timestep $t$ uniformly from $\{1,\dots,T\}$. 3. Sample Gaussian noise $\epsilon \sim \mathcal{N}(0,I)$. 4. Form a noisy image $x_t = \sqrt{\bar\alpha_t}\,x_0 + \sqrt{1-\bar\alpha_t}\,\epsilon$ (this is effectively one forward diffusion step *integrated* to time $t$ in closed form). 5. Feed $x_t$ and $t$ into the U-Net $\epsilon_\theta(x_t, t)$ to get a predicted noise $\hat\epsilon$. 6. Compute the loss $||\hat\epsilon - \epsilon||^2$ and backpropagate to update parameters $\theta$. This procedure trains the model on *all* timesteps in parallel (by random sampling of $t$ each batch) rather than unrolling a $T$-step chain per data example&#8203;:contentReference[oaicite:9]{index=9}. This significantly improves training efficiency and stability, since the model does not need to perfectly denoise from $x_T$ all the way to $x_0$ in one go; instead it incrementally learns each transition. In early training, it may only succeed on small $t$ (slight noise), but over time it becomes proficient at denoising even very noisy inputs ($t$ large). The original DDPM paper also included a *variational lower-bound loss* $L_{\text{vlb}}$ in addition to $L_{\text{simple}}$ to account for terms like the prior $KL(q(x_T|x_0)||p(x_T))$ and to optionally learn $\Sigma_\theta$. In practice, the simple MSE loss dominates and is often weighted much higher than the exact ELBO. Empirically, this simplified objective yields better sample quality&#8203;:contentReference[oaicite:10]{index=10}. Thus, diffusion model training largely reduces to a **denoising score matching** task: the model is trained to output the score (gradient of log density) of $q(x_t)$ up to a constant, since predicting $\epsilon$ is equivalent to estimating $\nabla_{x_t} \log q(x_t)$ (this connection is formalized in score-based models, Section 6.2). ## 4. Inference: Generating Samples by Reverse Diffusion Once the model $\epsilon_\theta(x_t,t)$ (or $\mu_\theta$) is trained, we can generate new data by simulating the learned reverse Markov chain $p_\theta(x_{0:T})$. **Sampling** from a diffusion model proceeds as follows: 1. Start with $x_T \sim \mathcal{N}(0,I)$, a random noise sample in the data space (for images, this is pure Gaussian noise). 2. For $t = T, T-1, \dots, 1$: use the model to compute the conditional $p_\theta(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t,t), \Sigma_\theta(x_t,t))$. Then sample $x_{t-1}$ from this Gaussian distribution. 3. Output $x_0$, the final sample, which should lie in the data distribution. In practice, using the parameterization by noise prediction, one typically computes $\mu_\theta(x_t,t)$ via the relation: $$\mu_\theta(x_t,t) = \frac{1}{\sqrt{\alpha_t}}\Big(x_t - \frac{\beta_t}{\sqrt{1-\bar\alpha_t}}\,\epsilon_\theta(x_t,t)\Big),$$ which comes from rearranging the formula for $\tilde\mu_t(x_t,x_0)$ and substituting the model's estimate of $\epsilon$ in place of the unknown true noise. The variance $\Sigma_\theta$ is often fixed to $\sigma_t^2 = \beta_t$ or $\tilde\beta_t$ (as was done in DDPM for most timesteps) or sometimes learned as a small network output. After computing $\mu_\theta$, one adds Gaussian noise with variance $\sigma_t^2$ to sample $x_{t-1}$. When $t=1$, we usually take $\sigma_1^2 \to 0$ (or simply output $\mu_\theta(x_1,1)$) to obtain a final clean sample. **Sampling Cost and Accelerations:** A straightforward implementation of the above requires $T$ forward passes through the U-Net, one for each timestep. Typically $T$ is on the order of 1000 for high-quality images&#8203;:contentReference[oaicite:11]{index=11}. This can make sampling quite slow – e.g. generating a single image might require 1000 neural network evaluations, which is far more expensive than GAN generation (one pass) or autoregressive models (order of image size, e.g. 1024 steps). Researchers have developed several techniques to speed up diffusion sampling: - **Fewer Diffusion Steps:** One can trade off generation speed and quality by reducing $T$. Even after training a model with $T=1000$, it's been observed that decent samples can be obtained by using only, say, 100 or 200 steps if the noise schedule is adjusted or if one skips some steps (though too few steps hurt quality). For example, early diffusion models took around 20 hours to sample 50k images (32×32) with $T=1000$, whereas a GAN could do so in under a minute. Subsequent research showed that as few as 50 steps can yield good images with careful methods&#8203;:contentReference[oaicite:12]{index=12}. - **Denoising Diffusion Implicit Models (DDIM):** DDIM is a method introduced by Song *et al.* (2021) that formulates a *deterministic* non-Markovian reverse process which is consistent with the same trained model&#8203;:contentReference[oaicite:13]{index=13}. Essentially, DDIM shows that there are infinitely many possible reverse processes yielding the same marginals $q(x_{t}|x_0)$; by choosing one without the random noise, one can sample in fewer steps without retraining. Empirically, DDIM can generate high-quality images in 10–50 inference steps (a 10×–20× speedup) with only a slight degradation in diversity&#8203;:contentReference[oaicite:14]{index=14}&#8203;:contentReference[oaicite:15]{index=15}. The trade-off is that DDIM's process is no longer probabilistic, but for image synthesis diversity can be maintained by adding some randomness or using different noise seeds. - **Advanced ODE/SDE Solvers:** Viewing the diffusion model in continuous time (see Section 6.2), one can employ numerical SDE or ODE solvers of higher order to take larger steps. Methods like the predictor-corrector samplers&#8203;:contentReference[oaicite:16]{index=16}, or more recent ones like DPM-Solver, apply tailored solvers (e.g. Runge-Kutta or multistep methods) to effectively jump through the reverse process in fewer function evaluations. These can achieve significant speedups (sampling in tens of steps) while preserving fidelity. - **Model Distillation:** Another approach is to train a separate model to mimic the $T$-step sampler with far fewer steps. E.g. one can distill a diffusion model into a model that requires only e.g. 4 or 8 steps, by directly training it to generate equally good samples in that many iterations. This is a research direction aimed at real-time diffusion model sampling. In summary, the naive sampling procedure of diffusion models is slow but conceptually straightforward: iteratively apply the learned denoiser. Numerous approximations and algorithmic improvements exist to make this tractable for practical use. In cutting-edge implementations like *Stable Diffusion*, optimized samplers (often variants of DDIM or specialized SDE solvers) are used to generate high-resolution images in dozens of steps without noticeable quality loss, which is crucial for real-world deployment. ## 5. Applications Beyond Image Generation While diffusion models rose to prominence through image synthesis, their core idea of noising and denoising has been successfully applied in other domains as well. **5.1 Audio Synthesis:** Diffusion models have been adapted to generate waveforms and audio signals, offering an alternative to autoregressive models (like WaveNet) and GANs for speech and music. For example, *DiffWave* (Kong et al., 2020) is a diffusion probabilistic model for audio that can function as a high-quality neural vocoder (turning spectrograms into waveforms) or even for unconditional audio generation&#8203;:contentReference[oaicite:17]{index=17}. DiffWave takes Gaussian noise and iteratively refines it into a speech waveform, much like an image diffusion model does for images. Impressively, it matches the quality of a well-known WaveNet vocoder (MOS ~4.44) but with much faster sampling since generation is non-autoregressive&#8203;:contentReference[oaicite:18]{index=18}. Similarly, *WaveGrad* (Chen et al., 2020) and subsequent models apply diffusion to text-to-speech synthesis, producing highly natural speech. The diffusion approach to audio has also shown advantages in *unconditional* audio generation, where models like DiffWave produced more diverse and higher-fidelity sounds than GANs in experimental comparisons&#8203;:contentReference[oaicite:19]{index=19}. Beyond speech, diffusion models have been used for music generation and audio enhancement (e.g. removing noise from recordings via learned denoising). The success in audio suggests that diffusion models are a general paradigm for generative modeling of signals, not just limited to images. **5.2 Molecular Design:** Generating novel molecules (for drug discovery or material design) is another frontier where diffusion models have made an impact. Recent work represents molecular structures (for instance, as graphs of atoms or as 3D point clouds of atomic coordinates) and then applies a diffusion process in that space to generate new chemical designs&#8203;:contentReference[oaicite:20]{index=20}. For example, methods have been developed to diffuse on molecular graphs, adding and removing atoms/bonds as a form of noise, or to diffuse in continuous 3D space to generate stable molecular conformations. One study notes that *"generative deep learning methods have recently been proposed for generating 3D molecules using equivariant graph neural networks within a denoising diffusion framework."*&#8203;:contentReference[oaicite:21]{index=21}. These models, by incorporating physical symmetries, can produce valid 3D molecular structures and even complex proteins. Diffusion models have shown promise in generating molecules that are both novel and chemically valid, and can be conditioned on desired properties (targeting molecules with certain pharmacological characteristics). For instance, a *geometry-complete diffusion model* was able to generate larger molecules that are geometrically and energetically stable, outperforming prior approaches&#8203;:contentReference[oaicite:22]{index=22}. In general, the ability of diffusion models to *cover* the space of outputs (as opposed to mode-collapsing to a few solutions) is advantageous in design tasks where diversity is crucial (e.g. exploring many possible drug candidates)&#8203;:contentReference[oaicite:23]{index=23}. **Other domains:** Diffusion-based generative models have also been explored in areas like **computer graphics (3D shape generation)**, **video synthesis** (generating video frames sequentially with diffusion in latent spaces), **text generation** (diffusion in embedding space or for generating strokes of handwritten text), and more. The flexibility of defining a noising process in any data space means diffusion is a broadly applicable idea. The results in audio and molecular generation demonstrate that with the right data representation and network (e.g. graph neural networks for molecules), diffusion models can rival or surpass domain-specific generative models in quality. ## 6. Recent Extensions and Variants of Diffusion Models Diffusion models have rapidly evolved, and several notable variants/extensions have been introduced. Here we discuss three important developments: **DDPM**, **score-based generative models (SGM)**, and **latent diffusion models (LDM)**, which have shaped the state of the art. ### 6.1 Denoising Diffusion Probabilistic Models (DDPM) DDPM refers to the specific formulation by Ho, Jain, and Abbeel (2020), which popularized diffusion models by demonstrating high-quality image synthesis for the first time with this approach. In DDPM, the forward process is as described in Section 1 (using a variance schedule $\beta_t$ linearly increasing), and the reverse process is parameterized by a U-Net that predicts $\epsilon_\theta(x_t,t)$. The model is trained by optimizing a weighted variational bound, but the authors found that a simplified objective (our $L_{\text{simple}}$ above) — essentially *denoising score matching* at each timestep — was effective and yielded better samples&#8203;:contentReference[oaicite:24]{index=24}. DDPM showed that diffusion models can produce *diverse* and *high-fidelity* images without adversarial training, addressing some drawbacks of GANs. On CIFAR-10 (32×32 images), DDPM achieved a then state-of-the-art Fréchet Inception Distance (FID) of 3.17 and a high Inception Score, indicating very realistic samples. One of DDPM's contributions is drawing a connection between diffusion models and earlier concepts: it showed that this procedure is equivalent to optimizing denoising score matching at multiple noise levels, and the sampling procedure is equivalent to an annealed Langevin dynamics sampler. This insight unified diffusion models with the score-based approaches developed around the same time (see SGM below). DDPMs also introduced techniques like learning *time-conditional normalization* (so the network knows the noise level) and underscoring the importance of variance scheduling (e.g. they used linear or cosine schedules for $\beta_t$ that balance difficult denoising steps). Following DDPM, many works built on its framework, including conditional diffusion models (for class-conditional image generation, by feeding class information into the U-Net) and improved training losses (e.g. a hybrid of $L_{\text{simple}}$ and $L_{\text{vlb}}$ to ensure good likelihoods). In short, DDPM is the cornerstone that proved diffusion models "*beat GANs on image synthesis*" in likelihood-based generation&#8203;:contentReference[oaicite:25]{index=25}, and it remains a foundation for more advanced variants. ### 6.2 Score-Based Generative Models (SGM) Score-based generative models were developed by Song and colleagues (2019–2021) in parallel to DDPMs, and ultimately the two approaches were shown to be closely related. In an SGM, instead of explicitly setting up a forward diffusion with a specified $\beta_t$ schedule, one directly trains a neural network $s_\theta(x,t)$ to estimate the **score** $\nabla_x \log q_t(x)$ of the data distribution perturbed with noise. Song & Ermon (2019) first introduced **Noise Conditional Score Networks (NCSN)**, where they added noise of various levels to data and trained a network to output the gradient of the log-density (the score) at each noise level by minimizing a denoising score matching objective. To generate data, they used Langevin dynamics: an iterative procedure $x_{n+1} = x_n + \frac{\delta^2}{2} s_\theta(x_n,\sigma) + \delta\, z_n$ (with $z_n\sim N(0,I)$) that gradually removes noise, guided by the learned score function. This produced high-quality images comparable to GANs, validating the score-based approach. The approach was further refined in **Score-Based Generative Modeling through Stochastic Differential Equations** (Song *et al.*, 2021). They proposed a *continuous-time diffusion process*: instead of discrete steps $t=1,\dots,T$, they considered an Itô **stochastic differential equation (SDE)**: $$ dX = f(X,t)\,dt + g(t)\,dW, $$ which defines a continuum of noise perturbation from $t=0$ (data) to $t=T$ (noise). For example, a simple diffusion SDE is the Variance Exploding (VE) SDE: $dX = \sqrt{d[\sigma^2(t)]}\,dW$ where $\sigma(t)$ grows from 0 to large value as $t$ increases, injecting noise. They showed that the **reverse-time SDE** is $$ dX = [f(X,t) - g(t)^2 \nabla_X \log q_t(X)]\,dt + g(t)\,d\bar W,$$ i.e. it requires knowing the score $\nabla_X \log q_t(X)$ at each continuous time. The neural network $s_\theta(x,t)$ is trained to approximate that score for all $t$. Using an SDE solver (or an equivalent ordinary differential equation formulation with a "probability flow ODE"), one can then sample from the model by integrating from $t=T$ (noise) back to $t=0$ (data). This SDE framework **unifies** previous discrete diffusion models and score matching: it was shown that diffusion probabilistic models (like DDPM) are a special case of a discretized SDE approach. SGMs also introduced the **predictor-corrector scheme**&#8203;:contentReference[oaicite:26]{index=26}: one can alternate between a "predictor" step (e.g. using an ODE or coarse SDE step to evolve the sample) and a "corrector" step (e.g. a few Langevin iterations that use the score to refine the sample at the same $t$) to improve sample quality. This combination yielded record-breaking performance on CIFAR-10 (FID 2.20, which was better than DDPM) and even produced high-resolution $1024\times1024$ images of high fidelity for the first time with a score-based model. Another benefit of the SDE formulation is the ability to compute likelihoods: by deriving the probability flow ODE, one can compute exact log-density for generated samples (something not straightforward in discrete DDPM without large $T$). In summary, score-based generative models provide a more *continuous and unified* view of diffusion, treating it as learning a time-dependent vector field (score) that guides a noise-to-data transformation. SGM and DDPM are largely equivalent approaches now, but SGM literature often emphasizes the theoretical link to SDEs and introduces new sampling algorithms grounded in differential equation solvers. ### 6.3 Latent Diffusion Models (LDM) A major drawback of vanilla diffusion models is the computational cost of operating in high-dimensional data space for many timesteps&#8203;:contentReference[oaicite:27]{index=27}. *Latent diffusion models* address this by performing the diffusion process in a **lower-dimensional latent space** rather than pixel space&#8203;:contentReference[oaicite:28]{index=28}. The idea, introduced by Rombach *et al.* (2022), is to first train an **autoencoder** to compress images into a latent representation (and decode back to images). This autoencoder is often designed to keep most perceptually relevant features in the latent, but drop high-frequency details that are less important&#8203;:contentReference[oaicite:29]{index=29}. Then, a diffusion model is trained on these latent codes instead of on images. Because the latent typically has far fewer dimensions (e.g. 64×64×4 instead of 256×256×3 for an image), each diffusion step and each U-Net pass is much faster and uses less memory. Rombach *et al.* call the resulting two-stage approach a **Latent Diffusion Model (LDM)**&#8203;:contentReference[oaicite:30]{index=30}. An advantage of decoupling compression and diffusion is that the autoencoder needs to be trained only once, and can then be reused for various diffusion models or tasks&#8203;:contentReference[oaicite:31]{index=31}. For instance, one can train an LDM for unconditional image generation, and separately train another small network to condition the LDM on text, all while using the same fixed autoencoder for encoding/decoding images. Despite operating in a compressed space, LDMs were shown to generate images with excellent detail: the autoencoder ensures that generated latent codes correspond to coherent images. In fact, *Stable Diffusion* – a widely used text-to-image model – is exactly an LDM: it uses a variational autoencoder to map 512×512 images to a 64×64 latent, and a diffusion U-Net that is conditioned on text embeddings to generate latents, which are then decoded to images. This achieved high fidelity and diversity with a fraction of the compute that a pixel-space diffusion would require. The authors note that LDMs "significantly improve both the training and sampling efficiency" of diffusion models, making high-resolution image synthesis feasible on moderate hardware&#8203;:contentReference[oaicite:32]{index=32}&#8203;:contentReference[oaicite:33]{index=33}. Another benefit is that the compression stage can be seen as focusing the diffusion model on "semantic compression" (global content) after "perceptual compression" is handled by the autoencoder&#8203;:contentReference[oaicite:34]{index=34}. This means the diffusion model doesn't waste capacity modeling pixel-level details, which can improve sampling efficiency without sacrificing image quality&#8203;:contentReference[oaicite:35]{index=35}. LDMs have also been extended to other domains, like video (operating on video feature latents) and other data that benefit from a two-stage approach. Overall, latent diffusion has been a critical innovation for scaling diffusion up to ultra-high resolutions and enabling their use in memory-constrained or real-time settings. --- **Conclusion:** Diffusion models provide a powerful and theoretically elegant framework for generative modeling. We started from a rigorous definition of a forward noising process and its reverse, saw how a carefully designed U-Net learns to invert that process, and how training reduces to matching the added noise (or score) at each step. We detailed the sampling procedure and practical tricks to accelerate it, and highlighted the versatility of diffusion models across image, audio, and molecular domains. Finally, we examined major variants like DDPM (which introduced the modern formulation), score-based models (which connected to SDEs and inspired new samplers), and latent diffusion (which made high-resolution generation more accessible). Together, these developments show how diffusion models evolved into a leading approach in generative modeling, combining rigorous probabilistic foundations with intuitive noise-removal dynamics to create complex data from random noise. With ongoing research (e.g. consistency models, improved conditional diffusion, etc.), diffusion models continue to expand their reach, offering both deep theoretical connections and state-of-the-art practical results. **Sources:** 1. Ho et al., "*Denoising Diffusion Probabilistic Models,"* NeurIPS 2020. 2. Song & Ermon, "*Generative Modeling by Estimating Gradients of the Data Distribution,"* NeurIPS 2019 (NCSN). 3. Song et al., "*Score-Based Generative Modeling through Stochastic Differential Equations,"* ICLR 2021. 4. Dhariwal & Nichol, "*Diffusion Models Beat GANs on Image Synthesis,"* NeurIPS 2021&#8203;:contentReference[oaicite:36]{index=36}. 5. Rombach et al., "*High-Resolution Image Synthesis with Latent Diffusion Models,"* CVPR 2022&#8203;:contentReference[oaicite:37]{index=37}&#8203;:contentReference[oaicite:38]{index=38}. 6. Lilian Weng, "*What are Diffusion Models?"* Lil'Log Blog (2021). 7. LabML AI, "*U-Net for DDPM,"* 2021&#8203;:contentReference[oaicite:39]{index=39}. 8. Gabriel Mongaras, "*Diffusion Models — DDPMs, DDIMs, and Classifier Free Guidance,"* Medium 2022&#8203;:contentReference[oaicite:40]{index=40}. 9. Kong et al., "*DiffWave: A Versatile Diffusion Model for Audio Synthesis,"* ICLR 2021&#8203;:contentReference[oaicite:41]{index=41}. 10. Morehead & Cheng, "*Geometry-Complete Diffusion for 3D Molecule Generation,"* Comm. Chem. 2024&#8203;:contentReference[oaicite:42]{index=42}.