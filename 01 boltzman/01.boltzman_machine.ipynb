{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and define common functions. Do not modify.\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid , save_image\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_and_save(img, file_name):\n",
    "    \"\"\"Show and save the image.\n",
    "    Args:\n",
    "        img (Tensor): The image.\n",
    "        file_name (Str): The destination.\n",
    "    \"\"\"\n",
    "    npimg = np.transpose(img.numpy(), (1, 2, 0))\n",
    "    f = \"./%s.png\" % file_name\n",
    "    plt.imshow(npimg, cmap='gray')\n",
    "    plt.show()\n",
    "    plt.imsave(f, npimg)\n",
    "\n",
    "def train(model, train_loader, n_epochs=20, lr=0.01):\n",
    "    \"\"\"Train a generative model.\n",
    "    Args:\n",
    "        model: The model.\n",
    "        train_loader (DataLoader): The data loader.\n",
    "        n_epochs (int, optional): The number of epochs. Defaults to 20.\n",
    "        lr (Float, optional): The learning rate. Defaults to 0.01.\n",
    "    Returns:\n",
    "        The trained model.\n",
    "    \"\"\"\n",
    "    train_op = optim.Adam(model.parameters(), lr)\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        loss_ = []\n",
    "        for _, (data, target) in enumerate(train_loader):\n",
    "            loss = model.get_loss(data.view(-1, 784))\n",
    "            loss_.append(loss.item())\n",
    "            train_op.zero_grad()\n",
    "            loss.backward()\n",
    "            train_op.step()\n",
    "        print('Epoch %d\\t Loss=%.4f' % (epoch, np.mean(loss_)))\n",
    "    return model\n",
    "\n",
    "def train_vae2(model, train_loader, n_epochs=20, lr=0.01):\n",
    "    \"\"\"Train a generative model.\n",
    "    Args:\n",
    "        model: The model.\n",
    "        train_loader (DataLoader): The data loader.\n",
    "        n_epochs (int, optional): The number of epochs. Defaults to 20.\n",
    "        lr (Float, optional): The learning rate. Defaults to 0.01.\n",
    "    Returns:\n",
    "        The trained model.\n",
    "    \"\"\"\n",
    "    train_op = optim.Adam(model.parameters(), lr)\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        loss_ = []\n",
    "        for _, (data, target) in enumerate(train_loader):\n",
    "            loss = model.get_loss(data)\n",
    "            loss_.append(loss.item())\n",
    "            train_op.zero_grad()\n",
    "            loss.backward()\n",
    "            train_op.step()\n",
    "        print('Epoch %d\\t Loss=%.4f' % (epoch, np.mean(loss_)))\n",
    "    return model\n",
    "\n",
    "seed = 2025\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM(nn.Module):\n",
    "    \"\"\"Restricted Boltzmann Machine.\n",
    "    Args:\n",
    "        n_vis (int, optional): The size of visible layer. Defaults to 784.\n",
    "        n_hid (int, optional): The size of hidden layer. Defaults to 128.\n",
    "        k (int, optional): The number of Gibbs sampling. Defaults to 1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_vis=784, n_hid=128, k=1):\n",
    "        \"\"\"Create a RBM.\"\"\"\n",
    "        super(RBM, self).__init__()\n",
    "        self.v = nn.Parameter(torch.randn(1, n_vis))\n",
    "        self.h = nn.Parameter(torch.randn(1, n_hid))\n",
    "        self.W = nn.Parameter(torch.randn(n_hid, n_vis))\n",
    "        self.k = k\n",
    "\n",
    "    def visible_to_hidden(self, v):\n",
    "        r\"\"\"Conditional sampling a hidden variable given a visible variable.\n",
    "        Args:\n",
    "            v (Tensor): The visible variable.\n",
    "        Returns:\n",
    "            Tensor: The hidden variable.\n",
    "        \"\"\"\n",
    "        p = torch.sigmoid(F.linear(v, self.W, self.h))\n",
    "        return p.bernoulli()\n",
    "\n",
    "    def hidden_to_visible(self, h):\n",
    "        r\"\"\"Conditional sampling a visible variable given a hidden variable.\n",
    "        Args:\n",
    "            h (Tendor): The hidden variable.\n",
    "        Returns:\n",
    "            Tensor: The visible variable.\n",
    "        \"\"\"\n",
    "        p = torch.sigmoid(F.linear(h, self.W.t(), self.v))\n",
    "        return p.bernoulli()\n",
    "\n",
    "    def free_energy(self, v, reduction='mean'):\n",
    "        r\"\"\"Free energy function.\n",
    "        .. math::\n",
    "            \\begin{align}\n",
    "                F(x) &= -\\log \\sum_h \\exp (-E(x, h)) \\\\\n",
    "                &= -a^\\top x - \\sum_j \\log (1 + \\exp(W^{\\top}_jx + b_j))\\,.\n",
    "            \\end{align}\n",
    "        Args:\n",
    "            v (Tensor): The visible variable.\n",
    "        Returns:\n",
    "            energy (FloatTensor): The free energy value.\n",
    "        \"\"\"\n",
    "        v_term = torch.matmul(v, self.v.t())\n",
    "        h_term = torch.sum(F.softplus(F.linear(v, self.W, self.h)), dim=-1, keepdim=True)\n",
    "        energy = -v_term - h_term\n",
    "        if reduction == 'none':\n",
    "            return energy.squeeze(-1)\n",
    "        elif reduction == 'mean':\n",
    "            return torch.mean(energy)\n",
    "\n",
    "    def forward(self, v):\n",
    "        r\"\"\"Compute the real and generated examples.\n",
    "        Args:\n",
    "            v (Tensor): The visible variable.\n",
    "        Returns:\n",
    "            (Tensor, Tensor): The real and generagted variables.\n",
    "        \"\"\"\n",
    "        for _ in range(self.k):\n",
    "            h = self.visible_to_hidden(v)\n",
    "            v_gibb = self.hidden_to_visible(h)\n",
    "        return v, v_gibb\n",
    "    \n",
    "    def get_loss(self, inputs):\n",
    "        r\"\"\"Compute the loss for training the model.\n",
    "        Args:\n",
    "            inputs (Tensor): The visible variable.\n",
    "        Returns:\n",
    "            Tensor: Loss.\n",
    "        \"\"\"\n",
    "        v, v_gibb = self.forward(inputs)\n",
    "        loss = self.free_energy(v) - self.free_energy(v_gibb)\n",
    "        return loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def pseudo_likelihood(self, v):\n",
    "        \"\"\"DO NOT MODIFY THIS FUNCTION\"\"\"\n",
    "        # Randomly corrupt one feature in each sample in v.\n",
    "        ind = (np.arange(v.shape[0]), np.random.randint(0, v.shape[1], v.shape[0]))\n",
    "        v_ = v.clone()\n",
    "        v_[ind] = 1 - v_[ind]\n",
    "        fe = self.free_energy(v, reduction='none')\n",
    "        fe_ = self.free_energy(v_, reduction='none')\n",
    "        m = torch.nn.LogSigmoid()\n",
    "        score = v.shape[1] * m(fe_ - fe)\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "train_dataset = datasets.MNIST('./data',\n",
    "    train=True,\n",
    "    download = True,\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), lambda x: (x > 0).float()]\n",
    "    )\n",
    ")\n",
    "test_dataset = datasets.MNIST('./data',\n",
    "    train=False,\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), lambda x: (x > 0).float()]\n",
    "    )\n",
    ")\n",
    "\n",
    "batch_size = 128\n",
    "n_hid = 128\n",
    "n_vis = 784\n",
    "n_epochs = 20\n",
    "lr = 0.01\n",
    "rbm_ckpt_fn = 'model_rbm_seed2025.pt'\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can try different k for the report, but use k=1 when submitting the checkpoint\n",
    "# Do not modify other parts of this cell\n",
    "if not os.path.exists(rbm_ckpt_fn):\n",
    "    model_rbm = RBM(n_vis=n_vis, n_hid=n_hid, k=1)\n",
    "    model_rbm = train(model_rbm, train_loader, n_epochs=n_epochs, lr=lr)\n",
    "    # save model, do not change the filename.\n",
    "    torch.save(model_rbm.state_dict(), rbm_ckpt_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use the following code to visualize reconstructed samples\n",
    "model_rbm.eval()\n",
    "vis_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64)\n",
    "images = next(iter(vis_loader))[0]\n",
    "v, v_gibbs = model_rbm(images.view(-1, 784))\n",
    "show_and_save(make_grid(v_gibbs.view(64, 1, 28, 28).data), 'rbm_fake')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
